{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tnsuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tnsuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "from string import digits\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "# 3 столбца - id, text, author\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "def tokenize_stem(file_text):\n",
    "    #firstly let's apply nltk tokenization\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #deleting stop_words\n",
    "    stop_words = set(stopwords.words('english')).union(set(get_stop_words('english')))\n",
    "    tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "    \n",
    "    # additional stop words removal\n",
    "    \n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words('english')).union(set(get_stop_words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(file_text):\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "    except:\n",
    "        nltk.download('punkt')\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "        \n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return len(set(tokens))/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[matrix([[ 0.00054413,  0.00054413,  0.00108826, ...,  0.00054413,\n",
      "          0.        ,  0.        ]]), matrix([[ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.0013942,\n",
      "          0.0006971]]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "# вытаскиваем \"значимые\" слова\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "raw_documents_authors = ['', '', '']\n",
    "\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    if row['author'] == 'EAP':\n",
    "        raw_documents_authors[0] += row['cleaned_text_string'] + ' '\n",
    "    elif row['author'] == 'HPL':\n",
    "        raw_documents_authors[1] += row['cleaned_text_string'] + ' '\n",
    "    else:\n",
    "        raw_documents_authors[2] += row['cleaned_text_string'] + ' '\n",
    "        \n",
    "\n",
    "# удалим уникальные слова, не встречающиеся у других писателей\n",
    "\n",
    "eap_only = set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "hpl_only = set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "msh_only = set(raw_documents_authors[2].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' '))\n",
    "\n",
    "unique_words = eap_only.union(hpl_only).union(msh_only)\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word')\n",
    "idf_matrix =  tf.fit_transform(raw_documents_authors)\n",
    "feature_names = tf.get_feature_names()\n",
    "# dictionary_word = dict(zip(feature_names, idf_matrix))\n",
    "\n",
    "dense_idf = [i.todense() for i in idf_matrix]\n",
    "print(dense_idf)\n",
    "\n",
    "max_weighted_term = []\n",
    "\n",
    "eap_dense_list = dense_idf[0].tolist()[0]\n",
    "hpl_dense_list = dense_idf[1].tolist()[0]\n",
    "mws_dense_list = dense_idf[2].tolist()[0]\n",
    "\n",
    "for inum, i in enumerate(eap_dense_list):\n",
    "    max_weighted_term.append(max(hpl_dense_list[inum], mws_dense_list[inum], \n",
    "                             i))\n",
    "\n",
    "max_tf_dict = dict(zip(feature_names, max_weighted_term))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_top_words(tfidfdict, numwrd):\n",
    "\n",
    "    top_word_dict, min_value, min_key = {}, 99, ''\n",
    "    \n",
    "\n",
    "    for k, v in max_tf_dict.items():\n",
    "        # print(top_word_dict.values())\n",
    "        # print(v)\n",
    "        if k not in unique_words and k not in eng_stopwords:\n",
    "        \n",
    "            if len(top_word_dict) < numwrd:\n",
    "                top_word_dict[k] = v\n",
    "                if v <= min_value:\n",
    "                    min_key = k\n",
    "            else:\n",
    "                # print(v, min(list(top_word_dict.values())))\n",
    "                if v > min(list(top_word_dict.values())) and k not in eng_stopwords:\n",
    "\n",
    "                    min_value = min(top_word_dict.values())\n",
    "\n",
    "                    for ky, va in top_word_dict.items():\n",
    "                        if va == min_value:\n",
    "                            min_key = ky\n",
    "\n",
    "                    top_word_dict.pop(min_key)\n",
    "                    top_word_dict[k] = v\n",
    "                \n",
    "    return top_word_dict\n",
    "\n",
    "\n",
    "\n",
    "def count_topwords(target_df):\n",
    "\n",
    "    for word in high_tf_idf_words_columns:\n",
    "        \n",
    "        # TODO: костыль, нужен, когда у нас уже есть такие столбцы\n",
    "        # в датасете\n",
    "#         try:\n",
    "#             target_df = target_df.drop(word, 1)\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "        \n",
    "\n",
    "        def count_numwords(collist):\n",
    "            value = 0\n",
    "\n",
    "            for wd in collist:\n",
    "                if wd == word:\n",
    "                    value += 1\n",
    "            return value\n",
    "\n",
    "\n",
    "        target_df[word] = target_df.cleaned_text.apply(count_numwords)\n",
    "        \n",
    "\n",
    "another_top_words_dict = extract_top_words(max_tf_dict, 80)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_tf_idf_words_columns = list(another_top_words_dict.keys())\n",
    "\n",
    "count_topwords(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>great</th>\n",
       "      <th>year</th>\n",
       "      <th>littl</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper  ...    great  year  \\\n",
       "0                35                 7                2  ...        0     0   \n",
       "1                14                 1                0  ...        0     0   \n",
       "2                32                 5                0  ...        0     0   \n",
       "\n",
       "   littl  whose  came  said  see  door  certain  found  \n",
       "0      0      0     0     0    0     0        0      0  \n",
       "1      0      0     0     0    0     0        0      0  \n",
       "2      0      0     0     0    0     0        0      0  \n",
       "\n",
       "[3 rows x 94 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['cleaned_text'] = df_train.text.apply(tokenize_stem)\n",
    "df_train['cleaned_text_string'] = df_train.cleaned_text.apply(' '.join)\n",
    "df_train.head(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>great</th>\n",
       "      <th>year</th>\n",
       "      <th>littl</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper  ...    great  year  \\\n",
       "0                35                 7                2  ...        0     0   \n",
       "1                14                 1                0  ...        0     0   \n",
       "2                32                 5                0  ...        0     0   \n",
       "\n",
       "   littl  whose  came  said  see  door  certain  found  \n",
       "0      0      0     0     0    0     0        0      0  \n",
       "1      0      0     0     0    0     0        0      0  \n",
       "2      0      0     0     0    0     0        0      0  \n",
       "\n",
       "[3 rows x 94 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['length']=df_train['cleaned_text_string'].apply(len)\n",
    "df_train[\"num_words\"] = df_train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_train[\"num_unique_words\"] = df_train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_train[\"num_punctuations\"] =df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_train[\"num_words_upper\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_train[\"num_words_title\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_train[\"mean_word_len\"] = df_train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_train[\"num_stopwords\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_train['lexical_diversity'] = df_train.text.apply(lexical_diversity)\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>great</th>\n",
       "      <th>year</th>\n",
       "      <th>littl</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>92.474357</td>\n",
       "      <td>27.799645</td>\n",
       "      <td>24.437977</td>\n",
       "      <td>3.206921</td>\n",
       "      <td>0.500266</td>\n",
       "      <td>2.334694</td>\n",
       "      <td>4.625193</td>\n",
       "      <td>13.150665</td>\n",
       "      <td>0.889980</td>\n",
       "      <td>0.041526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036025</td>\n",
       "      <td>0.028749</td>\n",
       "      <td>0.024135</td>\n",
       "      <td>0.030169</td>\n",
       "      <td>0.038332</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.028394</td>\n",
       "      <td>0.027329</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.033895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>50.839074</td>\n",
       "      <td>14.123252</td>\n",
       "      <td>11.053739</td>\n",
       "      <td>2.108637</td>\n",
       "      <td>0.852313</td>\n",
       "      <td>2.041579</td>\n",
       "      <td>0.554917</td>\n",
       "      <td>6.916422</td>\n",
       "      <td>0.076026</td>\n",
       "      <td>0.200409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208825</td>\n",
       "      <td>0.171311</td>\n",
       "      <td>0.160270</td>\n",
       "      <td>0.183094</td>\n",
       "      <td>0.192013</td>\n",
       "      <td>0.159049</td>\n",
       "      <td>0.171370</td>\n",
       "      <td>0.176640</td>\n",
       "      <td>0.170821</td>\n",
       "      <td>0.183894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>57.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.258065</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>117.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.961538</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>561.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>7.833333</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            length    num_words  num_unique_words  num_punctuations  \\\n",
       "count  5635.000000  5635.000000       5635.000000       5635.000000   \n",
       "mean     92.474357    27.799645         24.437977          3.206921   \n",
       "std      50.839074    14.123252         11.053739          2.108637   \n",
       "min       7.000000     4.000000          3.000000          1.000000   \n",
       "25%      57.000000    18.000000         17.000000          2.000000   \n",
       "50%      84.000000    26.000000         23.000000          3.000000   \n",
       "75%     117.000000    35.000000         30.000000          4.000000   \n",
       "max     561.000000   147.000000        102.000000         28.000000   \n",
       "\n",
       "       num_words_upper  num_words_title  mean_word_len  num_stopwords  \\\n",
       "count      5635.000000      5635.000000    5635.000000    5635.000000   \n",
       "mean          0.500266         2.334694       4.625193      13.150665   \n",
       "std           0.852313         2.041579       0.554917       6.916422   \n",
       "min           0.000000         0.000000       2.222222       0.000000   \n",
       "25%           0.000000         1.000000       4.258065       8.000000   \n",
       "50%           0.000000         2.000000       4.600000      12.000000   \n",
       "75%           1.000000         3.000000       4.961538      17.000000   \n",
       "max          10.000000        39.000000       7.833333      63.000000   \n",
       "\n",
       "       lexical_diversity         even     ...             great         year  \\\n",
       "count        5635.000000  5635.000000     ...       5635.000000  5635.000000   \n",
       "mean            0.889980     0.041526     ...          0.036025     0.028749   \n",
       "std             0.076026     0.200409     ...          0.208825     0.171311   \n",
       "min             0.250000     0.000000     ...          0.000000     0.000000   \n",
       "25%             0.840000     0.000000     ...          0.000000     0.000000   \n",
       "50%             0.891892     0.000000     ...          0.000000     0.000000   \n",
       "75%             0.947368     0.000000     ...          0.000000     0.000000   \n",
       "max             1.000000     2.000000     ...          4.000000     2.000000   \n",
       "\n",
       "             littl        whose         came         said          see  \\\n",
       "count  5635.000000  5635.000000  5635.000000  5635.000000  5635.000000   \n",
       "mean      0.024135     0.030169     0.038332     0.024845     0.028394   \n",
       "std       0.160270     0.183094     0.192013     0.159049     0.171370   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       2.000000     2.000000     1.000000     2.000000     3.000000   \n",
       "\n",
       "              door      certain        found  \n",
       "count  5635.000000  5635.000000  5635.000000  \n",
       "mean      0.027329     0.028571     0.033895  \n",
       "std       0.176640     0.170821     0.183894  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000  \n",
       "max       4.000000     2.000000     2.000000  \n",
       "\n",
       "[8 rows x 89 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hpl=df_train[df_train['author']=='HPL']\n",
    "df_hpl.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>great</th>\n",
       "      <th>year</th>\n",
       "      <th>littl</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>80.893038</td>\n",
       "      <td>25.442405</td>\n",
       "      <td>21.894937</td>\n",
       "      <td>4.096329</td>\n",
       "      <td>0.553291</td>\n",
       "      <td>2.102405</td>\n",
       "      <td>4.644952</td>\n",
       "      <td>12.747595</td>\n",
       "      <td>0.886060</td>\n",
       "      <td>0.037468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032278</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.035063</td>\n",
       "      <td>0.011646</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.045063</td>\n",
       "      <td>0.018481</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>0.013038</td>\n",
       "      <td>0.030253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>59.749772</td>\n",
       "      <td>18.567706</td>\n",
       "      <td>13.727397</td>\n",
       "      <td>3.573788</td>\n",
       "      <td>0.892966</td>\n",
       "      <td>2.052241</td>\n",
       "      <td>0.631340</td>\n",
       "      <td>9.619779</td>\n",
       "      <td>0.097354</td>\n",
       "      <td>0.195826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190537</td>\n",
       "      <td>0.134984</td>\n",
       "      <td>0.195948</td>\n",
       "      <td>0.119568</td>\n",
       "      <td>0.129235</td>\n",
       "      <td>0.223892</td>\n",
       "      <td>0.143784</td>\n",
       "      <td>0.153253</td>\n",
       "      <td>0.119953</td>\n",
       "      <td>0.174950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>925.000000</td>\n",
       "      <td>267.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            length    num_words  num_unique_words  num_punctuations  \\\n",
       "count  7900.000000  7900.000000       7900.000000       7900.000000   \n",
       "mean     80.893038    25.442405         21.894937          4.096329   \n",
       "std      59.749772    18.567706         13.727397          3.573788   \n",
       "min       5.000000     2.000000          2.000000          1.000000   \n",
       "25%      40.000000    12.000000         12.000000          2.000000   \n",
       "50%      65.000000    21.000000         19.000000          3.000000   \n",
       "75%     106.000000    33.000000         29.000000          5.000000   \n",
       "max     925.000000   267.000000        155.000000         71.000000   \n",
       "\n",
       "       num_words_upper  num_words_title  mean_word_len  num_stopwords  \\\n",
       "count      7900.000000      7900.000000    7900.000000    7900.000000   \n",
       "mean          0.553291         2.102405       4.644952      12.747595   \n",
       "std           0.892966         2.052241       0.631340       9.619779   \n",
       "min           0.000000         0.000000       2.000000       0.000000   \n",
       "25%           0.000000         1.000000       4.250000       6.000000   \n",
       "50%           0.000000         1.000000       4.600000      10.000000   \n",
       "75%           1.000000         2.000000       5.000000      17.000000   \n",
       "max          15.000000        43.000000      11.000000     135.000000   \n",
       "\n",
       "       lexical_diversity         even     ...             great         year  \\\n",
       "count        7900.000000  7900.000000     ...       7900.000000  7900.000000   \n",
       "mean            0.886060     0.037468     ...          0.032278     0.016203   \n",
       "std             0.097354     0.195826     ...          0.190537     0.134984   \n",
       "min             0.333333     0.000000     ...          0.000000     0.000000   \n",
       "25%             0.821429     0.000000     ...          0.000000     0.000000   \n",
       "50%             0.894737     0.000000     ...          0.000000     0.000000   \n",
       "75%             1.000000     0.000000     ...          0.000000     0.000000   \n",
       "max             1.000000     2.000000     ...          4.000000     2.000000   \n",
       "\n",
       "             littl        whose         came         said          see  \\\n",
       "count  7900.000000  7900.000000  7900.000000  7900.000000  7900.000000   \n",
       "mean      0.035063     0.011646     0.016203     0.045063     0.018481   \n",
       "std       0.195948     0.119568     0.129235     0.223892     0.143784   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       3.000000     3.000000     2.000000     6.000000     4.000000   \n",
       "\n",
       "              door      certain        found  \n",
       "count  7900.000000  7900.000000  7900.000000  \n",
       "mean      0.017722     0.013038     0.030253  \n",
       "std       0.153253     0.119953     0.174950  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000  \n",
       "max       4.000000     3.000000     2.000000  \n",
       "\n",
       "[8 rows x 89 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eap=df_train[df_train['author']=='EAP']\n",
    "df_eap.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>great</th>\n",
       "      <th>year</th>\n",
       "      <th>littl</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>85.267869</td>\n",
       "      <td>27.417273</td>\n",
       "      <td>23.544672</td>\n",
       "      <td>3.833719</td>\n",
       "      <td>0.751489</td>\n",
       "      <td>2.124255</td>\n",
       "      <td>4.598182</td>\n",
       "      <td>13.896923</td>\n",
       "      <td>0.883407</td>\n",
       "      <td>0.049305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017704</td>\n",
       "      <td>0.025645</td>\n",
       "      <td>0.019854</td>\n",
       "      <td>0.022998</td>\n",
       "      <td>0.019358</td>\n",
       "      <td>0.034414</td>\n",
       "      <td>0.024653</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.024156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>71.372940</td>\n",
       "      <td>23.134440</td>\n",
       "      <td>14.925835</td>\n",
       "      <td>2.840625</td>\n",
       "      <td>1.203636</td>\n",
       "      <td>1.759572</td>\n",
       "      <td>0.561558</td>\n",
       "      <td>12.196599</td>\n",
       "      <td>0.086804</td>\n",
       "      <td>0.225507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141565</td>\n",
       "      <td>0.166251</td>\n",
       "      <td>0.144178</td>\n",
       "      <td>0.168612</td>\n",
       "      <td>0.137791</td>\n",
       "      <td>0.184112</td>\n",
       "      <td>0.168378</td>\n",
       "      <td>0.091508</td>\n",
       "      <td>0.066693</td>\n",
       "      <td>0.158844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>73.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.560791</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>107.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.907156</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2709.000000</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            length    num_words  num_unique_words  num_punctuations  \\\n",
       "count  6044.000000  6044.000000       6044.000000       6044.000000   \n",
       "mean     85.267869    27.417273         23.544672          3.833719   \n",
       "std      71.372940    23.134440         14.925835          2.840625   \n",
       "min       3.000000     2.000000          2.000000          1.000000   \n",
       "25%      48.000000    15.000000         14.000000          2.000000   \n",
       "50%      73.000000    23.000000         21.000000          3.000000   \n",
       "75%     107.000000    34.000000         30.000000          5.000000   \n",
       "max    2709.000000   861.000000        429.000000         59.000000   \n",
       "\n",
       "       num_words_upper  num_words_title  mean_word_len  num_stopwords  \\\n",
       "count      6044.000000      6044.000000    6044.000000    6044.000000   \n",
       "mean          0.751489         2.124255       4.598182      13.896923   \n",
       "std           1.203636         1.759572       0.561558      12.196599   \n",
       "min           0.000000         0.000000       2.666667       0.000000   \n",
       "25%           0.000000         1.000000       4.250000       7.000000   \n",
       "50%           0.000000         2.000000       4.560791      12.000000   \n",
       "75%           1.000000         3.000000       4.907156      18.000000   \n",
       "max          27.000000        46.000000      10.500000     437.000000   \n",
       "\n",
       "       lexical_diversity         even     ...             great         year  \\\n",
       "count        6044.000000  6044.000000     ...       6044.000000  6044.000000   \n",
       "mean            0.883407     0.049305     ...          0.017704     0.025645   \n",
       "std             0.086804     0.225507     ...          0.141565     0.166251   \n",
       "min             0.398990     0.000000     ...          0.000000     0.000000   \n",
       "25%             0.823529     0.000000     ...          0.000000     0.000000   \n",
       "50%             0.885714     0.000000     ...          0.000000     0.000000   \n",
       "75%             0.950000     0.000000     ...          0.000000     0.000000   \n",
       "max             1.000000     3.000000     ...          4.000000     3.000000   \n",
       "\n",
       "             littl        whose         came         said          see  \\\n",
       "count  6044.000000  6044.000000  6044.000000  6044.000000  6044.000000   \n",
       "mean      0.019854     0.022998     0.019358     0.034414     0.024653   \n",
       "std       0.144178     0.168612     0.137791     0.184112     0.168378   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       2.000000     3.000000     1.000000     2.000000     4.000000   \n",
       "\n",
       "              door      certain        found  \n",
       "count  6044.000000  6044.000000  6044.000000  \n",
       "mean      0.008107     0.004467     0.024156  \n",
       "std       0.091508     0.066693     0.158844  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000  \n",
       "max       2.000000     1.000000     2.000000  \n",
       "\n",
       "[8 rows x 89 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mws=df_train[df_train['author']=='MWS']\n",
    "df_mws.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordset=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#делаю сет со всеми словами\n",
    "for i in df_train.index:\n",
    "    wordset |= set(df_train['cleaned_text'][i])\n",
    "wordlist=list(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>mws</th>\n",
       "      <th>eap</th>\n",
       "      <th>hpl</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>executor</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>journi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unpav</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ave</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  mws  eap  hpl  all\n",
       "0  executor    0    0    0    0\n",
       "1    journi    0    0    0    0\n",
       "2      that    0    0    0    0\n",
       "3     unpav    0    0    0    0\n",
       "4       ave    0    0    0    0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#делаю фрейм со словами\n",
    "df_word=pd.DataFrame(columns=[\"word\", \"mws\", \"eap\", \"hpl\", \"all\"])\n",
    "df_word[\"word\"]=wordlist\n",
    "df_word[\"mws\"]=0\n",
    "df_word[\"eap\"]=0\n",
    "df_word[\"hpl\"]=0\n",
    "df_word[\"all\"]=0\n",
    "df_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# как мы будем эту штуку правильнее делать (возможно это жуткий костыль), я хз\n",
    "# сначала создаем словарь где ключ - уникальное слово, а значение - его порядковый номер\n",
    "# затем создаем разреженную матрицу, которую заполняем в зависимости от порядковых номеров \n",
    "word_dict = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#делаю сет со всеми словами\n",
    "# и сразу заготовку под шапку(потом увидишь зачем)\n",
    "counter = 0\n",
    "head = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    for word in wordlist:\n",
    "        if word not in word_dict:\n",
    "            head.append(word)\n",
    "            word_dict[word] = counter\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# видоизменять колонки в pandas руками по одному значению в строке или столбце - очень плохая идея\n",
    "# колонка это numpy.ndarray, а значит при каждой итерации она будет пересоздаваться\n",
    "# что угробит производительность\n",
    "# делаем значит так. считаем где сколько и где встречались отдельные слова, затем создаем строку за строкой для \n",
    "# каждого предложения\n",
    "\n",
    "list_of_lists = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    row = [0 for i in range(len(word_dict))]\n",
    "    for word in wordlist:\n",
    "        row[word_dict[word]] += 1\n",
    "    list_of_lists.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... и для того чтобы посмотреть встречаемость того или иного слова по авторам добавим такую колонку\n",
    "\n",
    "count_frame = pd.DataFrame(list_of_lists)\n",
    "count_frame['author'] = df_train['author']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... и теперь нормальную шапку делаем\n",
    "\n",
    "count_frame.columns = head + ['author']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   this  process  howev  afford  mean  ascertain  dimens  dungeon  i  might  \\\n",
      "0     1        1      1       1     1          1       1        1  2      1   \n",
      "1     0        0      0       0     0          0       0        0  0      1   \n",
      "2     0        0      0       0     0          0       0        0  0      0   \n",
      "3     0        0      0       0     0          0       0        0  0      0   \n",
      "4     0        0      0       0     0          0       0        0  0      0   \n",
      "\n",
      "    ...    aegidus  burr  bentley  waltzer  binder  brusqueri  adriat  ancona  \\\n",
      "0   ...          0     0        0        0       0          0       0       0   \n",
      "1   ...          0     0        0        0       0          0       0       0   \n",
      "2   ...          0     0        0        0       0          0       0       0   \n",
      "3   ...          0     0        0        0       0          0       0       0   \n",
      "4   ...          0     0        0        0       0          0       0       0   \n",
      "\n",
      "   agir  author  \n",
      "0     0     EAP  \n",
      "1     0     HPL  \n",
      "2     0     EAP  \n",
      "3     0     MWS  \n",
      "4     0     HPL  \n",
      "\n",
      "[5 rows x 15230 columns]\n"
     ]
    }
   ],
   "source": [
    "print(count_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Пока объединим все, потом может быть будем использовать\n",
    "col=list(count_frame.columns)\n",
    "col[-1]='author_name'\n",
    "count_frame.columns=col\n",
    "pivot_col=pd.pivot_table(count_frame, aggfunc=np.sum, values=col, index=['author_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 14350 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  \\\n",
       "author_name                                                                  \n",
       "EAP             1      2      0       22       0     2      1     2      3   \n",
       "HPL             0      0      0       17      24     0      1     3      0   \n",
       "MWS             0      0      1        9       0     0      0     1      2   \n",
       "\n",
       "             abbrevi  ...    æmilianus  æneid  ærial  æronaut  ærostat  \\\n",
       "author_name           ...                                                \n",
       "EAP                2  ...            0      0      1        3        1   \n",
       "HPL                0  ...            2      1      0        0        0   \n",
       "MWS                0  ...            0      0      0        0        0   \n",
       "\n",
       "             æschylus  élite  émeut  οἶδα  υπνος  \n",
       "author_name                                       \n",
       "EAP                 1      1      1     0      0  \n",
       "HPL                 0      0      0     2      1  \n",
       "MWS                 0      0      0     0      0  \n",
       "\n",
       "[3 rows x 14350 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Убираем лишние слова, которые не учли раньше\n",
    "col=list(pivot_col.columns)\n",
    "col2=[string for string in col if (string[0]!='\"' and string[0]!=\"'\"\n",
    "                                  and string[0]!='.' and string[0]!='`'\n",
    "                                   and len(string)>3 and '.' not in string)]\n",
    "col=[]\n",
    "pivot_col=pivot_col[col2]\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUMA</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 14350 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  abbrevi  \\\n",
       "EAP      1      2      0       22       0     2      1     2      3        2   \n",
       "HPL      0      0      0       17      24     0      1     3      0        0   \n",
       "MWS      0      0      1        9       0     0      0     1      2        0   \n",
       "SUMA     1      2      1       48      24     2      2     6      5        2   \n",
       "\n",
       "      ...    æmilianus  æneid  ærial  æronaut  ærostat  æschylus  élite  \\\n",
       "EAP   ...            0      0      1        3        1         1      1   \n",
       "HPL   ...            2      1      0        0        0         0      0   \n",
       "MWS   ...            0      0      0        0        0         0      0   \n",
       "SUMA  ...            2      1      1        3        1         1      1   \n",
       "\n",
       "      émeut  οἶδα  υπνος  \n",
       "EAP       1     0      0  \n",
       "HPL       0     2      1  \n",
       "MWS       0     0      0  \n",
       "SUMA      1     2      1  \n",
       "\n",
       "[4 rows x 14350 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create pivot\n",
    "pivot_col=pivot_col.append(pivot_col.sum(), ignore_index=True)\n",
    "pivot_col.index=['EAP', 'HPL', 'MWS', 'SUMA']\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "      <th>summa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>73404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUMA</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>232610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 14351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  abbrevi  \\\n",
       "EAP      1      2      0       22       0     2      1     2      3        2   \n",
       "HPL      0      0      0       17      24     0      1     3      0        0   \n",
       "MWS      0      0      1        9       0     0      0     1      2        0   \n",
       "SUMA     1      2      1       48      24     2      2     6      5        2   \n",
       "\n",
       "       ...    æneid  ærial  æronaut  ærostat  æschylus  élite  émeut  οἶδα  \\\n",
       "EAP    ...        0      1        3        1         1      1      1     0   \n",
       "HPL    ...        1      0        0        0         0      0      0     2   \n",
       "MWS    ...        0      0        0        0         0      0      0     0   \n",
       "SUMA   ...        1      1        3        1         1      1      1     2   \n",
       "\n",
       "      υπνος   summa  \n",
       "EAP       0   86909  \n",
       "HPL       1   73404  \n",
       "MWS       0   72297  \n",
       "SUMA      1  232610  \n",
       "\n",
       "[4 rows x 14351 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa=[pivot_col.loc['EAP'].sum(), pivot_col.loc['HPL'].sum(), \n",
    "       pivot_col.loc['MWS'].sum(), pivot_col.loc['SUMA'].sum()]\n",
    "pivot_col['summa']=summa\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdic</th>\n",
       "      <th>aberr</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abhorr</th>\n",
       "      <th>abil</th>\n",
       "      <th>abject</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>your</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zenith</th>\n",
       "      <th>zest</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zone</th>\n",
       "      <th>summa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.373625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.315567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 6618 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandon  abash      abat  abbey     abdic     aberr     abhor    abhorr  \\\n",
       "EAP  0.458333    0.5  0.333333    0.6  0.142857  0.166667  0.058824  0.111111   \n",
       "HPL  0.354167    0.5  0.500000    0.0  0.000000  0.666667  0.235294  0.555556   \n",
       "MWS  0.187500    0.0  0.166667    0.4  0.857143  0.166667  0.705882  0.333333   \n",
       "\n",
       "         abil    abject    ...      younger  youngest      your     youth  \\\n",
       "EAP  0.789474  0.333333    ...     0.272727       0.2  0.534884  0.101562   \n",
       "HPL  0.052632  0.000000    ...     0.000000       0.4  0.069767  0.429688   \n",
       "MWS  0.157895  0.666667    ...     0.727273       0.4  0.395349  0.468750   \n",
       "\n",
       "         zeal  zenith  zest  zigzag      zone     summa  \n",
       "EAP  0.117647     0.4   0.2     0.4  0.666667  0.373625  \n",
       "HPL  0.470588     0.6   0.2     0.6  0.333333  0.315567  \n",
       "MWS  0.411765     0.0   0.6     0.0  0.000000  0.310808  \n",
       "\n",
       "[3 rows x 6618 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create probability of author text knowing that a word was used\n",
    "pivot_part=pivot_col\n",
    "pivot_part.loc['EAP']=pivot_col.loc['EAP']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['HPL']=pivot_col.loc['HPL']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['MWS']=pivot_col.loc['MWS']/pivot_col.loc['SUMA']\n",
    "pivot_part=pivot_part.loc[['EAP', 'HPL', 'MWS']]\n",
    "# Delete unique words\n",
    "pivot_part=pivot_part.loc[:, (pivot_part!=1).all(axis=0)]\n",
    "pivot_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44859813084112149"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will be easier to work this way\n",
    "eap_dict=pivot_part.loc['EAP'].to_dict()\n",
    "hpl_dict=pivot_part.loc['HPL'].to_dict()\n",
    "mws_dict=pivot_part.loc['MWS'].to_dict()\n",
    "eap_dict['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create author score \n",
    "def ind_val_eap(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=eap_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_hpl(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=hpl_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_mws(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=mws_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.074388</td>\n",
       "      <td>0.034504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.034739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper    ...      whose  \\\n",
       "0                35                 7                2    ...          0   \n",
       "1                14                 1                0    ...          0   \n",
       "2                32                 5                0    ...          0   \n",
       "\n",
       "   came  said  see  door  certain  found  mws_index  eap_index  hpl_index  \n",
       "0     0     0    0     0        0      0   0.035935   0.074388   0.034504  \n",
       "1     0     0    0     0        0      0   0.035860   0.060980   0.034739  \n",
       "2     0     0    0     0        0      0   0.026900   0.047832   0.037337  \n",
       "\n",
       "[3 rows x 97 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add index of author\n",
    "df_train['mws_index']=df_train['cleaned_text'].apply(ind_val_mws)/df_train['length']\n",
    "df_train['eap_index']=df_train['cleaned_text'].apply(ind_val_eap)/df_train['length']\n",
    "df_train['hpl_index']=df_train['cleaned_text'].apply(ind_val_hpl)/df_train['length']\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author2</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>...</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.074388</td>\n",
       "      <td>0.034504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.034739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, love, spring, as, look, windsor, terrac,...</td>\n",
       "      <td>how love spring as look windsor terrac sixteen...</td>\n",
       "      <td>144</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.033438</td>\n",
       "      <td>0.033601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[find, noth, els, even, gold, superintend, aba...</td>\n",
       "      <td>find noth els even gold superintend abandon at...</td>\n",
       "      <td>102</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036859</td>\n",
       "      <td>0.056661</td>\n",
       "      <td>0.043735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author2       id                                               text author  \\\n",
       "0        0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1        1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2        0  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3        2  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4        1  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "3  [how, love, spring, as, look, windsor, terrac,...   \n",
       "4  [find, noth, els, even, gold, superintend, aba...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "3  how love spring as look windsor terrac sixteen...     144         34   \n",
       "4  find noth els even gold superintend abandon at...     102         27   \n",
       "\n",
       "   num_unique_words  num_punctuations    ...      whose  came  said  see  \\\n",
       "0                35                 7    ...          0     0     0    0   \n",
       "1                14                 1    ...          0     0     0    0   \n",
       "2                32                 5    ...          0     0     0    0   \n",
       "3                32                 4    ...          0     0     0    0   \n",
       "4                25                 4    ...          0     0     0    0   \n",
       "\n",
       "   door  certain  found  mws_index  eap_index  hpl_index  \n",
       "0     0        0      0   0.035935   0.074388   0.034504  \n",
       "1     0        0      0   0.035860   0.060980   0.034739  \n",
       "2     0        0      0   0.026900   0.047832   0.037337  \n",
       "3     0        0      0   0.071850   0.033438   0.033601  \n",
       "4     0        0      0   0.036859   0.056661   0.043735  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform authors' names to numeric\n",
    "df_train['author']=df_train['author'].astype('category')\n",
    "df_train['author2']=df_train['author'].cat.codes\n",
    "# Create different features \n",
    "df_train.head(n=3)\n",
    "mid = df_train['author2']\n",
    "df_train.drop(labels=['author2'], axis=1,inplace = True)\n",
    "df_train.insert(0, 'author2', mid)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = df_train['author'].map(author_mapping_dict)\n",
    "train_id = df_train['id'].values\n",
    "test_id = df_test['id'].values\n",
    "cols_to_drop = ['id', 'text']\n",
    "train_X = df_train.drop(cols_to_drop+['author'], axis=1)\n",
    "test_X = df_test.drop(cols_to_drop, axis=1)\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf = tfidf_vec.fit_transform(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.842216198361\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "df_test = pd.concat([df_test, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the count vectorizer ###\n",
    "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "tfidf_vec.fit(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.450918416166\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# add the predictions as new features #\n",
    "df_train[\"nb_cvec_eap\"] = pred_train[:,0]\n",
    "df_train[\"nb_cvec_hpl\"] = pred_train[:,1]\n",
    "df_train[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "df_test[\"nb_cvec_eap\"] = pred_full_test[:,0]\n",
    "df_test[\"nb_cvec_hpl\"] = pred_full_test[:,1]\n",
    "df_test[\"nb_cvec_mws\"] = pred_full_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create data set\n",
    "ds_train=df_train.values\n",
    "X=ds_train[:, 6:]\n",
    "Y=ds_train[:, 0]\n",
    "seed=7\n",
    "test_size=0.3\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X,Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.835585\ttest-mlogloss:0.837725\n",
      "[1]\ttrain-mlogloss:0.68415\ttest-mlogloss:0.687706\n",
      "[2]\ttrain-mlogloss:0.587061\ttest-mlogloss:0.592043\n",
      "[3]\ttrain-mlogloss:0.521673\ttest-mlogloss:0.527715\n",
      "[4]\ttrain-mlogloss:0.476203\ttest-mlogloss:0.483356\n",
      "[5]\ttrain-mlogloss:0.443035\ttest-mlogloss:0.451809\n",
      "[6]\ttrain-mlogloss:0.419353\ttest-mlogloss:0.429201\n",
      "[7]\ttrain-mlogloss:0.400974\ttest-mlogloss:0.412195\n",
      "[8]\ttrain-mlogloss:0.387214\ttest-mlogloss:0.40011\n",
      "[9]\ttrain-mlogloss:0.375655\ttest-mlogloss:0.39004\n",
      "[10]\ttrain-mlogloss:0.366574\ttest-mlogloss:0.382388\n",
      "[11]\ttrain-mlogloss:0.358829\ttest-mlogloss:0.376405\n",
      "[12]\ttrain-mlogloss:0.352795\ttest-mlogloss:0.371874\n",
      "[13]\ttrain-mlogloss:0.347295\ttest-mlogloss:0.367584\n",
      "[14]\ttrain-mlogloss:0.342647\ttest-mlogloss:0.36442\n",
      "[15]\ttrain-mlogloss:0.338423\ttest-mlogloss:0.361727\n",
      "[16]\ttrain-mlogloss:0.334811\ttest-mlogloss:0.359318\n",
      "[17]\ttrain-mlogloss:0.331139\ttest-mlogloss:0.35714\n",
      "[18]\ttrain-mlogloss:0.328364\ttest-mlogloss:0.355447\n",
      "[19]\ttrain-mlogloss:0.325305\ttest-mlogloss:0.352994\n",
      "[20]\ttrain-mlogloss:0.322119\ttest-mlogloss:0.350542\n",
      "[21]\ttrain-mlogloss:0.319794\ttest-mlogloss:0.349249\n",
      "[22]\ttrain-mlogloss:0.316907\ttest-mlogloss:0.347616\n",
      "[23]\ttrain-mlogloss:0.314426\ttest-mlogloss:0.346598\n",
      "[24]\ttrain-mlogloss:0.311809\ttest-mlogloss:0.345387\n",
      "[25]\ttrain-mlogloss:0.309887\ttest-mlogloss:0.344942\n",
      "[26]\ttrain-mlogloss:0.308003\ttest-mlogloss:0.343978\n",
      "[27]\ttrain-mlogloss:0.305548\ttest-mlogloss:0.34336\n",
      "[28]\ttrain-mlogloss:0.303932\ttest-mlogloss:0.34238\n",
      "[29]\ttrain-mlogloss:0.302133\ttest-mlogloss:0.342035\n",
      "[30]\ttrain-mlogloss:0.300035\ttest-mlogloss:0.341176\n",
      "[31]\ttrain-mlogloss:0.298006\ttest-mlogloss:0.340698\n",
      "[32]\ttrain-mlogloss:0.295826\ttest-mlogloss:0.33986\n",
      "[33]\ttrain-mlogloss:0.294698\ttest-mlogloss:0.339788\n",
      "[34]\ttrain-mlogloss:0.292917\ttest-mlogloss:0.33952\n",
      "[35]\ttrain-mlogloss:0.290839\ttest-mlogloss:0.338726\n",
      "[36]\ttrain-mlogloss:0.288822\ttest-mlogloss:0.338372\n",
      "[37]\ttrain-mlogloss:0.287194\ttest-mlogloss:0.338032\n",
      "[38]\ttrain-mlogloss:0.285623\ttest-mlogloss:0.337184\n",
      "[39]\ttrain-mlogloss:0.284408\ttest-mlogloss:0.336862\n",
      "[40]\ttrain-mlogloss:0.28334\ttest-mlogloss:0.336597\n",
      "[41]\ttrain-mlogloss:0.282424\ttest-mlogloss:0.336453\n",
      "[42]\ttrain-mlogloss:0.281122\ttest-mlogloss:0.336237\n",
      "[43]\ttrain-mlogloss:0.280143\ttest-mlogloss:0.335983\n",
      "[44]\ttrain-mlogloss:0.278748\ttest-mlogloss:0.335791\n",
      "[45]\ttrain-mlogloss:0.27758\ttest-mlogloss:0.33524\n",
      "[46]\ttrain-mlogloss:0.276401\ttest-mlogloss:0.335061\n",
      "[47]\ttrain-mlogloss:0.275483\ttest-mlogloss:0.334842\n",
      "[48]\ttrain-mlogloss:0.273968\ttest-mlogloss:0.334702\n",
      "[49]\ttrain-mlogloss:0.272973\ttest-mlogloss:0.334426\n",
      "[50]\ttrain-mlogloss:0.271775\ttest-mlogloss:0.334399\n",
      "[51]\ttrain-mlogloss:0.270336\ttest-mlogloss:0.334156\n",
      "[52]\ttrain-mlogloss:0.269494\ttest-mlogloss:0.333969\n",
      "[53]\ttrain-mlogloss:0.268118\ttest-mlogloss:0.333971\n",
      "[54]\ttrain-mlogloss:0.267113\ttest-mlogloss:0.333528\n",
      "[55]\ttrain-mlogloss:0.266108\ttest-mlogloss:0.33315\n",
      "[56]\ttrain-mlogloss:0.265003\ttest-mlogloss:0.33318\n",
      "[57]\ttrain-mlogloss:0.263666\ttest-mlogloss:0.33324\n",
      "[58]\ttrain-mlogloss:0.262536\ttest-mlogloss:0.333168\n",
      "[59]\ttrain-mlogloss:0.261605\ttest-mlogloss:0.333251\n",
      "[60]\ttrain-mlogloss:0.260706\ttest-mlogloss:0.333276\n",
      "[61]\ttrain-mlogloss:0.259881\ttest-mlogloss:0.333145\n",
      "[62]\ttrain-mlogloss:0.259069\ttest-mlogloss:0.33326\n",
      "[63]\ttrain-mlogloss:0.258449\ttest-mlogloss:0.33294\n",
      "[64]\ttrain-mlogloss:0.25778\ttest-mlogloss:0.332959\n",
      "[65]\ttrain-mlogloss:0.256979\ttest-mlogloss:0.332954\n",
      "[66]\ttrain-mlogloss:0.255961\ttest-mlogloss:0.332901\n",
      "[67]\ttrain-mlogloss:0.254913\ttest-mlogloss:0.333017\n",
      "[68]\ttrain-mlogloss:0.254136\ttest-mlogloss:0.332796\n",
      "[69]\ttrain-mlogloss:0.253452\ttest-mlogloss:0.33272\n",
      "Test error using softmax = 0.13006469186244468\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "xg_train=xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test=xgb.DMatrix(X_test, label=y_test)\n",
    "xg_t=xgb.DMatrix(X, label=Y)\n",
    "param={}\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['eta'] = 0.3\n",
    "param['max_depth'] = 3\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 3\n",
    "param['eval_metric']= \"mlogloss\"\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 70\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# get prediction\n",
    "pred = bst.predict(xg_test)\n",
    "error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.835585\ttest-mlogloss:0.837725\n",
      "[1]\ttrain-mlogloss:0.68415\ttest-mlogloss:0.687706\n",
      "[2]\ttrain-mlogloss:0.587061\ttest-mlogloss:0.592043\n",
      "[3]\ttrain-mlogloss:0.521673\ttest-mlogloss:0.527715\n",
      "[4]\ttrain-mlogloss:0.476203\ttest-mlogloss:0.483356\n",
      "[5]\ttrain-mlogloss:0.443035\ttest-mlogloss:0.451809\n",
      "[6]\ttrain-mlogloss:0.419353\ttest-mlogloss:0.429201\n",
      "[7]\ttrain-mlogloss:0.400974\ttest-mlogloss:0.412195\n",
      "[8]\ttrain-mlogloss:0.387214\ttest-mlogloss:0.40011\n",
      "[9]\ttrain-mlogloss:0.375655\ttest-mlogloss:0.39004\n",
      "[10]\ttrain-mlogloss:0.366574\ttest-mlogloss:0.382388\n",
      "[11]\ttrain-mlogloss:0.358829\ttest-mlogloss:0.376405\n",
      "[12]\ttrain-mlogloss:0.352795\ttest-mlogloss:0.371874\n",
      "[13]\ttrain-mlogloss:0.347295\ttest-mlogloss:0.367584\n",
      "[14]\ttrain-mlogloss:0.342647\ttest-mlogloss:0.36442\n",
      "[15]\ttrain-mlogloss:0.338423\ttest-mlogloss:0.361727\n",
      "[16]\ttrain-mlogloss:0.334811\ttest-mlogloss:0.359318\n",
      "[17]\ttrain-mlogloss:0.331139\ttest-mlogloss:0.35714\n",
      "[18]\ttrain-mlogloss:0.328364\ttest-mlogloss:0.355447\n",
      "[19]\ttrain-mlogloss:0.325305\ttest-mlogloss:0.352994\n",
      "[20]\ttrain-mlogloss:0.322119\ttest-mlogloss:0.350542\n",
      "[21]\ttrain-mlogloss:0.319794\ttest-mlogloss:0.349249\n",
      "[22]\ttrain-mlogloss:0.316907\ttest-mlogloss:0.347616\n",
      "[23]\ttrain-mlogloss:0.314426\ttest-mlogloss:0.346598\n",
      "[24]\ttrain-mlogloss:0.311809\ttest-mlogloss:0.345387\n",
      "[25]\ttrain-mlogloss:0.309887\ttest-mlogloss:0.344942\n",
      "[26]\ttrain-mlogloss:0.308003\ttest-mlogloss:0.343978\n",
      "[27]\ttrain-mlogloss:0.305548\ttest-mlogloss:0.34336\n",
      "[28]\ttrain-mlogloss:0.303932\ttest-mlogloss:0.34238\n",
      "[29]\ttrain-mlogloss:0.302133\ttest-mlogloss:0.342035\n",
      "[30]\ttrain-mlogloss:0.300035\ttest-mlogloss:0.341176\n",
      "[31]\ttrain-mlogloss:0.298006\ttest-mlogloss:0.340698\n",
      "[32]\ttrain-mlogloss:0.295826\ttest-mlogloss:0.33986\n",
      "[33]\ttrain-mlogloss:0.294698\ttest-mlogloss:0.339788\n",
      "[34]\ttrain-mlogloss:0.292917\ttest-mlogloss:0.33952\n",
      "[35]\ttrain-mlogloss:0.290839\ttest-mlogloss:0.338726\n",
      "[36]\ttrain-mlogloss:0.288822\ttest-mlogloss:0.338372\n",
      "[37]\ttrain-mlogloss:0.287194\ttest-mlogloss:0.338032\n",
      "[38]\ttrain-mlogloss:0.285623\ttest-mlogloss:0.337184\n",
      "[39]\ttrain-mlogloss:0.284408\ttest-mlogloss:0.336862\n",
      "[40]\ttrain-mlogloss:0.28334\ttest-mlogloss:0.336597\n",
      "[41]\ttrain-mlogloss:0.282424\ttest-mlogloss:0.336453\n",
      "[42]\ttrain-mlogloss:0.281122\ttest-mlogloss:0.336237\n",
      "[43]\ttrain-mlogloss:0.280143\ttest-mlogloss:0.335983\n",
      "[44]\ttrain-mlogloss:0.278748\ttest-mlogloss:0.335791\n",
      "[45]\ttrain-mlogloss:0.27758\ttest-mlogloss:0.33524\n",
      "[46]\ttrain-mlogloss:0.276401\ttest-mlogloss:0.335061\n",
      "[47]\ttrain-mlogloss:0.275483\ttest-mlogloss:0.334842\n",
      "[48]\ttrain-mlogloss:0.273968\ttest-mlogloss:0.334702\n",
      "[49]\ttrain-mlogloss:0.272973\ttest-mlogloss:0.334426\n",
      "[50]\ttrain-mlogloss:0.271775\ttest-mlogloss:0.334399\n",
      "[51]\ttrain-mlogloss:0.270336\ttest-mlogloss:0.334156\n",
      "[52]\ttrain-mlogloss:0.269494\ttest-mlogloss:0.333969\n",
      "[53]\ttrain-mlogloss:0.268118\ttest-mlogloss:0.333971\n",
      "[54]\ttrain-mlogloss:0.267113\ttest-mlogloss:0.333528\n",
      "[55]\ttrain-mlogloss:0.266108\ttest-mlogloss:0.33315\n",
      "[56]\ttrain-mlogloss:0.265003\ttest-mlogloss:0.33318\n",
      "[57]\ttrain-mlogloss:0.263666\ttest-mlogloss:0.33324\n",
      "[58]\ttrain-mlogloss:0.262536\ttest-mlogloss:0.333168\n",
      "[59]\ttrain-mlogloss:0.261605\ttest-mlogloss:0.333251\n",
      "[60]\ttrain-mlogloss:0.260706\ttest-mlogloss:0.333276\n",
      "[61]\ttrain-mlogloss:0.259881\ttest-mlogloss:0.333145\n",
      "[62]\ttrain-mlogloss:0.259069\ttest-mlogloss:0.33326\n",
      "[63]\ttrain-mlogloss:0.258449\ttest-mlogloss:0.33294\n",
      "[64]\ttrain-mlogloss:0.25778\ttest-mlogloss:0.332959\n",
      "[65]\ttrain-mlogloss:0.256979\ttest-mlogloss:0.332954\n",
      "[66]\ttrain-mlogloss:0.255961\ttest-mlogloss:0.332901\n",
      "[67]\ttrain-mlogloss:0.254913\ttest-mlogloss:0.333017\n",
      "[68]\ttrain-mlogloss:0.254136\ttest-mlogloss:0.332796\n",
      "[69]\ttrain-mlogloss:0.253452\ttest-mlogloss:0.33272\n",
      "Test error using softprob = 0.13006469186244468\n"
     ]
    }
   ],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_test).reshape(y_test.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != y_test) / y_test.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.836322\ttest-mlogloss:0.835926\n",
      "[1]\ttrain-mlogloss:0.685431\ttest-mlogloss:0.684692\n",
      "[2]\ttrain-mlogloss:0.588655\ttest-mlogloss:0.587662\n",
      "[3]\ttrain-mlogloss:0.523748\ttest-mlogloss:0.522269\n",
      "[4]\ttrain-mlogloss:0.478684\ttest-mlogloss:0.476855\n",
      "[5]\ttrain-mlogloss:0.44625\ttest-mlogloss:0.444233\n",
      "[6]\ttrain-mlogloss:0.423007\ttest-mlogloss:0.42076\n",
      "[7]\ttrain-mlogloss:0.404697\ttest-mlogloss:0.402665\n",
      "[8]\ttrain-mlogloss:0.391392\ttest-mlogloss:0.389223\n",
      "[9]\ttrain-mlogloss:0.380203\ttest-mlogloss:0.378347\n",
      "[10]\ttrain-mlogloss:0.371416\ttest-mlogloss:0.369665\n",
      "[11]\ttrain-mlogloss:0.36354\ttest-mlogloss:0.362414\n",
      "[12]\ttrain-mlogloss:0.357645\ttest-mlogloss:0.356544\n",
      "[13]\ttrain-mlogloss:0.352108\ttest-mlogloss:0.351681\n",
      "[14]\ttrain-mlogloss:0.347992\ttest-mlogloss:0.347653\n",
      "[15]\ttrain-mlogloss:0.343259\ttest-mlogloss:0.343469\n",
      "[16]\ttrain-mlogloss:0.33965\ttest-mlogloss:0.340181\n",
      "[17]\ttrain-mlogloss:0.336416\ttest-mlogloss:0.33691\n",
      "[18]\ttrain-mlogloss:0.333316\ttest-mlogloss:0.334061\n",
      "[19]\ttrain-mlogloss:0.33014\ttest-mlogloss:0.331316\n",
      "[20]\ttrain-mlogloss:0.327775\ttest-mlogloss:0.328821\n",
      "[21]\ttrain-mlogloss:0.325236\ttest-mlogloss:0.326021\n",
      "[22]\ttrain-mlogloss:0.323189\ttest-mlogloss:0.324142\n",
      "[23]\ttrain-mlogloss:0.321147\ttest-mlogloss:0.322247\n",
      "[24]\ttrain-mlogloss:0.31887\ttest-mlogloss:0.319957\n",
      "[25]\ttrain-mlogloss:0.31675\ttest-mlogloss:0.317883\n",
      "[26]\ttrain-mlogloss:0.314714\ttest-mlogloss:0.315647\n",
      "[27]\ttrain-mlogloss:0.313267\ttest-mlogloss:0.314106\n",
      "[28]\ttrain-mlogloss:0.311246\ttest-mlogloss:0.312123\n",
      "[29]\ttrain-mlogloss:0.309978\ttest-mlogloss:0.311019\n",
      "[30]\ttrain-mlogloss:0.308091\ttest-mlogloss:0.309669\n",
      "[31]\ttrain-mlogloss:0.30635\ttest-mlogloss:0.308246\n",
      "[32]\ttrain-mlogloss:0.304977\ttest-mlogloss:0.306805\n",
      "[33]\ttrain-mlogloss:0.303649\ttest-mlogloss:0.305427\n",
      "[34]\ttrain-mlogloss:0.301833\ttest-mlogloss:0.304114\n",
      "[35]\ttrain-mlogloss:0.300341\ttest-mlogloss:0.302594\n",
      "[36]\ttrain-mlogloss:0.298998\ttest-mlogloss:0.3018\n",
      "[37]\ttrain-mlogloss:0.297597\ttest-mlogloss:0.300622\n",
      "[38]\ttrain-mlogloss:0.296127\ttest-mlogloss:0.298979\n",
      "[39]\ttrain-mlogloss:0.294618\ttest-mlogloss:0.297674\n",
      "[40]\ttrain-mlogloss:0.293492\ttest-mlogloss:0.296886\n",
      "[41]\ttrain-mlogloss:0.292332\ttest-mlogloss:0.295314\n",
      "[42]\ttrain-mlogloss:0.291766\ttest-mlogloss:0.294671\n",
      "[43]\ttrain-mlogloss:0.290749\ttest-mlogloss:0.293647\n",
      "[44]\ttrain-mlogloss:0.289886\ttest-mlogloss:0.292794\n",
      "[45]\ttrain-mlogloss:0.288734\ttest-mlogloss:0.291803\n",
      "[46]\ttrain-mlogloss:0.287381\ttest-mlogloss:0.290499\n",
      "[47]\ttrain-mlogloss:0.286082\ttest-mlogloss:0.289243\n",
      "[48]\ttrain-mlogloss:0.284995\ttest-mlogloss:0.288117\n",
      "[49]\ttrain-mlogloss:0.283919\ttest-mlogloss:0.287184\n",
      "[50]\ttrain-mlogloss:0.283154\ttest-mlogloss:0.286326\n",
      "[51]\ttrain-mlogloss:0.282257\ttest-mlogloss:0.285648\n",
      "[52]\ttrain-mlogloss:0.281516\ttest-mlogloss:0.285093\n",
      "[53]\ttrain-mlogloss:0.280547\ttest-mlogloss:0.284135\n",
      "[54]\ttrain-mlogloss:0.279448\ttest-mlogloss:0.283124\n",
      "[55]\ttrain-mlogloss:0.278542\ttest-mlogloss:0.282395\n",
      "[56]\ttrain-mlogloss:0.277664\ttest-mlogloss:0.281671\n",
      "[57]\ttrain-mlogloss:0.276773\ttest-mlogloss:0.280595\n",
      "[58]\ttrain-mlogloss:0.27612\ttest-mlogloss:0.280066\n",
      "[59]\ttrain-mlogloss:0.275266\ttest-mlogloss:0.279269\n",
      "[60]\ttrain-mlogloss:0.27428\ttest-mlogloss:0.278572\n",
      "[61]\ttrain-mlogloss:0.273579\ttest-mlogloss:0.27791\n",
      "[62]\ttrain-mlogloss:0.272843\ttest-mlogloss:0.276894\n",
      "[63]\ttrain-mlogloss:0.271846\ttest-mlogloss:0.275578\n",
      "[64]\ttrain-mlogloss:0.271044\ttest-mlogloss:0.275078\n",
      "[65]\ttrain-mlogloss:0.269882\ttest-mlogloss:0.274101\n",
      "[66]\ttrain-mlogloss:0.269187\ttest-mlogloss:0.273479\n",
      "[67]\ttrain-mlogloss:0.268684\ttest-mlogloss:0.272792\n",
      "[68]\ttrain-mlogloss:0.268108\ttest-mlogloss:0.272234\n",
      "[69]\ttrain-mlogloss:0.267174\ttest-mlogloss:0.271338\n",
      "Test error using softprob = 0.10720670105725522\n"
     ]
    }
   ],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_t, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_t).reshape(Y.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != Y) / Y.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_13</th>\n",
       "      <th>svd_word_14</th>\n",
       "      <th>svd_word_15</th>\n",
       "      <th>svd_word_16</th>\n",
       "      <th>svd_word_17</th>\n",
       "      <th>svd_word_18</th>\n",
       "      <th>svd_word_19</th>\n",
       "      <th>nb_cvec_eap</th>\n",
       "      <th>nb_cvec_hpl</th>\n",
       "      <th>nb_cvec_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011837</td>\n",
       "      <td>0.036064</td>\n",
       "      <td>-0.016591</td>\n",
       "      <td>-0.025580</td>\n",
       "      <td>-0.018785</td>\n",
       "      <td>0.031289</td>\n",
       "      <td>-0.047220</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.978387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004397</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.008583</td>\n",
       "      <td>0.006335</td>\n",
       "      <td>-0.004216</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006063</td>\n",
       "      <td>-0.003324</td>\n",
       "      <td>-0.009452</td>\n",
       "      <td>0.013239</td>\n",
       "      <td>0.004852</td>\n",
       "      <td>-0.007478</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.217325</td>\n",
       "      <td>0.782527</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7     ...       svd_word_13  svd_word_14  svd_word_15  \\\n",
       "0    0.003240     ...         -0.011837     0.036064    -0.016591   \n",
       "1   -0.000215     ...         -0.004397    -0.000020    -0.008583   \n",
       "2   -0.004598     ...          0.006063    -0.003324    -0.009452   \n",
       "\n",
       "   svd_word_16  svd_word_17  svd_word_18  svd_word_19  nb_cvec_eap  \\\n",
       "0    -0.025580    -0.018785     0.031289    -0.047220     0.021018   \n",
       "1     0.006335    -0.004216     0.001810     0.001767     0.999985   \n",
       "2     0.013239     0.004852    -0.007478     0.002786     0.217325   \n",
       "\n",
       "   nb_cvec_hpl  nb_cvec_mws  \n",
       "0     0.000595     0.978387  \n",
       "1     0.000009     0.000006  \n",
       "2     0.782527     0.000148  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>[still, i, urg, leav, ireland, inquietud, impa...</td>\n",
       "      <td>still i urg leav ireland inquietud impati fath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>[if, fire, want, fan, readili, fan, newspap, g...</td>\n",
       "      <td>if fire want fan readili fan newspap govern gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>[and, broken, frail, door, found, two, clean, ...</td>\n",
       "      <td>and broken frail door found two clean pick hum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [still, i, urg, leav, ireland, inquietud, impa...   \n",
       "1  [if, fire, want, fan, readili, fan, newspap, g...   \n",
       "2  [and, broken, frail, door, found, two, clean, ...   \n",
       "\n",
       "                                 cleaned_text_string  \n",
       "0  still i urg leav ireland inquietud impati fath...  \n",
       "1  if fire want fan readili fan newspap govern gr...  \n",
       "2  and broken frail door found two clean pick hum...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['cleaned_text'] = df_test.text.apply(tokenize_stem)\n",
    "df_test['cleaned_text_string'] = df_test.cleaned_text.apply(' '.join)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'text', 'cleaned_text', 'cleaned_text_string', 'length',\n",
       "       'num_words', 'num_unique_words', 'num_punctuations',\n",
       "       'num_words_upper', 'num_words_title', 'mean_word_len',\n",
       "       'num_stopwords', 'lexical_diversity', 'even', 'seem', 'friend',\n",
       "       'thought', 'hand', 'hope', 'fear', 'mind', 'ever', 'place', 'feel',\n",
       "       'room', 'natur', 'everi', 'face', 'happi', 'love', 'death', 'heard',\n",
       "       'heart', 'raymond', 'chang', 'life', 'like', 'still', 'strang',\n",
       "       'much', 'night', 'two', 'window', 'saw', 'made', 'men', 'mani',\n",
       "       'must', 'appear', 'never', 'thus', 'dark', 'know', 'eye', 'pass',\n",
       "       'howev', 'dream', 'shall', 'man', 'us', 'long', 'might', 'yet',\n",
       "       'look', 'light', 'die', 'old', 'may', 'first', 'street', 'say',\n",
       "       'well', 'upon', 'one', 'father', 'come', 'thing', 'time', 'word',\n",
       "       'though', 'day', 'hous', 'return', 'great', 'year', 'littl',\n",
       "       'whose', 'came', 'said', 'see', 'door', 'certain', 'found'], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['author2', 'id', 'text', 'author', 'cleaned_text',\n",
       "       'cleaned_text_string', 'length', 'num_words', 'num_unique_words',\n",
       "       'num_punctuations', 'num_words_upper', 'num_words_title',\n",
       "       'mean_word_len', 'num_stopwords', 'lexical_diversity', 'even',\n",
       "       'seem', 'friend', 'thought', 'hand', 'hope', 'fear', 'mind', 'ever',\n",
       "       'place', 'feel', 'room', 'natur', 'everi', 'face', 'happi', 'love',\n",
       "       'death', 'heard', 'heart', 'raymond', 'chang', 'life', 'like',\n",
       "       'still', 'strang', 'much', 'night', 'two', 'window', 'saw', 'made',\n",
       "       'men', 'mani', 'must', 'appear', 'never', 'thus', 'dark', 'know',\n",
       "       'eye', 'pass', 'howev', 'dream', 'shall', 'man', 'us', 'long',\n",
       "       'might', 'yet', 'look', 'light', 'die', 'old', 'may', 'first',\n",
       "       'street', 'say', 'well', 'upon', 'one', 'father', 'come', 'thing',\n",
       "       'time', 'word', 'though', 'day', 'hous', 'return', 'great', 'year',\n",
       "       'littl', 'whose', 'came', 'said', 'see', 'door', 'certain', 'found',\n",
       "       'mws_index', 'eap_index', 'hpl_index'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>...</th>\n",
       "      <th>great</th>\n",
       "      <th>year</th>\n",
       "      <th>littl</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>[still, i, urg, leav, ireland, inquietud, impa...</td>\n",
       "      <td>still i urg leav ireland inquietud impati fath...</td>\n",
       "      <td>67</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>[if, fire, want, fan, readili, fan, newspap, g...</td>\n",
       "      <td>if fire want fan readili fan newspap govern gr...</td>\n",
       "      <td>175</td>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>[and, broken, frail, door, found, two, clean, ...</td>\n",
       "      <td>and broken frail door found two clean pick hum...</td>\n",
       "      <td>114</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [still, i, urg, leav, ireland, inquietud, impa...   \n",
       "1  [if, fire, want, fan, readili, fan, newspap, g...   \n",
       "2  [and, broken, frail, door, found, two, clean, ...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  still i urg leav ireland inquietud impati fath...      67         19   \n",
       "1  if fire want fan readili fan newspap govern gr...     175         62   \n",
       "2  and broken frail door found two clean pick hum...     114         33   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper  num_words_title  \\\n",
       "0                19                 3                1                3   \n",
       "1                49                 7                1                3   \n",
       "2                30                 3                0                1   \n",
       "\n",
       "   ...    great  year  littl  whose  came  said  see  door  certain  found  \n",
       "0  ...        0     0      0      0     0     0    0     0        0      0  \n",
       "1  ...        0     0      0      0     0     0    0     0        0      0  \n",
       "2  ...        0     0      0      0     0     0    0     1        0      1  \n",
       "\n",
       "[3 rows x 93 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_test['length']=df_test['cleaned_text_string'].apply(len)\n",
    "df_test[\"num_words\"] = df_test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_test[\"num_unique_words\"] = df_test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_test[\"num_punctuations\"] =df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test[\"num_words_upper\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_test[\"num_words_title\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_test[\"mean_word_len\"] = df_test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test[\"num_stopwords\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_test['lexical_diversity'] = df_test.text.apply(lexical_diversity)\n",
    "count_topwords(df_test)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>...</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>[still, i, urg, leav, ireland, inquietud, impa...</td>\n",
       "      <td>still i urg leav ireland inquietud impati fath...</td>\n",
       "      <td>67</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071473</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>0.026541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>[if, fire, want, fan, readili, fan, newspap, g...</td>\n",
       "      <td>if fire want fan readili fan newspap govern gr...</td>\n",
       "      <td>175</td>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035520</td>\n",
       "      <td>0.063073</td>\n",
       "      <td>0.038550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>[and, broken, frail, door, found, two, clean, ...</td>\n",
       "      <td>and broken frail door found two clean pick hum...</td>\n",
       "      <td>114</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.057723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [still, i, urg, leav, ireland, inquietud, impa...   \n",
       "1  [if, fire, want, fan, readili, fan, newspap, g...   \n",
       "2  [and, broken, frail, door, found, two, clean, ...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  still i urg leav ireland inquietud impati fath...      67         19   \n",
       "1  if fire want fan readili fan newspap govern gr...     175         62   \n",
       "2  and broken frail door found two clean pick hum...     114         33   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper  num_words_title  \\\n",
       "0                19                 3                1                3   \n",
       "1                49                 7                1                3   \n",
       "2                30                 3                0                1   \n",
       "\n",
       "     ...      whose  came  said  see  door  certain  found  mws_index  \\\n",
       "0    ...          0     0     0    0     0        0      0   0.071473   \n",
       "1    ...          0     0     0    0     0        0      0   0.035520   \n",
       "2    ...          0     0     0    0     1        0      1   0.027486   \n",
       "\n",
       "   eap_index  hpl_index  \n",
       "0   0.036315   0.026541  \n",
       "1   0.063073   0.038550  \n",
       "2   0.055142   0.057723  \n",
       "\n",
       "[3 rows x 96 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['mws_index']=df_test['cleaned_text'].apply(ind_val_mws)/df_test['length']\n",
    "df_test['eap_index']=df_test['cleaned_text'].apply(ind_val_eap)/df_test['length']\n",
    "df_test['hpl_index']=df_test['cleaned_text'].apply(ind_val_hpl)/df_test['length']\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_test['cleaned_text']\n",
    "del df_test['cleaned_text_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author2', 'author', 'cleaned_text', 'cleaned_text_string']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=(df_train.columns.tolist())[6:]\n",
    "[item for item in df_train.columns.tolist() if item not in df_test.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_13</th>\n",
       "      <th>svd_word_14</th>\n",
       "      <th>svd_word_15</th>\n",
       "      <th>svd_word_16</th>\n",
       "      <th>svd_word_17</th>\n",
       "      <th>svd_word_18</th>\n",
       "      <th>svd_word_19</th>\n",
       "      <th>nb_cvec_eap</th>\n",
       "      <th>nb_cvec_hpl</th>\n",
       "      <th>nb_cvec_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>67</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.842105</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011837</td>\n",
       "      <td>0.036064</td>\n",
       "      <td>-0.016591</td>\n",
       "      <td>-0.025580</td>\n",
       "      <td>-0.018785</td>\n",
       "      <td>0.031289</td>\n",
       "      <td>-0.047220</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.978387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>175</td>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.338710</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004397</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.008583</td>\n",
       "      <td>0.006335</td>\n",
       "      <td>-0.004216</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>114</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.757576</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006063</td>\n",
       "      <td>-0.003324</td>\n",
       "      <td>-0.009452</td>\n",
       "      <td>0.013239</td>\n",
       "      <td>0.004852</td>\n",
       "      <td>-0.007478</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.217325</td>\n",
       "      <td>0.782527</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "      <td>124</td>\n",
       "      <td>41</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.463415</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>-0.006865</td>\n",
       "      <td>-0.007960</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>-0.004558</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>0.753591</td>\n",
       "      <td>0.246408</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.909091</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001825</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>-0.006504</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>-0.004004</td>\n",
       "      <td>-0.002902</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.970950</td>\n",
       "      <td>0.021824</td>\n",
       "      <td>0.007226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  length  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...      67   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...     175   \n",
       "2  id00134  And when they had broken down the frail door t...     114   \n",
       "3  id27757  While I was thinking how I should possibly man...     124   \n",
       "4  id04081  I am not sure to what limit his knowledge may ...      32   \n",
       "\n",
       "   num_words  num_unique_words  num_punctuations  num_words_upper  \\\n",
       "0         19                19                 3                1   \n",
       "1         62                49                 7                1   \n",
       "2         33                30                 3                0   \n",
       "3         41                34                 5                2   \n",
       "4         11                11                 1                1   \n",
       "\n",
       "   num_words_title  mean_word_len  num_stopwords     ...       svd_word_13  \\\n",
       "0                3       4.842105              9     ...         -0.011837   \n",
       "1                3       4.338710             34     ...         -0.004397   \n",
       "2                1       4.757576             15     ...          0.006063   \n",
       "3                3       4.463415             19     ...          0.004783   \n",
       "4                1       3.909091              6     ...         -0.001825   \n",
       "\n",
       "   svd_word_14  svd_word_15  svd_word_16  svd_word_17  svd_word_18  \\\n",
       "0     0.036064    -0.016591    -0.025580    -0.018785     0.031289   \n",
       "1    -0.000020    -0.008583     0.006335    -0.004216     0.001810   \n",
       "2    -0.003324    -0.009452     0.013239     0.004852    -0.007478   \n",
       "3    -0.006865    -0.007960     0.006763     0.002540    -0.004558   \n",
       "4     0.000123    -0.006504     0.002533    -0.004004    -0.002902   \n",
       "\n",
       "   svd_word_19  nb_cvec_eap  nb_cvec_hpl  nb_cvec_mws  \n",
       "0    -0.047220     0.021018     0.000595     0.978387  \n",
       "1     0.001767     0.999985     0.000009     0.000006  \n",
       "2     0.002786     0.217325     0.782527     0.000148  \n",
       "3    -0.000728     0.753591     0.246408     0.000001  \n",
       "4     0.000315     0.970950     0.021824     0.007226  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_test=df_test[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_t=ds_test[:, :]\n",
    "y_t=df_test['id'].values\n",
    "xg_t=xgb.DMatrix(x_t)\n",
    "pred_prob = bstp.predict(xg_t).reshape(y_t.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.07466558e-02   4.53274138e-03   9.74720597e-01]\n",
      " [  9.95783210e-01   2.83912686e-03   1.37771375e-03]\n",
      " [  1.08631335e-01   8.89534473e-01   1.83415657e-03]\n",
      " ..., \n",
      " [  9.35469210e-01   2.85144355e-02   3.60163823e-02]\n",
      " [  1.29575823e-02   2.84086540e-03   9.84201610e-01]\n",
      " [  8.50096811e-03   9.90899801e-01   5.99243445e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>0.974721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.001378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.108631</td>\n",
       "      <td>0.889534</td>\n",
       "      <td>0.001834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.842110</td>\n",
       "      <td>0.156198</td>\n",
       "      <td>0.001692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.972890</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>0.008357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.020747  0.004533  0.974721\n",
       "1  id24541  0.995783  0.002839  0.001378\n",
       "2  id00134  0.108631  0.889534  0.001834\n",
       "3  id27757  0.842110  0.156198  0.001692\n",
       "4  id04081  0.972890  0.018753  0.008357"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export=pd.DataFrame(pred_prob)\n",
    "export.insert(loc=0, column='id', value=y_t)\n",
    "export.columns=['id','EAP', 'HPL', 'MWS']\n",
    "export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8392"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>id23301</td>\n",
       "      <td>0.175089</td>\n",
       "      <td>0.22535</td>\n",
       "      <td>0.59956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       EAP      HPL      MWS\n",
       "6106  id23301  0.175089  0.22535  0.59956"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export[export['id']=='id23301']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export.to_csv(path_or_buf=\"../data/export.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
