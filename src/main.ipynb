{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tnsuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tnsuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tnsuser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from string import digits\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes, pipeline\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from stop_words import get_stop_words\n",
    "import gensim\n",
    "import re\n",
    "from gensim.models import ldamodel as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "# 3 столбца - id, text, author\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# нулей не должно быть!\n",
    "\n",
    "\n",
    "def sum_up_word2vec_array(clnd_text):\n",
    "    \n",
    "    total = None\n",
    "    \n",
    "    for word in clnd_text:\n",
    "        if word in w2v:\n",
    "            if total is None:\n",
    "                total = w2v[word]\n",
    "            else:\n",
    "                total = np.add(total, w2v[word])\n",
    "                \n",
    "    if total is None:\n",
    "        return np.zeros(w2v_array_len)\n",
    "    else:\n",
    "        return total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# нулей не должно быть!\n",
    "\n",
    "\n",
    "def avg_up_word2vec_array(clnd_text):\n",
    "    \n",
    "    total = None\n",
    "    \n",
    "    for word in clnd_text:\n",
    "        if word in w2v:\n",
    "            if total is None:\n",
    "                total = w2v[word]\n",
    "            else:                \n",
    "                total = np.mean([total, w2v[word]], axis=0)\n",
    "                \n",
    "                \n",
    "    if total is None:\n",
    "        return np.zeros(w2v_array_len)\n",
    "    else:\n",
    "        return total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "def tokenize_stem(file_text):\n",
    "    #firstly let's apply nltk tokenization\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #deleting stop_words\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens = [i for i in tokens if ( i not in eng_stopwords )]\n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['cleaned_text'] = df_train.text.apply(tokenize_stem)\n",
    "df_train['cleaned_text_string'] = df_train.cleaned_text.apply(' '.join)\n",
    "df_train.head(n=3)\n",
    "eng_stopwords = set(stopwords.words('english')).union(set(get_stop_words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " \"how's\",\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " \"let's\",\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'when',\n",
       " \"when's\",\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " \"who's\",\n",
       " 'whom',\n",
       " 'why',\n",
       " \"why's\",\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'would',\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(file_text):\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "    except:\n",
    "        nltk.download('punkt')\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "        \n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return len(set(tokens))/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[matrix([[ 0.00053369,  0.00053369,  0.00106738, ...,  0.00053369,\n",
      "          0.        ,  0.        ]]), matrix([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "          0.00135189,  0.00067595]]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "# вытаскиваем \"значимые\" слова\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "raw_documents_authors = ['', '', '']\n",
    "\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    if row['author'] == 'EAP':\n",
    "        raw_documents_authors[0] += row['cleaned_text_string'] + ' '\n",
    "    elif row['author'] == 'HPL':\n",
    "        raw_documents_authors[1] += row['cleaned_text_string'] + ' '\n",
    "    else:\n",
    "        raw_documents_authors[2] += row['cleaned_text_string'] + ' '\n",
    "        \n",
    "\n",
    "# удалим уникальные слова, не встречающиеся у других писателей\n",
    "\n",
    "eap_only = set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "hpl_only = set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "msh_only = set(raw_documents_authors[2].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' '))\n",
    "\n",
    "unique_words = eap_only.union(hpl_only).union(msh_only)\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word')\n",
    "idf_matrix =  tf.fit_transform(raw_documents_authors)\n",
    "feature_names = tf.get_feature_names()\n",
    "# dictionary_word = dict(zip(feature_names, idf_matrix))\n",
    "\n",
    "dense_idf = [i.todense() for i in idf_matrix]\n",
    "print(dense_idf)\n",
    "\n",
    "max_weighted_term = []\n",
    "\n",
    "eap_dense_list = dense_idf[0].tolist()[0]\n",
    "hpl_dense_list = dense_idf[1].tolist()[0]\n",
    "mws_dense_list = dense_idf[2].tolist()[0]\n",
    "\n",
    "for inum, i in enumerate(eap_dense_list):\n",
    "    max_weighted_term.append(max(hpl_dense_list[inum], mws_dense_list[inum], \n",
    "                             i))\n",
    "\n",
    "max_tf_dict = dict(zip(feature_names, max_weighted_term))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'millston',\n",
       " 'fum',\n",
       " 'sedul',\n",
       " 'bruis',\n",
       " 'jeremiad',\n",
       " 'bluddennuff',\n",
       " 'ververt',\n",
       " 'amulet',\n",
       " 'transgress',\n",
       " 'untru',\n",
       " 'inadvert',\n",
       " 'strow',\n",
       " 'maritim',\n",
       " 'romanis',\n",
       " 'clinic',\n",
       " 'chronicl',\n",
       " 'scarciti',\n",
       " 'parièr',\n",
       " \"beatin'est\",\n",
       " 'incivil',\n",
       " 'ligeia',\n",
       " 'nthlei',\n",
       " 'bier',\n",
       " 'pinxit',\n",
       " 'ast',\n",
       " 'vini',\n",
       " 'yer',\n",
       " 'ensanguin',\n",
       " 'spatial',\n",
       " 'genera',\n",
       " 'lamma',\n",
       " 'lath',\n",
       " 'homicid',\n",
       " 'favorit',\n",
       " 'unworldli',\n",
       " 'psych',\n",
       " 'inflat',\n",
       " 'thistl',\n",
       " 'ambul',\n",
       " 'incompat',\n",
       " 'seekest',\n",
       " 'pentland',\n",
       " 'emblazon',\n",
       " 'hade',\n",
       " \"lower'n\",\n",
       " 'outgrown',\n",
       " 'giovanni',\n",
       " 'unreprov',\n",
       " 'dubbl',\n",
       " 'entereth',\n",
       " 'mehitabel',\n",
       " 'valetudinarian',\n",
       " 'veal',\n",
       " 'controul',\n",
       " 'othair',\n",
       " 'sequenc',\n",
       " 'paower',\n",
       " 'lombard',\n",
       " 'florentin',\n",
       " 'alderney',\n",
       " 'plouton',\n",
       " 'incent',\n",
       " 'sashless',\n",
       " 'piltdown',\n",
       " 'alcmaeon',\n",
       " 'roodma',\n",
       " 'beldam',\n",
       " 'caw',\n",
       " 'biolog',\n",
       " 'janus',\n",
       " 'irrat',\n",
       " 'tiara',\n",
       " 'archaism',\n",
       " 'parapet',\n",
       " 'surcingl',\n",
       " 'unmor',\n",
       " 'schiller',\n",
       " 'bridal',\n",
       " 'sundri',\n",
       " 'parker',\n",
       " 'gyratori',\n",
       " 'jabber',\n",
       " 'kalend',\n",
       " 'alexandr',\n",
       " \"n'est\",\n",
       " 'saffron',\n",
       " 'lavish',\n",
       " 'whitman',\n",
       " 'teloth',\n",
       " 'invert',\n",
       " 'bearabl',\n",
       " 'motivirt',\n",
       " 'foibl',\n",
       " 'buttress',\n",
       " 'madhous',\n",
       " 'edwin',\n",
       " 'guiltless',\n",
       " 'knickerbock',\n",
       " 'ut',\n",
       " 'tumbler',\n",
       " 'daisi',\n",
       " 'seced',\n",
       " 'finicki',\n",
       " 'landaff',\n",
       " 'tu',\n",
       " 'turribl',\n",
       " 'paragrab',\n",
       " 'theoriz',\n",
       " 'surli',\n",
       " \"o'bump\",\n",
       " 'shuttlecock',\n",
       " 'writ',\n",
       " 'freemasonri',\n",
       " 'bouffon',\n",
       " 'cain',\n",
       " 'simian',\n",
       " 'storag',\n",
       " 'cometari',\n",
       " 'gull',\n",
       " \"r'lyeh\",\n",
       " 'anthil',\n",
       " 'realist',\n",
       " 'tendril',\n",
       " 'berth',\n",
       " 'blear',\n",
       " 'punctur',\n",
       " 'perdita',\n",
       " 'lachad',\n",
       " 'mus',\n",
       " 'pratol',\n",
       " 'bloat',\n",
       " 'sanskrit',\n",
       " 'sobr',\n",
       " 'flint',\n",
       " 'debaucheri',\n",
       " 'frederick',\n",
       " 'brandish',\n",
       " 'ed',\n",
       " 'crate',\n",
       " 'amphibian',\n",
       " 'postul',\n",
       " 'discont',\n",
       " 'papillon',\n",
       " 'claudius',\n",
       " 'oasi',\n",
       " \"cap'n\",\n",
       " 'voodoo',\n",
       " 'mahmoud',\n",
       " 'saunter',\n",
       " 'ce',\n",
       " 'opportunit',\n",
       " 'unforest',\n",
       " 'usuri',\n",
       " 'quarri',\n",
       " 'scientist',\n",
       " 'bornes',\n",
       " 'tappa',\n",
       " 'blockad',\n",
       " 'antigon',\n",
       " 'nyarlathotep',\n",
       " 'funest',\n",
       " 'baulk',\n",
       " 'fopperi',\n",
       " 'liven',\n",
       " 'hanger',\n",
       " 'elliot',\n",
       " 'repeopl',\n",
       " 'pseudo',\n",
       " 'chalde',\n",
       " 'fraser',\n",
       " 'burthensom',\n",
       " 'grayish',\n",
       " 'shill',\n",
       " 'disingenu',\n",
       " 'jine',\n",
       " 'fairer',\n",
       " 'duquel',\n",
       " 'utica',\n",
       " 'gx',\n",
       " 'mebb',\n",
       " 'incandesc',\n",
       " 'unloveli',\n",
       " 'fac',\n",
       " \"n'bangus\",\n",
       " 'acet',\n",
       " 'unborn',\n",
       " 'silliman',\n",
       " 'analog',\n",
       " 'iri',\n",
       " 'forebor',\n",
       " 'playth',\n",
       " 'rhyme',\n",
       " 'wellington',\n",
       " 'peaaema',\n",
       " 'præter',\n",
       " 'stael',\n",
       " 'subcenturio',\n",
       " 'upset',\n",
       " 'wrestl',\n",
       " 'ster',\n",
       " 'ambaaren',\n",
       " 'privateerin',\n",
       " 'invers',\n",
       " 'straighten',\n",
       " 'shicken',\n",
       " 'polyphem',\n",
       " 'legislatur',\n",
       " 'ge',\n",
       " 'xpx',\n",
       " 'nevil',\n",
       " 'goin',\n",
       " 'olathoë',\n",
       " 'carcass',\n",
       " 'mentor',\n",
       " 'smitten',\n",
       " 'unbear',\n",
       " 'tallow',\n",
       " 'esoter',\n",
       " 'pageantri',\n",
       " 'pourtray',\n",
       " 'vist',\n",
       " 'ese',\n",
       " 'untarnish',\n",
       " 'gauz',\n",
       " 'viper',\n",
       " 'phaseless',\n",
       " 'unredress',\n",
       " 'puff',\n",
       " 'vd',\n",
       " 'bennett',\n",
       " 'famin',\n",
       " 'lucki',\n",
       " 'binder',\n",
       " 'sefton',\n",
       " 'rapt',\n",
       " 'clam',\n",
       " 'mazurka',\n",
       " 'boswel',\n",
       " 'ibid',\n",
       " 'ritmo',\n",
       " 'fin',\n",
       " 'niggurath',\n",
       " 'leffert',\n",
       " 'oblivi',\n",
       " 'ancona',\n",
       " 'unsurpass',\n",
       " 'demis',\n",
       " 'sawyer',\n",
       " 'autr',\n",
       " 'unprincipl',\n",
       " 'implac',\n",
       " 'morceau',\n",
       " 'outgrowth',\n",
       " 'pail',\n",
       " 'betak',\n",
       " \"y'hah\",\n",
       " 'semit',\n",
       " 'scout',\n",
       " 'anywher',\n",
       " 'outbid',\n",
       " 'misrepresent',\n",
       " 'οἶδα',\n",
       " 'mixin',\n",
       " 'clare',\n",
       " 'unsanct',\n",
       " 'seashor',\n",
       " 'algebraist',\n",
       " 'romero',\n",
       " 'chapeau',\n",
       " 'rariti',\n",
       " 'forum',\n",
       " 'aeolus',\n",
       " 'waveless',\n",
       " 'proven',\n",
       " 'selfconcentr',\n",
       " 'nambi',\n",
       " 'himself',\n",
       " 'vaguest',\n",
       " 'evolv',\n",
       " 'soho',\n",
       " 'colonnad',\n",
       " 'sixpenc',\n",
       " 'hippodrom',\n",
       " 'undecay',\n",
       " 'inan',\n",
       " 'troi',\n",
       " 'subordin',\n",
       " 'exhaustless',\n",
       " 'musiqu',\n",
       " 'luckili',\n",
       " 'alow',\n",
       " 'ducal',\n",
       " 'drinen',\n",
       " 'infern',\n",
       " 'oversensit',\n",
       " 'overspread',\n",
       " 'crébillon',\n",
       " 'dumbarton',\n",
       " 'reverenti',\n",
       " 'bunkhous',\n",
       " 'caustic',\n",
       " 'ami',\n",
       " 'unmanag',\n",
       " 'yxur',\n",
       " 'omnisci',\n",
       " 'diverg',\n",
       " 'slash',\n",
       " 'armington',\n",
       " 'elenchi',\n",
       " 'jerki',\n",
       " 'u.',\n",
       " 'galvez',\n",
       " 'killer',\n",
       " 'kraut',\n",
       " 'recombin',\n",
       " 'shabbi',\n",
       " 'shovel',\n",
       " 'uncas',\n",
       " 'alm',\n",
       " 'lazuli',\n",
       " 'harley',\n",
       " 'livestock',\n",
       " 'worthier',\n",
       " 'overshoot',\n",
       " 'rowel',\n",
       " 'champagn',\n",
       " 'sime',\n",
       " 'trellic',\n",
       " 'collegian',\n",
       " 'leipsic',\n",
       " 'slit',\n",
       " 'crust',\n",
       " 'combattendo',\n",
       " 'mar',\n",
       " 'salamanca',\n",
       " 'drawl',\n",
       " 'concess',\n",
       " 'sorta',\n",
       " 'und',\n",
       " 'donner',\n",
       " 'musta',\n",
       " 'idiomat',\n",
       " 'thingumajig',\n",
       " 'vie',\n",
       " 'guarda',\n",
       " 'linden',\n",
       " 'abdomen',\n",
       " 'enslav',\n",
       " 'curius',\n",
       " 'superstructur',\n",
       " 'defac',\n",
       " 'hallucin',\n",
       " 'edgar',\n",
       " 'irreproach',\n",
       " 'capella',\n",
       " 'milwauke',\n",
       " 'anchorag',\n",
       " 'boi',\n",
       " 'bluish',\n",
       " 'adjur',\n",
       " 'loam',\n",
       " 'bicknel',\n",
       " 'morphin',\n",
       " 'mademoisell',\n",
       " 'ventum',\n",
       " 'angarola',\n",
       " 'rarefi',\n",
       " 'unmix',\n",
       " 'secundus',\n",
       " 'melbourn',\n",
       " 'neapoli',\n",
       " 'taran',\n",
       " 'elat',\n",
       " 'porgi',\n",
       " 'billowi',\n",
       " 'medder',\n",
       " 'mizen',\n",
       " 'codifi',\n",
       " 'phosphor',\n",
       " 'ero',\n",
       " 'bump',\n",
       " \"sep'rit\",\n",
       " 'scotch',\n",
       " 'eleonora',\n",
       " 'abstemi',\n",
       " 'q',\n",
       " 'pensé',\n",
       " 'typographi',\n",
       " 'median',\n",
       " 'commonplac',\n",
       " 'gauntlet',\n",
       " 'betid',\n",
       " 'seul',\n",
       " 'emmon',\n",
       " 'dessein',\n",
       " \"e'yaahhhh\",\n",
       " 'campfir',\n",
       " 'unscrupul',\n",
       " 'salon',\n",
       " 'incom',\n",
       " 'mene',\n",
       " 'surtout',\n",
       " 'unimport',\n",
       " 'auspici',\n",
       " 'recruit',\n",
       " 'botan',\n",
       " 'moreland',\n",
       " 'homouioisio',\n",
       " 'exet',\n",
       " 'poultri',\n",
       " 'hedelin',\n",
       " 'bienseanc',\n",
       " 'mauvai',\n",
       " 'nue',\n",
       " 'ermin',\n",
       " 'rekindl',\n",
       " 'burnish',\n",
       " 'mantelpiec',\n",
       " 'evilli',\n",
       " 'kidd',\n",
       " 'æneid',\n",
       " 'pirouet',\n",
       " 'eusebius',\n",
       " 'sumptuari',\n",
       " 'fob',\n",
       " 'templeton',\n",
       " 'stoneheng',\n",
       " 'ratiocin',\n",
       " 'convivi',\n",
       " 'chastiti',\n",
       " 'chat',\n",
       " 'gaol',\n",
       " 'noblemen',\n",
       " 'bearin',\n",
       " 'misde',\n",
       " 'dign',\n",
       " 'avarici',\n",
       " 'dori',\n",
       " 'engirdl',\n",
       " 'earring',\n",
       " 'bolster',\n",
       " 'publick',\n",
       " 'transcript',\n",
       " 'uncanni',\n",
       " 'evas',\n",
       " 'abbé',\n",
       " 'dunkeld',\n",
       " 'siniv',\n",
       " 'incontest',\n",
       " 'cervant',\n",
       " 'begirt',\n",
       " 'spiritless',\n",
       " 'polyp',\n",
       " 'fugu',\n",
       " 'hombr',\n",
       " 'drollest',\n",
       " 'filius',\n",
       " 'aigrett',\n",
       " 'anticlimact',\n",
       " 'dormer',\n",
       " 'lax',\n",
       " 'vineyard',\n",
       " 'dram',\n",
       " 'dartford',\n",
       " 'percha',\n",
       " 'edina',\n",
       " 'pete',\n",
       " 'hella',\n",
       " 'ponto',\n",
       " \"calc'lat\",\n",
       " 'pict',\n",
       " 'mollusc',\n",
       " 'unfortifi',\n",
       " 'germain',\n",
       " 'shorn',\n",
       " 'brownish',\n",
       " 'porridg',\n",
       " 'cloister',\n",
       " 'norwich',\n",
       " 'stanza',\n",
       " 'suarven',\n",
       " 'balk',\n",
       " 'veteran',\n",
       " 'avaunt',\n",
       " 'nemesi',\n",
       " 'coquettish',\n",
       " 'ministeri',\n",
       " 'rowdi',\n",
       " 'cresset',\n",
       " 'homesick',\n",
       " 'cookeri',\n",
       " 'inexpens',\n",
       " 'oman',\n",
       " 'metabol',\n",
       " 'foodless',\n",
       " 'alterc',\n",
       " 'complais',\n",
       " 'southeastern',\n",
       " 'undream',\n",
       " 'frye',\n",
       " 'parodi',\n",
       " 'acrost',\n",
       " 'vaow',\n",
       " 'yoke',\n",
       " 'moreau',\n",
       " 'gaillard',\n",
       " 'recomm',\n",
       " 'tis',\n",
       " 'effronteri',\n",
       " 'canon',\n",
       " 'geoffrey',\n",
       " 'whitish',\n",
       " 'bluster',\n",
       " 'tong',\n",
       " 'macabr',\n",
       " 'downtown',\n",
       " 'vouchsaf',\n",
       " 'spoilt',\n",
       " 'nuff',\n",
       " 'ashtophet',\n",
       " 'trabe',\n",
       " 'protestant',\n",
       " 'pabulum',\n",
       " 'hord',\n",
       " 'alarum',\n",
       " 'acumen',\n",
       " 'arth',\n",
       " 'shoal',\n",
       " 'censer',\n",
       " 'headquart',\n",
       " 'grandfath',\n",
       " 'silius',\n",
       " 'boullard',\n",
       " 'hark',\n",
       " 'stag',\n",
       " 'swine',\n",
       " 'abaout',\n",
       " 'fonder',\n",
       " 'sublunari',\n",
       " 'aaem',\n",
       " 'overrun',\n",
       " 'cadaver',\n",
       " 'pugnaci',\n",
       " 'appendix',\n",
       " 'lass',\n",
       " 'sunship',\n",
       " 'nx',\n",
       " 'mice',\n",
       " 'torpid',\n",
       " 'unerad',\n",
       " 'pitchfork',\n",
       " 'undamag',\n",
       " 'imperium',\n",
       " 'outspread',\n",
       " 'mongrel',\n",
       " 'maound',\n",
       " 'effervesc',\n",
       " 'unchang',\n",
       " 'bristlin',\n",
       " 'deprav',\n",
       " 'sallow',\n",
       " 'aoutsid',\n",
       " 'phipp',\n",
       " 'vondervotteimittiss',\n",
       " 'countrysid',\n",
       " 'ballad',\n",
       " 'delphi',\n",
       " 'liveliest',\n",
       " 'scoop',\n",
       " 'arv',\n",
       " 'adoni',\n",
       " 'vrow',\n",
       " 'dryer',\n",
       " 'grandsir',\n",
       " 'funniest',\n",
       " 'sumner',\n",
       " 'peleg',\n",
       " 'criticis',\n",
       " 'secess',\n",
       " 'unmar',\n",
       " \"o'connel\",\n",
       " 'pillag',\n",
       " 'infidel',\n",
       " 'flashi',\n",
       " 'stool',\n",
       " 'visitarem',\n",
       " 'entendr',\n",
       " 'heisenberg',\n",
       " 'unwhisper',\n",
       " 'ricci',\n",
       " 'nosolog',\n",
       " 'norfolk',\n",
       " 'avernus',\n",
       " 'stompin',\n",
       " 'metamorphos',\n",
       " 'calvin',\n",
       " 'questo',\n",
       " 'chasten',\n",
       " 'interspac',\n",
       " 'octavius',\n",
       " 'misarrang',\n",
       " 'unassassin',\n",
       " 'literatim',\n",
       " \"ph'nglui\",\n",
       " 'shoreward',\n",
       " 'mercantil',\n",
       " 'perpetua',\n",
       " 'electron',\n",
       " 'gras',\n",
       " 'charmion',\n",
       " 'tankard',\n",
       " 'sam',\n",
       " 'hannah',\n",
       " 'plaguy',\n",
       " 'ulul',\n",
       " 'stovepip',\n",
       " 'martens',\n",
       " 'makri',\n",
       " 'hearthsid',\n",
       " 'vext',\n",
       " 'ellips',\n",
       " 'apac',\n",
       " 'huguenot',\n",
       " 'occidit',\n",
       " 'partaken',\n",
       " 'interweav',\n",
       " 'cult',\n",
       " 'scarabæi',\n",
       " 'sedan',\n",
       " 'roanok',\n",
       " 'cuf',\n",
       " 'herschel',\n",
       " 'swan',\n",
       " 'cauliflow',\n",
       " 'mienn',\n",
       " 'beginnin',\n",
       " 'debil',\n",
       " 'heigho',\n",
       " 'ses',\n",
       " 'lodger',\n",
       " 'roderick',\n",
       " 'fiercest',\n",
       " 'refect',\n",
       " 'turgot',\n",
       " 'whist',\n",
       " 'reread',\n",
       " 'replenish',\n",
       " 'egham',\n",
       " 'com',\n",
       " 'woodvill',\n",
       " 'splice',\n",
       " 'mein',\n",
       " 'modicum',\n",
       " 'hawkin',\n",
       " 'drxwn',\n",
       " 'outdo',\n",
       " 'yore',\n",
       " 'infrequ',\n",
       " 'pigeon',\n",
       " 'saducismus',\n",
       " 'civilian',\n",
       " 'taba',\n",
       " 'obsess',\n",
       " 'capricornutti',\n",
       " 'convoc',\n",
       " 'greasi',\n",
       " 'mislaid',\n",
       " 'aldebaran',\n",
       " 'ångstrom',\n",
       " 'expeck',\n",
       " 'mallet',\n",
       " 'dullard',\n",
       " 'journalist',\n",
       " 'overwrought',\n",
       " 'whar',\n",
       " 'sightse',\n",
       " 'beefsteak',\n",
       " 'spect',\n",
       " 'mercuri',\n",
       " 'alcohol',\n",
       " 'bleach',\n",
       " 'traceri',\n",
       " 'euclidean',\n",
       " 'tim',\n",
       " 'dagon',\n",
       " 'bassianus',\n",
       " 'loch',\n",
       " 'protectorship',\n",
       " 'architrav',\n",
       " 'maenalus',\n",
       " 'substant',\n",
       " 'eberri',\n",
       " 'enshroud',\n",
       " 'annæus',\n",
       " 'baudelair',\n",
       " 'unhonour',\n",
       " 'grisett',\n",
       " 'ise',\n",
       " 'landslid',\n",
       " 'upstart',\n",
       " 'flummeri',\n",
       " 'flagston',\n",
       " 'hysteria',\n",
       " 'amour',\n",
       " 'fredericksburg',\n",
       " 'audient',\n",
       " 'magnalia',\n",
       " 'forewarn',\n",
       " 'gulph',\n",
       " 'retina',\n",
       " 'prowler',\n",
       " 'egre',\n",
       " 'lethal',\n",
       " 'wadin',\n",
       " 'chamier',\n",
       " 'drain',\n",
       " 'absurdum',\n",
       " 'romnod',\n",
       " 'vigintillion',\n",
       " 'rodent',\n",
       " 'albino',\n",
       " 'finder',\n",
       " 'camorin',\n",
       " 'allegori',\n",
       " 'gammel',\n",
       " 'ulterior',\n",
       " 'footnot',\n",
       " 'monsieur',\n",
       " 'unkempt',\n",
       " 'turkish',\n",
       " 'pon',\n",
       " 'bulkhead',\n",
       " 'sidl',\n",
       " 'cylind',\n",
       " 'rousseau',\n",
       " 'wilt',\n",
       " 'undersea',\n",
       " 'rehears',\n",
       " 'ranger',\n",
       " 'pigafetta',\n",
       " 'stateliest',\n",
       " 'razor',\n",
       " 'nonchal',\n",
       " 'kinsfolk',\n",
       " 'inchoat',\n",
       " 'incommunic',\n",
       " 'monast',\n",
       " 'ravenna',\n",
       " 'dilatori',\n",
       " 'forethought',\n",
       " 'tinctur',\n",
       " 'disfigur',\n",
       " 'mousseux',\n",
       " 'tabul',\n",
       " 'soak',\n",
       " 'neb',\n",
       " 'cn',\n",
       " 'cælius',\n",
       " 'cardboard',\n",
       " 'longev',\n",
       " 'cydathria',\n",
       " 'gown',\n",
       " 'baker',\n",
       " 'gird',\n",
       " 'wicker',\n",
       " 'subconsci',\n",
       " 'priori',\n",
       " 'melodrama',\n",
       " 'indig',\n",
       " 'talker',\n",
       " 'mollifi',\n",
       " 'moralis',\n",
       " 'courier',\n",
       " 'signboard',\n",
       " 'shrinkag',\n",
       " 'scorch',\n",
       " 'constructionem',\n",
       " 'eddic',\n",
       " 'casimir',\n",
       " 'ode',\n",
       " 'thrifti',\n",
       " 'prophetess',\n",
       " \"d'antin\",\n",
       " 'trebl',\n",
       " 'corinth',\n",
       " 'lotteri',\n",
       " 'roomi',\n",
       " 'wraith',\n",
       " 'infand',\n",
       " 'tome',\n",
       " 'overleap',\n",
       " 'pratt',\n",
       " 'bray',\n",
       " 'evict',\n",
       " 'sociabl',\n",
       " 'haowev',\n",
       " 'unexplain',\n",
       " 'vii',\n",
       " 'rhythmic',\n",
       " 'pele',\n",
       " 'conting',\n",
       " 'theorist',\n",
       " 'tête',\n",
       " 'oceanward',\n",
       " 'ransom',\n",
       " 'ostrogothen',\n",
       " 'moribund',\n",
       " 'appris',\n",
       " 'fossillus',\n",
       " 'slxw',\n",
       " 'quickest',\n",
       " 'moniteur',\n",
       " 'i.e.',\n",
       " 'xari',\n",
       " 's.e',\n",
       " 'respir',\n",
       " 'cincinnatus',\n",
       " 'inscrut',\n",
       " 'neuralg',\n",
       " 'salutari',\n",
       " 'sidney',\n",
       " 'confluenc',\n",
       " 'unsolv',\n",
       " 'quack',\n",
       " 'pavilion',\n",
       " 'viii',\n",
       " 'ashimah',\n",
       " 'tarraco',\n",
       " 'zebulon',\n",
       " 'plazer',\n",
       " 'spheric',\n",
       " 'into',\n",
       " 'unglov',\n",
       " 'besprinkl',\n",
       " 'convoy',\n",
       " 'remount',\n",
       " 'surcharg',\n",
       " 'conjoint',\n",
       " 'maw',\n",
       " 'pundit',\n",
       " 'ari',\n",
       " 'empyrean',\n",
       " 'wilili',\n",
       " 'rencontr',\n",
       " 'soiré',\n",
       " 'otello',\n",
       " 'interupt',\n",
       " 'wormius',\n",
       " 'jingl',\n",
       " 'muslin',\n",
       " 'stupifi',\n",
       " 'unspot',\n",
       " 'acquit',\n",
       " 'ionia',\n",
       " 'devote',\n",
       " 'pas',\n",
       " 'straightway',\n",
       " 'anthropolog',\n",
       " 'searcher',\n",
       " 'journi',\n",
       " 'stoppag',\n",
       " 'tout',\n",
       " 'nitrous',\n",
       " 'narg',\n",
       " 'lactea',\n",
       " 'ar',\n",
       " 'coelius',\n",
       " 'kulten',\n",
       " 'medoc',\n",
       " 'sienta',\n",
       " 'cognis',\n",
       " 'pentagon',\n",
       " 'tenuiti',\n",
       " 'braggadocio',\n",
       " 'abigail',\n",
       " 'stapl',\n",
       " 'dock',\n",
       " 'odenheim',\n",
       " 'philanthrop',\n",
       " 'sneez',\n",
       " 'unrecognis',\n",
       " 'uncurl',\n",
       " 'stabli',\n",
       " 'sech',\n",
       " 'zaiat',\n",
       " 'recount',\n",
       " 'rubicon',\n",
       " 'martin',\n",
       " 'irredeem',\n",
       " 'tulli',\n",
       " 'locket',\n",
       " 'ellipsoid',\n",
       " 'zimmerman',\n",
       " 'chinamen',\n",
       " 'dissoci',\n",
       " 'subdivis',\n",
       " 'pettitt',\n",
       " 'heraclid',\n",
       " 'fay',\n",
       " 'constantinopoli',\n",
       " 'morryst',\n",
       " 'axiom',\n",
       " 'ridgi',\n",
       " 'bah',\n",
       " 'portabl',\n",
       " 'conspir',\n",
       " 'frxg',\n",
       " 'rosi',\n",
       " 'sloven',\n",
       " 'interwoven',\n",
       " 'disobedi',\n",
       " 'flay',\n",
       " 'peel',\n",
       " 'atween',\n",
       " 'isidor',\n",
       " 'diagnos',\n",
       " 'revok',\n",
       " 'acolyt',\n",
       " 'salad',\n",
       " 'silverwar',\n",
       " 'wy',\n",
       " 'gaseous',\n",
       " 'allig',\n",
       " 'orthograph',\n",
       " 'belial',\n",
       " 'fun',\n",
       " 'pulpi',\n",
       " 'dæmonic',\n",
       " 'circumscrib',\n",
       " 'cappadocia',\n",
       " 'vid',\n",
       " 'pavé',\n",
       " 'swammerdamm',\n",
       " 'cydathrian',\n",
       " 'nath',\n",
       " 'bourn',\n",
       " 'vignet',\n",
       " 'schoolmat',\n",
       " 'calamit',\n",
       " 'torso',\n",
       " 'jet',\n",
       " 'incapac',\n",
       " 'evelyn',\n",
       " 'banof',\n",
       " 'matchless',\n",
       " 'spectatress',\n",
       " 'unwat',\n",
       " 'sage',\n",
       " 'nick',\n",
       " 'eyebrow',\n",
       " 'gloat',\n",
       " 'tutta',\n",
       " 'tibia',\n",
       " 'succour',\n",
       " 'canist',\n",
       " 'haemus',\n",
       " 'characteri',\n",
       " 'unselfish',\n",
       " 'bignonia',\n",
       " 'mulct',\n",
       " 'sie',\n",
       " 'unplumb',\n",
       " 'walpurgi',\n",
       " 'etruria',\n",
       " 'scantl',\n",
       " 'kiel',\n",
       " 'aerost',\n",
       " 'n.y.',\n",
       " 'hearten',\n",
       " 'unlaw',\n",
       " 'ronald',\n",
       " 'del',\n",
       " 'mongrelis',\n",
       " 'packag',\n",
       " 'roi',\n",
       " 'sarv',\n",
       " \"j'int\",\n",
       " 'provision',\n",
       " 'orlean',\n",
       " 'inhospit',\n",
       " 'paralys',\n",
       " 'cthulhu',\n",
       " 'flyspeck',\n",
       " 'bawl',\n",
       " 'impromptu',\n",
       " 'neptunian',\n",
       " 'java',\n",
       " 'thereunto',\n",
       " 'seaport',\n",
       " 'servi',\n",
       " 'roger',\n",
       " 'medean',\n",
       " 'fibe',\n",
       " 'sybil',\n",
       " 'begotten',\n",
       " 'stigma',\n",
       " 'sydney',\n",
       " 'shambl',\n",
       " 'quoi',\n",
       " 'tuh',\n",
       " ...}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(df_train['cleaned_text'], size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_array_len = list(w2v.items())[0][1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_top_words(tfidfdict, numwrd):\n",
    "\n",
    "    top_word_dict, min_value, min_key = {}, 99, ''\n",
    "    \n",
    "\n",
    "    for k, v in max_tf_dict.items():\n",
    "        # print(top_word_dict.values())\n",
    "        # print(v)\n",
    "        if k not in unique_words and k not in eng_stopwords:\n",
    "        \n",
    "            if len(top_word_dict) < numwrd:\n",
    "                top_word_dict[k] = v\n",
    "                if v <= min_value:\n",
    "                    min_key = k\n",
    "            else:\n",
    "                # print(v, min(list(top_word_dict.values())))\n",
    "                if v > min(list(top_word_dict.values())) and k not in eng_stopwords:\n",
    "\n",
    "                    min_value = min(top_word_dict.values())\n",
    "\n",
    "                    for ky, va in top_word_dict.items():\n",
    "                        if va == min_value:\n",
    "                            min_key = ky\n",
    "\n",
    "                    top_word_dict.pop(min_key)\n",
    "                    top_word_dict[k] = v\n",
    "                \n",
    "    return top_word_dict\n",
    "another_top_words_dict = extract_top_words(max_tf_dict, 80)\n",
    "high_tf_idf_words_columns = list(another_top_words_dict.keys())\n",
    "\n",
    "\n",
    "def count_topwords(target_df):\n",
    "\n",
    "    for word in high_tf_idf_words_columns:\n",
    "        \n",
    "        # TODO: костыль, нужен, когда у нас уже есть такие столбцы\n",
    "        # в датасете\n",
    "#         try:\n",
    "#             target_df = target_df.drop(word, 1)\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "        \n",
    "\n",
    "        def count_numwords(collist):\n",
    "            value = 0\n",
    "\n",
    "            for wd in collist:\n",
    "                if wd == word:\n",
    "                    value += 1\n",
    "            return value\n",
    "\n",
    "\n",
    "        target_df[word] = target_df.cleaned_text.apply(count_numwords)\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>street</th>\n",
       "      <th>night</th>\n",
       "      <th>may</th>\n",
       "      <th>much</th>\n",
       "      <th>strang</th>\n",
       "      <th>day</th>\n",
       "      <th>natur</th>\n",
       "      <th>must</th>\n",
       "      <th>though</th>\n",
       "      <th>us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper ...  street  night  \\\n",
       "0                35                 7                2 ...       0      0   \n",
       "1                14                 1                0 ...       0      0   \n",
       "2                32                 5                0 ...       0      0   \n",
       "\n",
       "   may  much strang  day  natur  must  though  us  \n",
       "0    0     0      0    0      0     0       0   0  \n",
       "1    0     0      0    0      0     0       0   0  \n",
       "2    0     0      0    0      0     0       0   0  \n",
       "\n",
       "[3 rows x 95 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['length']=df_train['cleaned_text_string'].apply(len)\n",
    "df_train[\"num_words\"] = df_train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_train[\"num_unique_words\"] = df_train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_train[\"num_punctuations\"] =df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_train[\"num_words_upper\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_train[\"num_words_title\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_train[\"mean_word_len\"] = df_train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_train[\"num_stopwords\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_train['lexical_diversity'] = df_train.text.apply(lexical_diversity)\n",
    "df_train['w2v_array'] = df_train.cleaned_text.apply(sum_up_word2vec_array)\n",
    "count_topwords(df_train)\n",
    "\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_w2v_columns(target_df):\n",
    "    \n",
    "    # сначала вытаскиваем колонку как список списков\n",
    "    \n",
    "    w2v_array = target_df['w2v_array'].tolist()\n",
    "    \n",
    "    for i in range(100):\n",
    "        \n",
    "        target_df['w2v_feature_' + str(i)] = [j[i] for j in w2v_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_w2v_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.942119</td>\n",
       "      <td>-0.358140</td>\n",
       "      <td>11.083304</td>\n",
       "      <td>0.984288</td>\n",
       "      <td>1.852068</td>\n",
       "      <td>1.080477</td>\n",
       "      <td>-0.010269</td>\n",
       "      <td>-7.757530</td>\n",
       "      <td>-3.017276</td>\n",
       "      <td>2.205754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.743495</td>\n",
       "      <td>0.119770</td>\n",
       "      <td>3.241399</td>\n",
       "      <td>0.255773</td>\n",
       "      <td>0.446480</td>\n",
       "      <td>0.274428</td>\n",
       "      <td>0.282043</td>\n",
       "      <td>-2.035049</td>\n",
       "      <td>-0.809668</td>\n",
       "      <td>0.596570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.073640</td>\n",
       "      <td>-1.091211</td>\n",
       "      <td>8.312735</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>2.103114</td>\n",
       "      <td>1.331694</td>\n",
       "      <td>-1.781096</td>\n",
       "      <td>-7.162513</td>\n",
       "      <td>-2.376012</td>\n",
       "      <td>0.950710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, love, spring, as, look, windsor, terrac,...</td>\n",
       "      <td>how love spring as look windsor terrac sixteen...</td>\n",
       "      <td>144</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.968259</td>\n",
       "      <td>-0.699704</td>\n",
       "      <td>9.393400</td>\n",
       "      <td>0.434389</td>\n",
       "      <td>2.018498</td>\n",
       "      <td>1.203822</td>\n",
       "      <td>-1.145513</td>\n",
       "      <td>-7.376847</td>\n",
       "      <td>-2.525478</td>\n",
       "      <td>1.201493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[find, noth, els, even, gold, superintend, aba...</td>\n",
       "      <td>find noth els even gold superintend abandon at...</td>\n",
       "      <td>102</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.384791</td>\n",
       "      <td>-0.201025</td>\n",
       "      <td>6.812135</td>\n",
       "      <td>0.414073</td>\n",
       "      <td>1.278332</td>\n",
       "      <td>0.731568</td>\n",
       "      <td>-0.165426</td>\n",
       "      <td>-4.971582</td>\n",
       "      <td>-1.783214</td>\n",
       "      <td>1.067079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "3  [how, love, spring, as, look, windsor, terrac,...   \n",
       "4  [find, noth, els, even, gold, superintend, aba...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "3  how love spring as look windsor terrac sixteen...     144         34   \n",
       "4  find noth els even gold superintend abandon at...     102         27   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper       ...        \\\n",
       "0                35                 7                2       ...         \n",
       "1                14                 1                0       ...         \n",
       "2                32                 5                0       ...         \n",
       "3                32                 4                0       ...         \n",
       "4                25                 4                0       ...         \n",
       "\n",
       "   w2v_feature_90  w2v_feature_91  w2v_feature_92  w2v_feature_93  \\\n",
       "0        6.942119       -0.358140       11.083304        0.984288   \n",
       "1        1.743495        0.119770        3.241399        0.255773   \n",
       "2        7.073640       -1.091211        8.312735        0.230420   \n",
       "3        6.968259       -0.699704        9.393400        0.434389   \n",
       "4        4.384791       -0.201025        6.812135        0.414073   \n",
       "\n",
       "  w2v_feature_94  w2v_feature_95  w2v_feature_96  w2v_feature_97  \\\n",
       "0       1.852068        1.080477       -0.010269       -7.757530   \n",
       "1       0.446480        0.274428        0.282043       -2.035049   \n",
       "2       2.103114        1.331694       -1.781096       -7.162513   \n",
       "3       2.018498        1.203822       -1.145513       -7.376847   \n",
       "4       1.278332        0.731568       -0.165426       -4.971582   \n",
       "\n",
       "   w2v_feature_98  w2v_feature_99  \n",
       "0       -3.017276        2.205754  \n",
       "1       -0.809668        0.596570  \n",
       "2       -2.376012        0.950710  \n",
       "3       -2.525478        1.201493  \n",
       "4       -1.783214        1.067079  \n",
       "\n",
       "[5 rows x 195 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>happi</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>81.543165</td>\n",
       "      <td>25.442405</td>\n",
       "      <td>21.894937</td>\n",
       "      <td>4.096329</td>\n",
       "      <td>0.553291</td>\n",
       "      <td>2.102405</td>\n",
       "      <td>4.644952</td>\n",
       "      <td>12.747595</td>\n",
       "      <td>0.886060</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.597563</td>\n",
       "      <td>-0.711742</td>\n",
       "      <td>-0.616946</td>\n",
       "      <td>-4.337550</td>\n",
       "      <td>-2.765478</td>\n",
       "      <td>-3.670272</td>\n",
       "      <td>-4.922817</td>\n",
       "      <td>-2.245777</td>\n",
       "      <td>1.494961</td>\n",
       "      <td>6.133319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>60.100183</td>\n",
       "      <td>18.567706</td>\n",
       "      <td>13.727397</td>\n",
       "      <td>3.573788</td>\n",
       "      <td>0.892966</td>\n",
       "      <td>2.052241</td>\n",
       "      <td>0.631340</td>\n",
       "      <td>9.619779</td>\n",
       "      <td>0.097354</td>\n",
       "      <td>0.070110</td>\n",
       "      <td>...</td>\n",
       "      <td>2.060546</td>\n",
       "      <td>0.492144</td>\n",
       "      <td>0.494714</td>\n",
       "      <td>3.051287</td>\n",
       "      <td>2.432590</td>\n",
       "      <td>3.045421</td>\n",
       "      <td>3.386442</td>\n",
       "      <td>1.728497</td>\n",
       "      <td>1.034629</td>\n",
       "      <td>4.427708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.132179</td>\n",
       "      <td>-7.327130</td>\n",
       "      <td>-10.641767</td>\n",
       "      <td>-45.374699</td>\n",
       "      <td>-54.478886</td>\n",
       "      <td>-32.246254</td>\n",
       "      <td>-54.783573</td>\n",
       "      <td>-19.000381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.045488</td>\n",
       "      <td>-0.911061</td>\n",
       "      <td>-0.793333</td>\n",
       "      <td>-5.618233</td>\n",
       "      <td>-3.482996</td>\n",
       "      <td>-5.050953</td>\n",
       "      <td>-6.285369</td>\n",
       "      <td>-2.985216</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>3.167969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.998157</td>\n",
       "      <td>-0.594339</td>\n",
       "      <td>-0.502430</td>\n",
       "      <td>-3.578559</td>\n",
       "      <td>-2.088254</td>\n",
       "      <td>-3.033720</td>\n",
       "      <td>-4.106851</td>\n",
       "      <td>-1.822431</td>\n",
       "      <td>1.238525</td>\n",
       "      <td>5.069916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.402371</td>\n",
       "      <td>-0.378131</td>\n",
       "      <td>-0.298529</td>\n",
       "      <td>-2.227310</td>\n",
       "      <td>-1.267485</td>\n",
       "      <td>-1.652519</td>\n",
       "      <td>-2.615695</td>\n",
       "      <td>-1.049155</td>\n",
       "      <td>1.916356</td>\n",
       "      <td>7.795946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>925.000000</td>\n",
       "      <td>267.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.529351</td>\n",
       "      <td>2.105792</td>\n",
       "      <td>4.669187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.967430</td>\n",
       "      <td>8.583466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408612</td>\n",
       "      <td>13.935845</td>\n",
       "      <td>91.209198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            length    num_words  num_unique_words  num_punctuations  \\\n",
       "count  7900.000000  7900.000000       7900.000000       7900.000000   \n",
       "mean     81.543165    25.442405         21.894937          4.096329   \n",
       "std      60.100183    18.567706         13.727397          3.573788   \n",
       "min       5.000000     2.000000          2.000000          1.000000   \n",
       "25%      40.000000    12.000000         12.000000          2.000000   \n",
       "50%      66.000000    21.000000         19.000000          3.000000   \n",
       "75%     106.000000    33.000000         29.000000          5.000000   \n",
       "max     925.000000   267.000000        155.000000         71.000000   \n",
       "\n",
       "       num_words_upper  num_words_title  mean_word_len  num_stopwords  \\\n",
       "count      7900.000000      7900.000000    7900.000000    7900.000000   \n",
       "mean          0.553291         2.102405       4.644952      12.747595   \n",
       "std           0.892966         2.052241       0.631340       9.619779   \n",
       "min           0.000000         0.000000       2.000000       0.000000   \n",
       "25%           0.000000         1.000000       4.250000       6.000000   \n",
       "50%           0.000000         1.000000       4.600000      10.000000   \n",
       "75%           1.000000         2.000000       5.000000      17.000000   \n",
       "max          15.000000        43.000000      11.000000     135.000000   \n",
       "\n",
       "       lexical_diversity        happi       ...        w2v_feature_90  \\\n",
       "count        7900.000000  7900.000000       ...           7900.000000   \n",
       "mean            0.886060     0.004684       ...             -1.597563   \n",
       "std             0.097354     0.070110       ...              2.060546   \n",
       "min             0.333333     0.000000       ...            -43.132179   \n",
       "25%             0.821429     0.000000       ...             -2.045488   \n",
       "50%             0.894737     0.000000       ...             -0.998157   \n",
       "75%             1.000000     0.000000       ...             -0.402371   \n",
       "max             1.000000     2.000000       ...              2.529351   \n",
       "\n",
       "       w2v_feature_91  w2v_feature_92  w2v_feature_93  w2v_feature_94  \\\n",
       "count     7900.000000     7900.000000     7900.000000     7900.000000   \n",
       "mean        -0.711742       -0.616946       -4.337550       -2.765478   \n",
       "std          0.492144        0.494714        3.051287        2.432590   \n",
       "min         -7.327130      -10.641767      -45.374699      -54.478886   \n",
       "25%         -0.911061       -0.793333       -5.618233       -3.482996   \n",
       "50%         -0.594339       -0.502430       -3.578559       -2.088254   \n",
       "75%         -0.378131       -0.298529       -2.227310       -1.267485   \n",
       "max          2.105792        4.669187        0.000000        2.967430   \n",
       "\n",
       "       w2v_feature_95  w2v_feature_96  w2v_feature_97  w2v_feature_98  \\\n",
       "count     7900.000000     7900.000000     7900.000000     7900.000000   \n",
       "mean        -3.670272       -4.922817       -2.245777        1.494961   \n",
       "std          3.045421        3.386442        1.728497        1.034629   \n",
       "min        -32.246254      -54.783573      -19.000381        0.000000   \n",
       "25%         -5.050953       -6.285369       -2.985216        0.788991   \n",
       "50%         -3.033720       -4.106851       -1.822431        1.238525   \n",
       "75%         -1.652519       -2.615695       -1.049155        1.916356   \n",
       "max          8.583466        0.000000        0.408612       13.935845   \n",
       "\n",
       "       w2v_feature_99  \n",
       "count     7900.000000  \n",
       "mean         6.133319  \n",
       "std          4.427708  \n",
       "min          0.000000  \n",
       "25%          3.167969  \n",
       "50%          5.069916  \n",
       "75%          7.795946  \n",
       "max         91.209198  \n",
       "\n",
       "[8 rows x 189 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eap=df_train[df_train['author']=='EAP']\n",
    "df_eap.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>happi</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>86.124586</td>\n",
       "      <td>27.417273</td>\n",
       "      <td>23.544672</td>\n",
       "      <td>3.833719</td>\n",
       "      <td>0.751489</td>\n",
       "      <td>2.124255</td>\n",
       "      <td>4.598182</td>\n",
       "      <td>13.896923</td>\n",
       "      <td>0.883407</td>\n",
       "      <td>0.030609</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.803665</td>\n",
       "      <td>-0.785075</td>\n",
       "      <td>-0.668287</td>\n",
       "      <td>-4.735346</td>\n",
       "      <td>-3.037104</td>\n",
       "      <td>-4.115414</td>\n",
       "      <td>-5.532511</td>\n",
       "      <td>-2.404975</td>\n",
       "      <td>1.703766</td>\n",
       "      <td>6.679250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>71.976281</td>\n",
       "      <td>23.134440</td>\n",
       "      <td>14.925835</td>\n",
       "      <td>2.840625</td>\n",
       "      <td>1.203636</td>\n",
       "      <td>1.759572</td>\n",
       "      <td>0.561558</td>\n",
       "      <td>12.196599</td>\n",
       "      <td>0.086804</td>\n",
       "      <td>0.180708</td>\n",
       "      <td>...</td>\n",
       "      <td>2.058326</td>\n",
       "      <td>0.667723</td>\n",
       "      <td>0.593003</td>\n",
       "      <td>4.007942</td>\n",
       "      <td>2.736596</td>\n",
       "      <td>3.838224</td>\n",
       "      <td>4.640517</td>\n",
       "      <td>2.154190</td>\n",
       "      <td>1.456653</td>\n",
       "      <td>5.568243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.951378</td>\n",
       "      <td>-26.515741</td>\n",
       "      <td>-22.135078</td>\n",
       "      <td>-158.751236</td>\n",
       "      <td>-85.285522</td>\n",
       "      <td>-150.461166</td>\n",
       "      <td>-179.890717</td>\n",
       "      <td>-85.041237</td>\n",
       "      <td>0.026838</td>\n",
       "      <td>0.104845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.252982</td>\n",
       "      <td>-0.979568</td>\n",
       "      <td>-0.849959</td>\n",
       "      <td>-5.920853</td>\n",
       "      <td>-3.670478</td>\n",
       "      <td>-5.349403</td>\n",
       "      <td>-6.842672</td>\n",
       "      <td>-3.101918</td>\n",
       "      <td>0.957934</td>\n",
       "      <td>3.778657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.560791</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.237344</td>\n",
       "      <td>-0.672247</td>\n",
       "      <td>-0.558439</td>\n",
       "      <td>-4.049242</td>\n",
       "      <td>-2.419662</td>\n",
       "      <td>-3.521115</td>\n",
       "      <td>-4.757585</td>\n",
       "      <td>-2.037106</td>\n",
       "      <td>1.459436</td>\n",
       "      <td>5.636068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>108.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.907156</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.615417</td>\n",
       "      <td>-0.445167</td>\n",
       "      <td>-0.350724</td>\n",
       "      <td>-2.676190</td>\n",
       "      <td>-1.562614</td>\n",
       "      <td>-2.122534</td>\n",
       "      <td>-3.140221</td>\n",
       "      <td>-1.265842</td>\n",
       "      <td>2.113236</td>\n",
       "      <td>8.213998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2715.000000</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.166953</td>\n",
       "      <td>-0.015917</td>\n",
       "      <td>0.125284</td>\n",
       "      <td>-0.069200</td>\n",
       "      <td>-0.032010</td>\n",
       "      <td>3.329655</td>\n",
       "      <td>-0.079661</td>\n",
       "      <td>0.309729</td>\n",
       "      <td>57.268726</td>\n",
       "      <td>208.190536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            length    num_words  num_unique_words  num_punctuations  \\\n",
       "count  6044.000000  6044.000000       6044.000000       6044.000000   \n",
       "mean     86.124586    27.417273         23.544672          3.833719   \n",
       "std      71.976281    23.134440         14.925835          2.840625   \n",
       "min       4.000000     2.000000          2.000000          1.000000   \n",
       "25%      48.000000    15.000000         14.000000          2.000000   \n",
       "50%      74.000000    23.000000         21.000000          3.000000   \n",
       "75%     108.000000    34.000000         30.000000          5.000000   \n",
       "max    2715.000000   861.000000        429.000000         59.000000   \n",
       "\n",
       "       num_words_upper  num_words_title  mean_word_len  num_stopwords  \\\n",
       "count      6044.000000      6044.000000    6044.000000    6044.000000   \n",
       "mean          0.751489         2.124255       4.598182      13.896923   \n",
       "std           1.203636         1.759572       0.561558      12.196599   \n",
       "min           0.000000         0.000000       2.666667       0.000000   \n",
       "25%           0.000000         1.000000       4.250000       7.000000   \n",
       "50%           0.000000         2.000000       4.560791      12.000000   \n",
       "75%           1.000000         3.000000       4.907156      18.000000   \n",
       "max          27.000000        46.000000      10.500000     437.000000   \n",
       "\n",
       "       lexical_diversity        happi       ...        w2v_feature_90  \\\n",
       "count        6044.000000  6044.000000       ...           6044.000000   \n",
       "mean            0.883407     0.030609       ...             -1.803665   \n",
       "std             0.086804     0.180708       ...              2.058326   \n",
       "min             0.398990     0.000000       ...            -44.951378   \n",
       "25%             0.823529     0.000000       ...             -2.252982   \n",
       "50%             0.885714     0.000000       ...             -1.237344   \n",
       "75%             0.950000     0.000000       ...             -0.615417   \n",
       "max             1.000000     2.000000       ...              1.166953   \n",
       "\n",
       "       w2v_feature_91  w2v_feature_92  w2v_feature_93  w2v_feature_94  \\\n",
       "count     6044.000000     6044.000000     6044.000000     6044.000000   \n",
       "mean        -0.785075       -0.668287       -4.735346       -3.037104   \n",
       "std          0.667723        0.593003        4.007942        2.736596   \n",
       "min        -26.515741      -22.135078     -158.751236      -85.285522   \n",
       "25%         -0.979568       -0.849959       -5.920853       -3.670478   \n",
       "50%         -0.672247       -0.558439       -4.049242       -2.419662   \n",
       "75%         -0.445167       -0.350724       -2.676190       -1.562614   \n",
       "max         -0.015917        0.125284       -0.069200       -0.032010   \n",
       "\n",
       "       w2v_feature_95  w2v_feature_96  w2v_feature_97  w2v_feature_98  \\\n",
       "count     6044.000000     6044.000000     6044.000000     6044.000000   \n",
       "mean        -4.115414       -5.532511       -2.404975        1.703766   \n",
       "std          3.838224        4.640517        2.154190        1.456653   \n",
       "min       -150.461166     -179.890717      -85.041237        0.026838   \n",
       "25%         -5.349403       -6.842672       -3.101918        0.957934   \n",
       "50%         -3.521115       -4.757585       -2.037106        1.459436   \n",
       "75%         -2.122534       -3.140221       -1.265842        2.113236   \n",
       "max          3.329655       -0.079661        0.309729       57.268726   \n",
       "\n",
       "       w2v_feature_99  \n",
       "count     6044.000000  \n",
       "mean         6.679250  \n",
       "std          5.568243  \n",
       "min          0.104845  \n",
       "25%          3.778657  \n",
       "50%          5.636068  \n",
       "75%          8.213998  \n",
       "max        208.190536  \n",
       "\n",
       "[8 rows x 189 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mws=df_train[df_train['author']=='MWS']\n",
    "df_mws.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordset=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#делаю сет со всеми словами\n",
    "for i in df_train.index:\n",
    "    wordset |= set(df_train['cleaned_text'][i])\n",
    "wordlist=list(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>mws</th>\n",
       "      <th>eap</th>\n",
       "      <th>hpl</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diplomaci</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>load</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>garrison</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jeremiad</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bluddennuff</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  mws  eap  hpl  all\n",
       "0    diplomaci    0    0    0    0\n",
       "1         load    0    0    0    0\n",
       "2     garrison    0    0    0    0\n",
       "3     jeremiad    0    0    0    0\n",
       "4  bluddennuff    0    0    0    0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#делаю фрейм со словами\n",
    "df_word=pd.DataFrame(columns=[\"word\", \"mws\", \"eap\", \"hpl\", \"all\"])\n",
    "df_word[\"word\"]=wordlist\n",
    "df_word[\"mws\"]=0\n",
    "df_word[\"eap\"]=0\n",
    "df_word[\"hpl\"]=0\n",
    "df_word[\"all\"]=0\n",
    "df_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# как мы будем эту штуку правильнее делать (возможно это жуткий костыль), я хз\n",
    "# сначала создаем словарь где ключ - уникальное слово, а значение - его порядковый номер\n",
    "# затем создаем разреженную матрицу, которую заполняем в зависимости от порядковых номеров \n",
    "word_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#делаю сет со всеми словами\n",
    "# и сразу заготовку под шапку(потом увидишь зачем)\n",
    "counter = 0\n",
    "head = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    for word in wordlist:\n",
    "        if word not in word_dict:\n",
    "            head.append(word)\n",
    "            word_dict[word] = counter\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# видоизменять колонки в pandas руками по одному значению в строке или столбце - очень плохая идея\n",
    "# колонка это numpy.ndarray, а значит при каждой итерации она будет пересоздаваться\n",
    "# что угробит производительность\n",
    "# делаем значит так. считаем где сколько и где встречались отдельные слова, затем создаем строку за строкой для \n",
    "# каждого предложения\n",
    "\n",
    "list_of_lists = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    row = [0 for i in range(len(word_dict))]\n",
    "    for word in wordlist:\n",
    "        row[word_dict[word]] += 1\n",
    "    list_of_lists.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... и для того чтобы посмотреть встречаемость того или иного слова по авторам добавим такую колонку\n",
    "\n",
    "count_frame = pd.DataFrame(list_of_lists)\n",
    "count_frame['author'] = df_train['author']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... и теперь нормальную шапку делаем\n",
    "\n",
    "count_frame.columns = head + ['author']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   this  process  howev  afford  mean  ascertain  dimens  dungeon  i  might  \\\n",
      "0     1        1      1       1     1          1       1        1  2      1   \n",
      "1     0        0      0       0     0          0       0        0  0      1   \n",
      "2     0        0      0       0     0          0       0        0  0      0   \n",
      "3     0        0      0       0     0          0       0        0  0      0   \n",
      "4     0        0      0       0     0          0       0        0  0      0   \n",
      "\n",
      "    ...    aegidus  burr  bentley  waltzer  binder  brusqueri  adriat  ancona  \\\n",
      "0   ...          0     0        0        0       0          0       0       0   \n",
      "1   ...          0     0        0        0       0          0       0       0   \n",
      "2   ...          0     0        0        0       0          0       0       0   \n",
      "3   ...          0     0        0        0       0          0       0       0   \n",
      "4   ...          0     0        0        0       0          0       0       0   \n",
      "\n",
      "   agir  author  \n",
      "0     0     EAP  \n",
      "1     0     HPL  \n",
      "2     0     EAP  \n",
      "3     0     MWS  \n",
      "4     0     HPL  \n",
      "\n",
      "[5 rows x 15231 columns]\n"
     ]
    }
   ],
   "source": [
    "print(count_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Пока объединим все, потом может быть будем использовать\n",
    "col=list(count_frame.columns)\n",
    "col[-1]='author_name'\n",
    "count_frame.columns=col\n",
    "pivot_col=pd.pivot_table(count_frame, aggfunc=np.sum, values=col, index=['author_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 14351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  \\\n",
       "author_name                                                                  \n",
       "EAP             1      2      0       22       0     2      1     2      3   \n",
       "HPL             0      0      0       17      24     0      1     3      0   \n",
       "MWS             0      0      1        9       0     0      0     1      2   \n",
       "\n",
       "             abbrevi  ...    æmilianus  æneid  ærial  æronaut  ærostat  \\\n",
       "author_name           ...                                                \n",
       "EAP                2  ...            0      0      1        3        1   \n",
       "HPL                0  ...            2      1      0        0        0   \n",
       "MWS                0  ...            0      0      0        0        0   \n",
       "\n",
       "             æschylus  élite  émeut  οἶδα  υπνος  \n",
       "author_name                                       \n",
       "EAP                 1      1      1     0      0  \n",
       "HPL                 0      0      0     2      1  \n",
       "MWS                 0      0      0     0      0  \n",
       "\n",
       "[3 rows x 14351 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Убираем лишние слова, которые не учли раньше\n",
    "col=list(pivot_col.columns)\n",
    "col2=[string for string in col if (string[0]!='\"' and string[0]!=\"'\"\n",
    "                                  and string[0]!='.' and string[0]!='`'\n",
    "                                   and len(string)>3 and '.' not in string)]\n",
    "col=[]\n",
    "pivot_col=pivot_col[col2]\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUMA</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 14351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  abbrevi  \\\n",
       "EAP      1      2      0       22       0     2      1     2      3        2   \n",
       "HPL      0      0      0       17      24     0      1     3      0        0   \n",
       "MWS      0      0      1        9       0     0      0     1      2        0   \n",
       "SUMA     1      2      1       48      24     2      2     6      5        2   \n",
       "\n",
       "      ...    æmilianus  æneid  ærial  æronaut  ærostat  æschylus  élite  \\\n",
       "EAP   ...            0      0      1        3        1         1      1   \n",
       "HPL   ...            2      1      0        0        0         0      0   \n",
       "MWS   ...            0      0      0        0        0         0      0   \n",
       "SUMA  ...            2      1      1        3        1         1      1   \n",
       "\n",
       "      émeut  οἶδα  υπνος  \n",
       "EAP       1     0      0  \n",
       "HPL       0     2      1  \n",
       "MWS       0     0      0  \n",
       "SUMA      1     2      1  \n",
       "\n",
       "[4 rows x 14351 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create pivot\n",
    "pivot_col=pivot_col.append(pivot_col.sum(), ignore_index=True)\n",
    "pivot_col.index=['EAP', 'HPL', 'MWS', 'SUMA']\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "      <th>summa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>74269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUMA</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>235194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 14352 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  abbrevi  \\\n",
       "EAP      1      2      0       22       0     2      1     2      3        2   \n",
       "HPL      0      0      0       17      24     0      1     3      0        0   \n",
       "MWS      0      0      1        9       0     0      0     1      2        0   \n",
       "SUMA     1      2      1       48      24     2      2     6      5        2   \n",
       "\n",
       "       ...    æneid  ærial  æronaut  ærostat  æschylus  élite  émeut  οἶδα  \\\n",
       "EAP    ...        0      1        3        1         1      1      1     0   \n",
       "HPL    ...        1      0        0        0         0      0      0     2   \n",
       "MWS    ...        0      0        0        0         0      0      0     0   \n",
       "SUMA   ...        1      1        3        1         1      1      1     2   \n",
       "\n",
       "      υπνος   summa  \n",
       "EAP       0   87765  \n",
       "HPL       1   74269  \n",
       "MWS       0   73160  \n",
       "SUMA      1  235194  \n",
       "\n",
       "[4 rows x 14352 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa=[pivot_col.loc['EAP'].sum(), pivot_col.loc['HPL'].sum(), \n",
    "       pivot_col.loc['MWS'].sum(), pivot_col.loc['SUMA'].sum()]\n",
    "pivot_col['summa']=summa\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdic</th>\n",
       "      <th>aberr</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abhorr</th>\n",
       "      <th>abil</th>\n",
       "      <th>abject</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>your</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zenith</th>\n",
       "      <th>zest</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zone</th>\n",
       "      <th>summa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.373160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.315778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 6619 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandon  abash      abat  abbey     abdic     aberr     abhor    abhorr  \\\n",
       "EAP  0.458333    0.5  0.333333    0.6  0.142857  0.166667  0.058824  0.111111   \n",
       "HPL  0.354167    0.5  0.500000    0.0  0.000000  0.666667  0.235294  0.555556   \n",
       "MWS  0.187500    0.0  0.166667    0.4  0.857143  0.166667  0.705882  0.333333   \n",
       "\n",
       "         abil    abject    ...      younger  youngest      your     youth  \\\n",
       "EAP  0.789474  0.333333    ...     0.272727       0.2  0.534884  0.101562   \n",
       "HPL  0.052632  0.000000    ...     0.000000       0.4  0.069767  0.429688   \n",
       "MWS  0.157895  0.666667    ...     0.727273       0.4  0.395349  0.468750   \n",
       "\n",
       "         zeal  zenith  zest  zigzag      zone     summa  \n",
       "EAP  0.117647     0.4   0.2     0.4  0.666667  0.373160  \n",
       "HPL  0.470588     0.6   0.2     0.6  0.333333  0.315778  \n",
       "MWS  0.411765     0.0   0.6     0.0  0.000000  0.311062  \n",
       "\n",
       "[3 rows x 6619 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create probability of author text knowing that a word was used\n",
    "pivot_part=pivot_col\n",
    "pivot_part.loc['EAP']=pivot_col.loc['EAP']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['HPL']=pivot_col.loc['HPL']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['MWS']=pivot_col.loc['MWS']/pivot_col.loc['SUMA']\n",
    "pivot_part=pivot_part.loc[['EAP', 'HPL', 'MWS']]\n",
    "# Delete unique words\n",
    "pivot_part=pivot_part.loc[:, (pivot_part!=1).all(axis=0)]\n",
    "pivot_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44859813084112149"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will be easier to work this way\n",
    "eap_dict=pivot_part.loc['EAP'].to_dict()\n",
    "hpl_dict=pivot_part.loc['HPL'].to_dict()\n",
    "mws_dict=pivot_part.loc['MWS'].to_dict()\n",
    "eap_dict['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create author score \n",
    "def ind_val_eap(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=eap_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_hpl(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=hpl_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_mws(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=mws_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984288</td>\n",
       "      <td>1.852068</td>\n",
       "      <td>1.080477</td>\n",
       "      <td>-0.010269</td>\n",
       "      <td>-7.757530</td>\n",
       "      <td>-3.017276</td>\n",
       "      <td>2.205754</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.074388</td>\n",
       "      <td>0.034504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255773</td>\n",
       "      <td>0.446480</td>\n",
       "      <td>0.274428</td>\n",
       "      <td>0.282043</td>\n",
       "      <td>-2.035049</td>\n",
       "      <td>-0.809668</td>\n",
       "      <td>0.596570</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.034739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>2.103114</td>\n",
       "      <td>1.331694</td>\n",
       "      <td>-1.781096</td>\n",
       "      <td>-7.162513</td>\n",
       "      <td>-2.376012</td>\n",
       "      <td>0.950710</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper    ...      \\\n",
       "0                35                 7                2    ...       \n",
       "1                14                 1                0    ...       \n",
       "2                32                 5                0    ...       \n",
       "\n",
       "   w2v_feature_93  w2v_feature_94  w2v_feature_95  w2v_feature_96  \\\n",
       "0        0.984288        1.852068        1.080477       -0.010269   \n",
       "1        0.255773        0.446480        0.274428        0.282043   \n",
       "2        0.230420        2.103114        1.331694       -1.781096   \n",
       "\n",
       "  w2v_feature_97  w2v_feature_98  w2v_feature_99  mws_index  eap_index  \\\n",
       "0      -7.757530       -3.017276        2.205754   0.035935   0.074388   \n",
       "1      -2.035049       -0.809668        0.596570   0.035860   0.060980   \n",
       "2      -7.162513       -2.376012        0.950710   0.026900   0.047832   \n",
       "\n",
       "   hpl_index  \n",
       "0   0.034504  \n",
       "1   0.034739  \n",
       "2   0.037337  \n",
       "\n",
       "[3 rows x 198 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add index of author\n",
    "df_train['mws_index']=df_train['cleaned_text'].apply(ind_val_mws)/df_train['length']\n",
    "df_train['eap_index']=df_train['cleaned_text'].apply(ind_val_eap)/df_train['length']\n",
    "df_train['hpl_index']=df_train['cleaned_text'].apply(ind_val_hpl)/df_train['length']\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author2</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984288</td>\n",
       "      <td>1.852068</td>\n",
       "      <td>1.080477</td>\n",
       "      <td>-0.010269</td>\n",
       "      <td>-7.757530</td>\n",
       "      <td>-3.017276</td>\n",
       "      <td>2.205754</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.074388</td>\n",
       "      <td>0.034504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255773</td>\n",
       "      <td>0.446480</td>\n",
       "      <td>0.274428</td>\n",
       "      <td>0.282043</td>\n",
       "      <td>-2.035049</td>\n",
       "      <td>-0.809668</td>\n",
       "      <td>0.596570</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.034739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>2.103114</td>\n",
       "      <td>1.331694</td>\n",
       "      <td>-1.781096</td>\n",
       "      <td>-7.162513</td>\n",
       "      <td>-2.376012</td>\n",
       "      <td>0.950710</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, love, spring, as, look, windsor, terrac,...</td>\n",
       "      <td>how love spring as look windsor terrac sixteen...</td>\n",
       "      <td>144</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434389</td>\n",
       "      <td>2.018498</td>\n",
       "      <td>1.203822</td>\n",
       "      <td>-1.145513</td>\n",
       "      <td>-7.376847</td>\n",
       "      <td>-2.525478</td>\n",
       "      <td>1.201493</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.033438</td>\n",
       "      <td>0.033601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[find, noth, els, even, gold, superintend, aba...</td>\n",
       "      <td>find noth els even gold superintend abandon at...</td>\n",
       "      <td>102</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414073</td>\n",
       "      <td>1.278332</td>\n",
       "      <td>0.731568</td>\n",
       "      <td>-0.165426</td>\n",
       "      <td>-4.971582</td>\n",
       "      <td>-1.783214</td>\n",
       "      <td>1.067079</td>\n",
       "      <td>0.036859</td>\n",
       "      <td>0.056661</td>\n",
       "      <td>0.043735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author2       id                                               text author  \\\n",
       "0        0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1        1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2        0  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3        2  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4        1  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "3  [how, love, spring, as, look, windsor, terrac,...   \n",
       "4  [find, noth, els, even, gold, superintend, aba...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "3  how love spring as look windsor terrac sixteen...     144         34   \n",
       "4  find noth els even gold superintend abandon at...     102         27   \n",
       "\n",
       "   num_unique_words  num_punctuations    ...      w2v_feature_93  \\\n",
       "0                35                 7    ...            0.984288   \n",
       "1                14                 1    ...            0.255773   \n",
       "2                32                 5    ...            0.230420   \n",
       "3                32                 4    ...            0.434389   \n",
       "4                25                 4    ...            0.414073   \n",
       "\n",
       "   w2v_feature_94  w2v_feature_95  w2v_feature_96  w2v_feature_97  \\\n",
       "0        1.852068        1.080477       -0.010269       -7.757530   \n",
       "1        0.446480        0.274428        0.282043       -2.035049   \n",
       "2        2.103114        1.331694       -1.781096       -7.162513   \n",
       "3        2.018498        1.203822       -1.145513       -7.376847   \n",
       "4        1.278332        0.731568       -0.165426       -4.971582   \n",
       "\n",
       "   w2v_feature_98  w2v_feature_99  mws_index  eap_index  hpl_index  \n",
       "0       -3.017276        2.205754   0.035935   0.074388   0.034504  \n",
       "1       -0.809668        0.596570   0.035860   0.060980   0.034739  \n",
       "2       -2.376012        0.950710   0.026900   0.047832   0.037337  \n",
       "3       -2.525478        1.201493   0.071850   0.033438   0.033601  \n",
       "4       -1.783214        1.067079   0.036859   0.056661   0.043735  \n",
       "\n",
       "[5 rows x 198 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform authors' names to numeric\n",
    "df_train['author']=df_train['author'].astype('category')\n",
    "df_train['author2']=df_train['author'].cat.codes\n",
    "# Create different features \n",
    "df_train.head(n=3)\n",
    "mid = df_train['author2']\n",
    "df_train.drop(labels=['author2'], axis=1,inplace = True)\n",
    "df_train.insert(0, 'author2', mid)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = df_train['author'].map(author_mapping_dict)\n",
    "train_id = df_train['id'].values\n",
    "test_id = df_test['id'].values\n",
    "cols_to_drop = ['id', 'text']\n",
    "train_X = df_train.drop(cols_to_drop+['author'], axis=1)\n",
    "test_X = df_test.drop(cols_to_drop, axis=1)\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf = tfidf_vec.fit_transform(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.862729297781\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=3, shuffle=True, random_state=194)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_comp = 50\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "df_test = pd.concat([df_test, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the count vectorizer ###\n",
    "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "tfidf_vec.fit(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.450918416166\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# add the predictions as new features #\n",
    "df_train[\"nb_cvec_eap\"] = pred_train[:,0]\n",
    "df_train[\"nb_cvec_hpl\"] = pred_train[:,1]\n",
    "df_train[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "df_test[\"nb_cvec_eap\"] = pred_full_test[:,0]\n",
    "df_test[\"nb_cvec_hpl\"] = pred_full_test[:,1]\n",
    "df_test[\"nb_cvec_mws\"] = pred_full_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author2</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_43</th>\n",
       "      <th>svd_word_44</th>\n",
       "      <th>svd_word_45</th>\n",
       "      <th>svd_word_46</th>\n",
       "      <th>svd_word_47</th>\n",
       "      <th>svd_word_48</th>\n",
       "      <th>svd_word_49</th>\n",
       "      <th>nb_cvec_eap</th>\n",
       "      <th>nb_cvec_hpl</th>\n",
       "      <th>nb_cvec_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060822</td>\n",
       "      <td>-0.019470</td>\n",
       "      <td>0.022374</td>\n",
       "      <td>0.019055</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.037074</td>\n",
       "      <td>0.002873</td>\n",
       "      <td>9.999933e-01</td>\n",
       "      <td>2.752790e-06</td>\n",
       "      <td>3.990111e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>-0.001032</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>-0.004602</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>-0.002629</td>\n",
       "      <td>8.226820e-01</td>\n",
       "      <td>1.492107e-01</td>\n",
       "      <td>2.810727e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031303</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>-0.014951</td>\n",
       "      <td>-0.028148</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.024380</td>\n",
       "      <td>-0.027564</td>\n",
       "      <td>9.999918e-01</td>\n",
       "      <td>8.206128e-06</td>\n",
       "      <td>1.064720e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, love, spring, as, look, windsor, terrac,...</td>\n",
       "      <td>how love spring as look windsor terrac sixteen...</td>\n",
       "      <td>144</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>-0.009910</td>\n",
       "      <td>0.015214</td>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.017966</td>\n",
       "      <td>-0.011112</td>\n",
       "      <td>1.436890e-09</td>\n",
       "      <td>7.472578e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[find, noth, els, even, gold, superintend, aba...</td>\n",
       "      <td>find noth els even gold superintend abandon at...</td>\n",
       "      <td>102</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>-0.001091</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.009183</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>8.960309e-01</td>\n",
       "      <td>1.016456e-01</td>\n",
       "      <td>2.323469e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author2       id                                               text author  \\\n",
       "0        0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1        1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2        0  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3        2  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4        1  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "3  [how, love, spring, as, look, windsor, terrac,...   \n",
       "4  [find, noth, els, even, gold, superintend, aba...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "3  how love spring as look windsor terrac sixteen...     144         34   \n",
       "4  find noth els even gold superintend abandon at...     102         27   \n",
       "\n",
       "   num_unique_words  num_punctuations      ...       svd_word_43  svd_word_44  \\\n",
       "0                35                 7      ...          0.060822    -0.019470   \n",
       "1                14                 1      ...          0.001035     0.000631   \n",
       "2                32                 5      ...         -0.031303     0.011510   \n",
       "3                32                 4      ...          0.009119    -0.009910   \n",
       "4                25                 4      ...          0.001328    -0.000561   \n",
       "\n",
       "   svd_word_45  svd_word_46  svd_word_47 svd_word_48  svd_word_49  \\\n",
       "0     0.022374     0.019055     0.001587    0.037074     0.002873   \n",
       "1    -0.001032     0.003187    -0.004602    0.004110    -0.002629   \n",
       "2    -0.014951    -0.028148     0.010121    0.024380    -0.027564   \n",
       "3     0.015214     0.013138     0.002048    0.017966    -0.011112   \n",
       "4     0.003316    -0.001091     0.000047   -0.009183    -0.002713   \n",
       "\n",
       "    nb_cvec_eap   nb_cvec_hpl   nb_cvec_mws  \n",
       "0  9.999933e-01  2.752790e-06  3.990111e-06  \n",
       "1  8.226820e-01  1.492107e-01  2.810727e-02  \n",
       "2  9.999918e-01  8.206128e-06  1.064720e-08  \n",
       "3  1.436890e-09  7.472578e-10  1.000000e+00  \n",
       "4  8.960309e-01  1.016456e-01  2.323469e-03  \n",
       "\n",
       "[5 rows x 252 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['author2', 'id', 'text', 'author', 'cleaned_text',\n",
       "       'cleaned_text_string', 'length', 'num_words', 'num_unique_words',\n",
       "       'num_punctuations', 'num_words_upper', 'num_words_title',\n",
       "       'mean_word_len', 'num_stopwords', 'lexical_diversity', 'long',\n",
       "       'see', 'year', 'happi', 'word', 'feel', 'death', 'fear', 'old',\n",
       "       'face', 'even', 'father', 'look', 'chang', 'came', 'yet', 'ever',\n",
       "       'door', 'first', 'made', 'place', 'raymond', 'heard', 'hope',\n",
       "       'time', 'littl', 'say', 'heart', 'thus', 'life', 'make', 'whose',\n",
       "       'pass', 'thought', 'never', 'room', 'men', 'great', 'still', 'come',\n",
       "       'dream', 'upon', 'found', 'love', 'well', 'mani', 'know', 'point',\n",
       "       'one', 'mind', 'howev', 'everi', 'two', 'seem', 'saw', 'appear',\n",
       "       'die', 'certain', 'light', 'man', 'like', 'shall', 'eye', 'thing',\n",
       "       'might', 'return', 'said', 'hous', 'friend', 'hand', 'street',\n",
       "       'night', 'may', 'much', 'strang', 'day', 'natur', 'must', 'though',\n",
       "       'us', 'w2v_feature_0', 'w2v_feature_1', 'w2v_feature_2',\n",
       "       'w2v_feature_3', 'w2v_feature_4', 'w2v_feature_5', 'w2v_feature_6',\n",
       "       'w2v_feature_7', 'w2v_feature_8', 'w2v_feature_9', 'w2v_feature_10',\n",
       "       'w2v_feature_11', 'w2v_feature_12', 'w2v_feature_13',\n",
       "       'w2v_feature_14', 'w2v_feature_15', 'w2v_feature_16',\n",
       "       'w2v_feature_17', 'w2v_feature_18', 'w2v_feature_19',\n",
       "       'w2v_feature_20', 'w2v_feature_21', 'w2v_feature_22',\n",
       "       'w2v_feature_23', 'w2v_feature_24', 'w2v_feature_25',\n",
       "       'w2v_feature_26', 'w2v_feature_27', 'w2v_feature_28',\n",
       "       'w2v_feature_29', 'w2v_feature_30', 'w2v_feature_31',\n",
       "       'w2v_feature_32', 'w2v_feature_33', 'w2v_feature_34',\n",
       "       'w2v_feature_35', 'w2v_feature_36', 'w2v_feature_37',\n",
       "       'w2v_feature_38', 'w2v_feature_39', 'w2v_feature_40',\n",
       "       'w2v_feature_41', 'w2v_feature_42', 'w2v_feature_43',\n",
       "       'w2v_feature_44', 'w2v_feature_45', 'w2v_feature_46',\n",
       "       'w2v_feature_47', 'w2v_feature_48', 'w2v_feature_49',\n",
       "       'w2v_feature_50', 'w2v_feature_51', 'w2v_feature_52',\n",
       "       'w2v_feature_53', 'w2v_feature_54', 'w2v_feature_55',\n",
       "       'w2v_feature_56', 'w2v_feature_57', 'w2v_feature_58',\n",
       "       'w2v_feature_59', 'w2v_feature_60', 'w2v_feature_61',\n",
       "       'w2v_feature_62', 'w2v_feature_63', 'w2v_feature_64',\n",
       "       'w2v_feature_65', 'w2v_feature_66', 'w2v_feature_67',\n",
       "       'w2v_feature_68', 'w2v_feature_69', 'w2v_feature_70',\n",
       "       'w2v_feature_71', 'w2v_feature_72', 'w2v_feature_73',\n",
       "       'w2v_feature_74', 'w2v_feature_75', 'w2v_feature_76',\n",
       "       'w2v_feature_77', 'w2v_feature_78', 'w2v_feature_79',\n",
       "       'w2v_feature_80', 'w2v_feature_81', 'w2v_feature_82',\n",
       "       'w2v_feature_83', 'w2v_feature_84', 'w2v_feature_85',\n",
       "       'w2v_feature_86', 'w2v_feature_87', 'w2v_feature_88',\n",
       "       'w2v_feature_89', 'w2v_feature_90', 'w2v_feature_91',\n",
       "       'w2v_feature_92', 'w2v_feature_93', 'w2v_feature_94',\n",
       "       'w2v_feature_95', 'w2v_feature_96', 'w2v_feature_97',\n",
       "       'w2v_feature_98', 'w2v_feature_99', 'mws_index', 'eap_index',\n",
       "       'hpl_index', 'svd_word_0', 'svd_word_1', 'svd_word_2', 'svd_word_3',\n",
       "       'svd_word_4', 'svd_word_5', 'svd_word_6', 'svd_word_7',\n",
       "       'svd_word_8', 'svd_word_9', 'svd_word_10', 'svd_word_11',\n",
       "       'svd_word_12', 'svd_word_13', 'svd_word_14', 'svd_word_15',\n",
       "       'svd_word_16', 'svd_word_17', 'svd_word_18', 'svd_word_19',\n",
       "       'svd_word_20', 'svd_word_21', 'svd_word_22', 'svd_word_23',\n",
       "       'svd_word_24', 'svd_word_25', 'svd_word_26', 'svd_word_27',\n",
       "       'svd_word_28', 'svd_word_29', 'svd_word_30', 'svd_word_31',\n",
       "       'svd_word_32', 'svd_word_33', 'svd_word_34', 'svd_word_35',\n",
       "       'svd_word_36', 'svd_word_37', 'svd_word_38', 'svd_word_39',\n",
       "       'svd_word_40', 'svd_word_41', 'svd_word_42', 'svd_word_43',\n",
       "       'svd_word_44', 'svd_word_45', 'svd_word_46', 'svd_word_47',\n",
       "       'svd_word_48', 'svd_word_49', 'nb_cvec_eap', 'nb_cvec_hpl',\n",
       "       'nb_cvec_mws'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del df_train['w2v_array']\n",
    "df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_train['nb_cvec_eap']\n",
    "del df_train['nb_cvec_hpl']\n",
    "del df_train['nb_cvec_mws']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n",
    "        print(message)\n",
    "        print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "class LemmaCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Storing the entire training text in a list\n",
    "text = list(df_train.text.values)\n",
    "# Calling our overwritten Count vectorizer\n",
    "tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n",
    "                                     min_df=2,\n",
    "                                     stop_words='english',\n",
    "                                     decode_error='ignore')\n",
    "tf = tf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=13, max_iter=5,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=13, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00452489,  0.00452489,  0.00452489, ...,  0.00452499,\n",
       "         0.06415156,  0.00452489],\n",
       "       [ 0.01538462,  0.01538462,  0.01538467, ...,  0.01538462,\n",
       "         0.01538462,  0.01538462],\n",
       "       [ 0.00404858,  0.00404859,  0.89772543, ...,  0.00404861,\n",
       "         0.00404865,  0.00404867],\n",
       "       ..., \n",
       "       [ 0.00961538,  0.00961538,  0.00961556, ...,  0.0096154 ,\n",
       "         0.00961538,  0.00961538],\n",
       "       [ 0.01098901,  0.21741942,  0.01098901, ...,  0.01098901,\n",
       "         0.01098901,  0.01098906],\n",
       "       [ 0.00961538,  0.00961538,  0.13461525, ...,  0.00961563,\n",
       "         0.00961538,  0.00961544]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create data set\n",
    "ds_train=df_train.values\n",
    "X=ds_train[:, 6:]\n",
    "Y=ds_train[:, 0]\n",
    "seed=7\n",
    "test_size=0.3\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X,Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.921491\ttest-mlogloss:0.922556\n",
      "[1]\ttrain-mlogloss:0.798604\ttest-mlogloss:0.800504\n",
      "[2]\ttrain-mlogloss:0.708918\ttest-mlogloss:0.711609\n",
      "[3]\ttrain-mlogloss:0.641138\ttest-mlogloss:0.644234\n",
      "[4]\ttrain-mlogloss:0.589194\ttest-mlogloss:0.592531\n",
      "[5]\ttrain-mlogloss:0.54839\ttest-mlogloss:0.552153\n",
      "[6]\ttrain-mlogloss:0.516373\ttest-mlogloss:0.520772\n",
      "[7]\ttrain-mlogloss:0.490757\ttest-mlogloss:0.495507\n",
      "[8]\ttrain-mlogloss:0.469581\ttest-mlogloss:0.475061\n",
      "[9]\ttrain-mlogloss:0.452602\ttest-mlogloss:0.458706\n",
      "[10]\ttrain-mlogloss:0.438182\ttest-mlogloss:0.44437\n",
      "[11]\ttrain-mlogloss:0.426372\ttest-mlogloss:0.432798\n",
      "[12]\ttrain-mlogloss:0.4165\ttest-mlogloss:0.423322\n",
      "[13]\ttrain-mlogloss:0.408062\ttest-mlogloss:0.415548\n",
      "[14]\ttrain-mlogloss:0.40077\ttest-mlogloss:0.408611\n",
      "[15]\ttrain-mlogloss:0.394704\ttest-mlogloss:0.402764\n",
      "[16]\ttrain-mlogloss:0.389174\ttest-mlogloss:0.397481\n",
      "[17]\ttrain-mlogloss:0.384247\ttest-mlogloss:0.393244\n",
      "[18]\ttrain-mlogloss:0.380209\ttest-mlogloss:0.389446\n",
      "[19]\ttrain-mlogloss:0.376382\ttest-mlogloss:0.386165\n",
      "[20]\ttrain-mlogloss:0.372874\ttest-mlogloss:0.383272\n",
      "[21]\ttrain-mlogloss:0.369807\ttest-mlogloss:0.380355\n",
      "[22]\ttrain-mlogloss:0.367108\ttest-mlogloss:0.378179\n",
      "[23]\ttrain-mlogloss:0.364248\ttest-mlogloss:0.37589\n",
      "[24]\ttrain-mlogloss:0.361871\ttest-mlogloss:0.373916\n",
      "[25]\ttrain-mlogloss:0.35933\ttest-mlogloss:0.371738\n",
      "[26]\ttrain-mlogloss:0.357122\ttest-mlogloss:0.370107\n",
      "[27]\ttrain-mlogloss:0.355187\ttest-mlogloss:0.368833\n",
      "[28]\ttrain-mlogloss:0.353017\ttest-mlogloss:0.367314\n",
      "[29]\ttrain-mlogloss:0.351283\ttest-mlogloss:0.365844\n",
      "[30]\ttrain-mlogloss:0.349667\ttest-mlogloss:0.36465\n",
      "[31]\ttrain-mlogloss:0.347984\ttest-mlogloss:0.363512\n",
      "[32]\ttrain-mlogloss:0.346273\ttest-mlogloss:0.362436\n",
      "[33]\ttrain-mlogloss:0.344756\ttest-mlogloss:0.361309\n",
      "[34]\ttrain-mlogloss:0.343422\ttest-mlogloss:0.360442\n",
      "[35]\ttrain-mlogloss:0.342238\ttest-mlogloss:0.359547\n",
      "[36]\ttrain-mlogloss:0.340702\ttest-mlogloss:0.3584\n",
      "[37]\ttrain-mlogloss:0.339433\ttest-mlogloss:0.357341\n",
      "[38]\ttrain-mlogloss:0.338227\ttest-mlogloss:0.356714\n",
      "[39]\ttrain-mlogloss:0.336746\ttest-mlogloss:0.355786\n",
      "[40]\ttrain-mlogloss:0.335556\ttest-mlogloss:0.354983\n",
      "[41]\ttrain-mlogloss:0.334403\ttest-mlogloss:0.354302\n",
      "[42]\ttrain-mlogloss:0.333291\ttest-mlogloss:0.353613\n",
      "[43]\ttrain-mlogloss:0.332053\ttest-mlogloss:0.352872\n",
      "[44]\ttrain-mlogloss:0.33099\ttest-mlogloss:0.352244\n",
      "[45]\ttrain-mlogloss:0.329789\ttest-mlogloss:0.351363\n",
      "[46]\ttrain-mlogloss:0.328755\ttest-mlogloss:0.350791\n",
      "[47]\ttrain-mlogloss:0.327752\ttest-mlogloss:0.350437\n",
      "[48]\ttrain-mlogloss:0.326766\ttest-mlogloss:0.349765\n",
      "[49]\ttrain-mlogloss:0.325855\ttest-mlogloss:0.349313\n",
      "[50]\ttrain-mlogloss:0.324758\ttest-mlogloss:0.348701\n",
      "[51]\ttrain-mlogloss:0.323943\ttest-mlogloss:0.348295\n",
      "[52]\ttrain-mlogloss:0.322922\ttest-mlogloss:0.347859\n",
      "[53]\ttrain-mlogloss:0.322023\ttest-mlogloss:0.347291\n",
      "[54]\ttrain-mlogloss:0.320991\ttest-mlogloss:0.34656\n",
      "[55]\ttrain-mlogloss:0.319947\ttest-mlogloss:0.346231\n",
      "[56]\ttrain-mlogloss:0.318968\ttest-mlogloss:0.345606\n",
      "[57]\ttrain-mlogloss:0.318241\ttest-mlogloss:0.345216\n",
      "[58]\ttrain-mlogloss:0.317446\ttest-mlogloss:0.344695\n",
      "[59]\ttrain-mlogloss:0.316675\ttest-mlogloss:0.3442\n",
      "[60]\ttrain-mlogloss:0.315812\ttest-mlogloss:0.343815\n",
      "[61]\ttrain-mlogloss:0.314948\ttest-mlogloss:0.343477\n",
      "[62]\ttrain-mlogloss:0.314045\ttest-mlogloss:0.343234\n",
      "[63]\ttrain-mlogloss:0.313184\ttest-mlogloss:0.342837\n",
      "[64]\ttrain-mlogloss:0.312344\ttest-mlogloss:0.34259\n",
      "[65]\ttrain-mlogloss:0.311792\ttest-mlogloss:0.342345\n",
      "[66]\ttrain-mlogloss:0.31109\ttest-mlogloss:0.342063\n",
      "[67]\ttrain-mlogloss:0.31029\ttest-mlogloss:0.341568\n",
      "[68]\ttrain-mlogloss:0.309638\ttest-mlogloss:0.341355\n",
      "[69]\ttrain-mlogloss:0.30885\ttest-mlogloss:0.34109\n",
      "[70]\ttrain-mlogloss:0.30816\ttest-mlogloss:0.340889\n",
      "[71]\ttrain-mlogloss:0.307484\ttest-mlogloss:0.340622\n",
      "[72]\ttrain-mlogloss:0.306774\ttest-mlogloss:0.340146\n",
      "[73]\ttrain-mlogloss:0.305997\ttest-mlogloss:0.339937\n",
      "[74]\ttrain-mlogloss:0.305341\ttest-mlogloss:0.339637\n",
      "[75]\ttrain-mlogloss:0.30464\ttest-mlogloss:0.339418\n",
      "[76]\ttrain-mlogloss:0.303847\ttest-mlogloss:0.339149\n",
      "[77]\ttrain-mlogloss:0.303013\ttest-mlogloss:0.338789\n",
      "[78]\ttrain-mlogloss:0.302311\ttest-mlogloss:0.338775\n",
      "[79]\ttrain-mlogloss:0.301651\ttest-mlogloss:0.338397\n",
      "[80]\ttrain-mlogloss:0.301019\ttest-mlogloss:0.338019\n",
      "[81]\ttrain-mlogloss:0.300483\ttest-mlogloss:0.33771\n",
      "[82]\ttrain-mlogloss:0.299976\ttest-mlogloss:0.337479\n",
      "[83]\ttrain-mlogloss:0.299326\ttest-mlogloss:0.337221\n",
      "[84]\ttrain-mlogloss:0.298647\ttest-mlogloss:0.337124\n",
      "[85]\ttrain-mlogloss:0.298069\ttest-mlogloss:0.337099\n",
      "[86]\ttrain-mlogloss:0.297523\ttest-mlogloss:0.336899\n",
      "[87]\ttrain-mlogloss:0.296953\ttest-mlogloss:0.336737\n",
      "[88]\ttrain-mlogloss:0.296447\ttest-mlogloss:0.33652\n",
      "[89]\ttrain-mlogloss:0.295895\ttest-mlogloss:0.33657\n",
      "[90]\ttrain-mlogloss:0.295467\ttest-mlogloss:0.336401\n",
      "[91]\ttrain-mlogloss:0.294898\ttest-mlogloss:0.336209\n",
      "[92]\ttrain-mlogloss:0.294346\ttest-mlogloss:0.336148\n",
      "[93]\ttrain-mlogloss:0.293747\ttest-mlogloss:0.336058\n",
      "[94]\ttrain-mlogloss:0.293231\ttest-mlogloss:0.335899\n",
      "[95]\ttrain-mlogloss:0.29262\ttest-mlogloss:0.335692\n",
      "[96]\ttrain-mlogloss:0.292116\ttest-mlogloss:0.335511\n",
      "[97]\ttrain-mlogloss:0.291575\ttest-mlogloss:0.335258\n",
      "[98]\ttrain-mlogloss:0.291008\ttest-mlogloss:0.335038\n",
      "[99]\ttrain-mlogloss:0.290621\ttest-mlogloss:0.334904\n",
      "[100]\ttrain-mlogloss:0.290158\ttest-mlogloss:0.334801\n",
      "[101]\ttrain-mlogloss:0.289555\ttest-mlogloss:0.334543\n",
      "[102]\ttrain-mlogloss:0.28906\ttest-mlogloss:0.334467\n",
      "[103]\ttrain-mlogloss:0.288533\ttest-mlogloss:0.334321\n",
      "[104]\ttrain-mlogloss:0.287963\ttest-mlogloss:0.334069\n",
      "[105]\ttrain-mlogloss:0.287434\ttest-mlogloss:0.333859\n",
      "[106]\ttrain-mlogloss:0.287002\ttest-mlogloss:0.333796\n",
      "[107]\ttrain-mlogloss:0.286375\ttest-mlogloss:0.333588\n",
      "[108]\ttrain-mlogloss:0.285917\ttest-mlogloss:0.333486\n",
      "[109]\ttrain-mlogloss:0.285371\ttest-mlogloss:0.333315\n",
      "[110]\ttrain-mlogloss:0.285011\ttest-mlogloss:0.333114\n",
      "[111]\ttrain-mlogloss:0.284494\ttest-mlogloss:0.333023\n",
      "[112]\ttrain-mlogloss:0.283938\ttest-mlogloss:0.332901\n",
      "[113]\ttrain-mlogloss:0.283406\ttest-mlogloss:0.332726\n",
      "[114]\ttrain-mlogloss:0.283001\ttest-mlogloss:0.332596\n",
      "[115]\ttrain-mlogloss:0.282554\ttest-mlogloss:0.332448\n",
      "[116]\ttrain-mlogloss:0.282152\ttest-mlogloss:0.332277\n",
      "[117]\ttrain-mlogloss:0.281537\ttest-mlogloss:0.332124\n",
      "[118]\ttrain-mlogloss:0.281093\ttest-mlogloss:0.331925\n",
      "[119]\ttrain-mlogloss:0.280709\ttest-mlogloss:0.331762\n",
      "[120]\ttrain-mlogloss:0.280217\ttest-mlogloss:0.331616\n",
      "[121]\ttrain-mlogloss:0.27983\ttest-mlogloss:0.33155\n",
      "[122]\ttrain-mlogloss:0.279443\ttest-mlogloss:0.331384\n",
      "[123]\ttrain-mlogloss:0.279089\ttest-mlogloss:0.331406\n",
      "[124]\ttrain-mlogloss:0.278686\ttest-mlogloss:0.331292\n",
      "[125]\ttrain-mlogloss:0.278252\ttest-mlogloss:0.331209\n",
      "[126]\ttrain-mlogloss:0.277738\ttest-mlogloss:0.330999\n",
      "[127]\ttrain-mlogloss:0.277313\ttest-mlogloss:0.330964\n",
      "[128]\ttrain-mlogloss:0.276792\ttest-mlogloss:0.330778\n",
      "[129]\ttrain-mlogloss:0.276405\ttest-mlogloss:0.330739\n",
      "[130]\ttrain-mlogloss:0.276034\ttest-mlogloss:0.330857\n",
      "[131]\ttrain-mlogloss:0.275636\ttest-mlogloss:0.330819\n",
      "[132]\ttrain-mlogloss:0.275207\ttest-mlogloss:0.330757\n",
      "[133]\ttrain-mlogloss:0.274798\ttest-mlogloss:0.330501\n",
      "[134]\ttrain-mlogloss:0.274407\ttest-mlogloss:0.330361\n",
      "[135]\ttrain-mlogloss:0.273933\ttest-mlogloss:0.33038\n",
      "[136]\ttrain-mlogloss:0.273449\ttest-mlogloss:0.330412\n",
      "[137]\ttrain-mlogloss:0.273136\ttest-mlogloss:0.330359\n",
      "[138]\ttrain-mlogloss:0.272723\ttest-mlogloss:0.330404\n",
      "[139]\ttrain-mlogloss:0.272367\ttest-mlogloss:0.330456\n",
      "[140]\ttrain-mlogloss:0.27197\ttest-mlogloss:0.330482\n",
      "[141]\ttrain-mlogloss:0.271603\ttest-mlogloss:0.330529\n",
      "[142]\ttrain-mlogloss:0.271184\ttest-mlogloss:0.330509\n",
      "[143]\ttrain-mlogloss:0.270792\ttest-mlogloss:0.330523\n",
      "[144]\ttrain-mlogloss:0.270361\ttest-mlogloss:0.330293\n",
      "[145]\ttrain-mlogloss:0.269941\ttest-mlogloss:0.33011\n",
      "[146]\ttrain-mlogloss:0.269596\ttest-mlogloss:0.329972\n",
      "[147]\ttrain-mlogloss:0.269206\ttest-mlogloss:0.329806\n",
      "[148]\ttrain-mlogloss:0.268801\ttest-mlogloss:0.329703\n",
      "[149]\ttrain-mlogloss:0.268407\ttest-mlogloss:0.329689\n",
      "[150]\ttrain-mlogloss:0.268025\ttest-mlogloss:0.329696\n",
      "[151]\ttrain-mlogloss:0.267601\ttest-mlogloss:0.329559\n",
      "[152]\ttrain-mlogloss:0.267266\ttest-mlogloss:0.329505\n",
      "[153]\ttrain-mlogloss:0.266843\ttest-mlogloss:0.329441\n",
      "[154]\ttrain-mlogloss:0.266478\ttest-mlogloss:0.329432\n",
      "[155]\ttrain-mlogloss:0.266197\ttest-mlogloss:0.329371\n",
      "[156]\ttrain-mlogloss:0.265811\ttest-mlogloss:0.329354\n",
      "[157]\ttrain-mlogloss:0.265426\ttest-mlogloss:0.329231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158]\ttrain-mlogloss:0.265062\ttest-mlogloss:0.329184\n",
      "[159]\ttrain-mlogloss:0.264783\ttest-mlogloss:0.329138\n",
      "[160]\ttrain-mlogloss:0.264363\ttest-mlogloss:0.329166\n",
      "[161]\ttrain-mlogloss:0.264102\ttest-mlogloss:0.329087\n",
      "[162]\ttrain-mlogloss:0.263768\ttest-mlogloss:0.329008\n",
      "[163]\ttrain-mlogloss:0.263426\ttest-mlogloss:0.328956\n",
      "[164]\ttrain-mlogloss:0.262933\ttest-mlogloss:0.328963\n",
      "[165]\ttrain-mlogloss:0.262586\ttest-mlogloss:0.328765\n",
      "[166]\ttrain-mlogloss:0.262196\ttest-mlogloss:0.328762\n",
      "[167]\ttrain-mlogloss:0.261851\ttest-mlogloss:0.328684\n",
      "[168]\ttrain-mlogloss:0.261571\ttest-mlogloss:0.328725\n",
      "[169]\ttrain-mlogloss:0.26122\ttest-mlogloss:0.328756\n",
      "[170]\ttrain-mlogloss:0.260942\ttest-mlogloss:0.328775\n",
      "[171]\ttrain-mlogloss:0.260638\ttest-mlogloss:0.328852\n",
      "[172]\ttrain-mlogloss:0.260242\ttest-mlogloss:0.328796\n",
      "[173]\ttrain-mlogloss:0.259864\ttest-mlogloss:0.328758\n",
      "[174]\ttrain-mlogloss:0.25949\ttest-mlogloss:0.328719\n",
      "[175]\ttrain-mlogloss:0.259133\ttest-mlogloss:0.328689\n",
      "[176]\ttrain-mlogloss:0.258801\ttest-mlogloss:0.328684\n",
      "[177]\ttrain-mlogloss:0.258406\ttest-mlogloss:0.328656\n",
      "[178]\ttrain-mlogloss:0.258058\ttest-mlogloss:0.328553\n",
      "[179]\ttrain-mlogloss:0.257809\ttest-mlogloss:0.328537\n",
      "[180]\ttrain-mlogloss:0.257542\ttest-mlogloss:0.328509\n",
      "[181]\ttrain-mlogloss:0.257138\ttest-mlogloss:0.328314\n",
      "[182]\ttrain-mlogloss:0.256819\ttest-mlogloss:0.328378\n",
      "[183]\ttrain-mlogloss:0.256489\ttest-mlogloss:0.328309\n",
      "[184]\ttrain-mlogloss:0.256191\ttest-mlogloss:0.328303\n",
      "[185]\ttrain-mlogloss:0.255863\ttest-mlogloss:0.328286\n",
      "[186]\ttrain-mlogloss:0.255521\ttest-mlogloss:0.328164\n",
      "[187]\ttrain-mlogloss:0.255207\ttest-mlogloss:0.328122\n",
      "[188]\ttrain-mlogloss:0.254821\ttest-mlogloss:0.327938\n",
      "[189]\ttrain-mlogloss:0.254453\ttest-mlogloss:0.327873\n",
      "[190]\ttrain-mlogloss:0.254038\ttest-mlogloss:0.327738\n",
      "[191]\ttrain-mlogloss:0.253683\ttest-mlogloss:0.327678\n",
      "[192]\ttrain-mlogloss:0.253394\ttest-mlogloss:0.327645\n",
      "[193]\ttrain-mlogloss:0.25312\ttest-mlogloss:0.327614\n",
      "[194]\ttrain-mlogloss:0.252756\ttest-mlogloss:0.327683\n",
      "[195]\ttrain-mlogloss:0.252433\ttest-mlogloss:0.327634\n",
      "[196]\ttrain-mlogloss:0.252152\ttest-mlogloss:0.327689\n",
      "[197]\ttrain-mlogloss:0.251768\ttest-mlogloss:0.327594\n",
      "[198]\ttrain-mlogloss:0.251428\ttest-mlogloss:0.327529\n",
      "[199]\ttrain-mlogloss:0.251015\ttest-mlogloss:0.327386\n",
      "[200]\ttrain-mlogloss:0.250701\ttest-mlogloss:0.327445\n",
      "[201]\ttrain-mlogloss:0.25041\ttest-mlogloss:0.32743\n",
      "[202]\ttrain-mlogloss:0.250129\ttest-mlogloss:0.32728\n",
      "[203]\ttrain-mlogloss:0.249814\ttest-mlogloss:0.327287\n",
      "[204]\ttrain-mlogloss:0.249528\ttest-mlogloss:0.327227\n",
      "[205]\ttrain-mlogloss:0.249218\ttest-mlogloss:0.327203\n",
      "[206]\ttrain-mlogloss:0.248865\ttest-mlogloss:0.327242\n",
      "[207]\ttrain-mlogloss:0.24862\ttest-mlogloss:0.327242\n",
      "[208]\ttrain-mlogloss:0.248402\ttest-mlogloss:0.327198\n",
      "[209]\ttrain-mlogloss:0.248142\ttest-mlogloss:0.327112\n",
      "[210]\ttrain-mlogloss:0.247833\ttest-mlogloss:0.327158\n",
      "[211]\ttrain-mlogloss:0.247543\ttest-mlogloss:0.327034\n",
      "[212]\ttrain-mlogloss:0.247225\ttest-mlogloss:0.32705\n",
      "[213]\ttrain-mlogloss:0.246954\ttest-mlogloss:0.326999\n",
      "[214]\ttrain-mlogloss:0.246732\ttest-mlogloss:0.326953\n",
      "[215]\ttrain-mlogloss:0.246392\ttest-mlogloss:0.326941\n",
      "[216]\ttrain-mlogloss:0.246048\ttest-mlogloss:0.326884\n",
      "[217]\ttrain-mlogloss:0.245797\ttest-mlogloss:0.326849\n",
      "[218]\ttrain-mlogloss:0.24548\ttest-mlogloss:0.326684\n",
      "[219]\ttrain-mlogloss:0.245182\ttest-mlogloss:0.326643\n",
      "[220]\ttrain-mlogloss:0.244847\ttest-mlogloss:0.326581\n",
      "[221]\ttrain-mlogloss:0.244566\ttest-mlogloss:0.326536\n",
      "[222]\ttrain-mlogloss:0.244272\ttest-mlogloss:0.326433\n",
      "[223]\ttrain-mlogloss:0.243941\ttest-mlogloss:0.326509\n",
      "[224]\ttrain-mlogloss:0.243616\ttest-mlogloss:0.32657\n",
      "[225]\ttrain-mlogloss:0.243313\ttest-mlogloss:0.326535\n",
      "[226]\ttrain-mlogloss:0.243044\ttest-mlogloss:0.326591\n",
      "[227]\ttrain-mlogloss:0.242821\ttest-mlogloss:0.326696\n",
      "[228]\ttrain-mlogloss:0.242637\ttest-mlogloss:0.326652\n",
      "[229]\ttrain-mlogloss:0.24241\ttest-mlogloss:0.326645\n",
      "[230]\ttrain-mlogloss:0.242069\ttest-mlogloss:0.326719\n",
      "[231]\ttrain-mlogloss:0.241806\ttest-mlogloss:0.326576\n",
      "[232]\ttrain-mlogloss:0.241552\ttest-mlogloss:0.326578\n",
      "[233]\ttrain-mlogloss:0.241308\ttest-mlogloss:0.32661\n",
      "[234]\ttrain-mlogloss:0.241067\ttest-mlogloss:0.326631\n",
      "[235]\ttrain-mlogloss:0.2408\ttest-mlogloss:0.32665\n",
      "[236]\ttrain-mlogloss:0.240528\ttest-mlogloss:0.326686\n",
      "[237]\ttrain-mlogloss:0.24025\ttest-mlogloss:0.326748\n",
      "[238]\ttrain-mlogloss:0.239962\ttest-mlogloss:0.326707\n",
      "[239]\ttrain-mlogloss:0.239748\ttest-mlogloss:0.326819\n",
      "Test error using softmax = 0.12751106571331292\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "xg_train=xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test=xgb.DMatrix(X_test, label=y_test)\n",
    "xg_t=xgb.DMatrix(X, label=Y)\n",
    "param={}\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['eta'] = 0.2\n",
    "param['max_depth'] = 2\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 3\n",
    "param['eval_metric']= \"mlogloss\"\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 240\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# get prediction\n",
    "pred = bst.predict(xg_test)\n",
    "error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.921491\ttest-mlogloss:0.922556\n",
      "[1]\ttrain-mlogloss:0.798604\ttest-mlogloss:0.800504\n",
      "[2]\ttrain-mlogloss:0.708918\ttest-mlogloss:0.711609\n",
      "[3]\ttrain-mlogloss:0.641138\ttest-mlogloss:0.644234\n",
      "[4]\ttrain-mlogloss:0.589194\ttest-mlogloss:0.592531\n",
      "[5]\ttrain-mlogloss:0.54839\ttest-mlogloss:0.552153\n",
      "[6]\ttrain-mlogloss:0.516373\ttest-mlogloss:0.520772\n",
      "[7]\ttrain-mlogloss:0.490757\ttest-mlogloss:0.495507\n",
      "[8]\ttrain-mlogloss:0.469581\ttest-mlogloss:0.475061\n",
      "[9]\ttrain-mlogloss:0.452602\ttest-mlogloss:0.458706\n",
      "[10]\ttrain-mlogloss:0.438182\ttest-mlogloss:0.44437\n",
      "[11]\ttrain-mlogloss:0.426372\ttest-mlogloss:0.432798\n",
      "[12]\ttrain-mlogloss:0.4165\ttest-mlogloss:0.423322\n",
      "[13]\ttrain-mlogloss:0.408062\ttest-mlogloss:0.415548\n",
      "[14]\ttrain-mlogloss:0.40077\ttest-mlogloss:0.408611\n",
      "[15]\ttrain-mlogloss:0.394704\ttest-mlogloss:0.402764\n",
      "[16]\ttrain-mlogloss:0.389174\ttest-mlogloss:0.397481\n",
      "[17]\ttrain-mlogloss:0.384247\ttest-mlogloss:0.393244\n",
      "[18]\ttrain-mlogloss:0.380209\ttest-mlogloss:0.389446\n",
      "[19]\ttrain-mlogloss:0.376382\ttest-mlogloss:0.386165\n",
      "[20]\ttrain-mlogloss:0.372874\ttest-mlogloss:0.383272\n",
      "[21]\ttrain-mlogloss:0.369807\ttest-mlogloss:0.380355\n",
      "[22]\ttrain-mlogloss:0.367108\ttest-mlogloss:0.378179\n",
      "[23]\ttrain-mlogloss:0.364248\ttest-mlogloss:0.37589\n",
      "[24]\ttrain-mlogloss:0.361871\ttest-mlogloss:0.373916\n",
      "[25]\ttrain-mlogloss:0.35933\ttest-mlogloss:0.371738\n",
      "[26]\ttrain-mlogloss:0.357122\ttest-mlogloss:0.370107\n",
      "[27]\ttrain-mlogloss:0.355187\ttest-mlogloss:0.368833\n",
      "[28]\ttrain-mlogloss:0.353017\ttest-mlogloss:0.367314\n",
      "[29]\ttrain-mlogloss:0.351283\ttest-mlogloss:0.365844\n",
      "[30]\ttrain-mlogloss:0.349667\ttest-mlogloss:0.36465\n",
      "[31]\ttrain-mlogloss:0.347984\ttest-mlogloss:0.363512\n",
      "[32]\ttrain-mlogloss:0.346273\ttest-mlogloss:0.362436\n",
      "[33]\ttrain-mlogloss:0.344756\ttest-mlogloss:0.361309\n",
      "[34]\ttrain-mlogloss:0.343422\ttest-mlogloss:0.360442\n",
      "[35]\ttrain-mlogloss:0.342238\ttest-mlogloss:0.359547\n",
      "[36]\ttrain-mlogloss:0.340702\ttest-mlogloss:0.3584\n",
      "[37]\ttrain-mlogloss:0.339433\ttest-mlogloss:0.357341\n",
      "[38]\ttrain-mlogloss:0.338227\ttest-mlogloss:0.356714\n",
      "[39]\ttrain-mlogloss:0.336746\ttest-mlogloss:0.355786\n",
      "[40]\ttrain-mlogloss:0.335556\ttest-mlogloss:0.354983\n",
      "[41]\ttrain-mlogloss:0.334403\ttest-mlogloss:0.354302\n",
      "[42]\ttrain-mlogloss:0.333291\ttest-mlogloss:0.353613\n",
      "[43]\ttrain-mlogloss:0.332053\ttest-mlogloss:0.352872\n",
      "[44]\ttrain-mlogloss:0.33099\ttest-mlogloss:0.352244\n",
      "[45]\ttrain-mlogloss:0.329789\ttest-mlogloss:0.351363\n",
      "[46]\ttrain-mlogloss:0.328755\ttest-mlogloss:0.350791\n",
      "[47]\ttrain-mlogloss:0.327752\ttest-mlogloss:0.350437\n",
      "[48]\ttrain-mlogloss:0.326766\ttest-mlogloss:0.349765\n",
      "[49]\ttrain-mlogloss:0.325855\ttest-mlogloss:0.349313\n",
      "[50]\ttrain-mlogloss:0.324758\ttest-mlogloss:0.348701\n",
      "[51]\ttrain-mlogloss:0.323943\ttest-mlogloss:0.348295\n",
      "[52]\ttrain-mlogloss:0.322922\ttest-mlogloss:0.347859\n",
      "[53]\ttrain-mlogloss:0.322023\ttest-mlogloss:0.347291\n",
      "[54]\ttrain-mlogloss:0.320991\ttest-mlogloss:0.34656\n",
      "[55]\ttrain-mlogloss:0.319947\ttest-mlogloss:0.346231\n",
      "[56]\ttrain-mlogloss:0.318968\ttest-mlogloss:0.345606\n",
      "[57]\ttrain-mlogloss:0.318241\ttest-mlogloss:0.345216\n",
      "[58]\ttrain-mlogloss:0.317446\ttest-mlogloss:0.344695\n",
      "[59]\ttrain-mlogloss:0.316675\ttest-mlogloss:0.3442\n",
      "[60]\ttrain-mlogloss:0.315812\ttest-mlogloss:0.343815\n",
      "[61]\ttrain-mlogloss:0.314948\ttest-mlogloss:0.343477\n",
      "[62]\ttrain-mlogloss:0.314045\ttest-mlogloss:0.343234\n",
      "[63]\ttrain-mlogloss:0.313184\ttest-mlogloss:0.342837\n",
      "[64]\ttrain-mlogloss:0.312344\ttest-mlogloss:0.34259\n",
      "[65]\ttrain-mlogloss:0.311792\ttest-mlogloss:0.342345\n",
      "[66]\ttrain-mlogloss:0.31109\ttest-mlogloss:0.342063\n",
      "[67]\ttrain-mlogloss:0.31029\ttest-mlogloss:0.341568\n",
      "[68]\ttrain-mlogloss:0.309638\ttest-mlogloss:0.341355\n",
      "[69]\ttrain-mlogloss:0.30885\ttest-mlogloss:0.34109\n",
      "[70]\ttrain-mlogloss:0.30816\ttest-mlogloss:0.340889\n",
      "[71]\ttrain-mlogloss:0.307484\ttest-mlogloss:0.340622\n",
      "[72]\ttrain-mlogloss:0.306774\ttest-mlogloss:0.340146\n",
      "[73]\ttrain-mlogloss:0.305997\ttest-mlogloss:0.339937\n",
      "[74]\ttrain-mlogloss:0.305341\ttest-mlogloss:0.339637\n",
      "[75]\ttrain-mlogloss:0.30464\ttest-mlogloss:0.339418\n",
      "[76]\ttrain-mlogloss:0.303847\ttest-mlogloss:0.339149\n",
      "[77]\ttrain-mlogloss:0.303013\ttest-mlogloss:0.338789\n",
      "[78]\ttrain-mlogloss:0.302311\ttest-mlogloss:0.338775\n",
      "[79]\ttrain-mlogloss:0.301651\ttest-mlogloss:0.338397\n",
      "[80]\ttrain-mlogloss:0.301019\ttest-mlogloss:0.338019\n",
      "[81]\ttrain-mlogloss:0.300483\ttest-mlogloss:0.33771\n",
      "[82]\ttrain-mlogloss:0.299976\ttest-mlogloss:0.337479\n",
      "[83]\ttrain-mlogloss:0.299326\ttest-mlogloss:0.337221\n",
      "[84]\ttrain-mlogloss:0.298647\ttest-mlogloss:0.337124\n",
      "[85]\ttrain-mlogloss:0.298069\ttest-mlogloss:0.337099\n",
      "[86]\ttrain-mlogloss:0.297523\ttest-mlogloss:0.336899\n",
      "[87]\ttrain-mlogloss:0.296953\ttest-mlogloss:0.336737\n",
      "[88]\ttrain-mlogloss:0.296447\ttest-mlogloss:0.33652\n",
      "[89]\ttrain-mlogloss:0.295895\ttest-mlogloss:0.33657\n",
      "[90]\ttrain-mlogloss:0.295467\ttest-mlogloss:0.336401\n",
      "[91]\ttrain-mlogloss:0.294898\ttest-mlogloss:0.336209\n",
      "[92]\ttrain-mlogloss:0.294346\ttest-mlogloss:0.336148\n",
      "[93]\ttrain-mlogloss:0.293747\ttest-mlogloss:0.336058\n",
      "[94]\ttrain-mlogloss:0.293231\ttest-mlogloss:0.335899\n",
      "[95]\ttrain-mlogloss:0.29262\ttest-mlogloss:0.335692\n",
      "[96]\ttrain-mlogloss:0.292116\ttest-mlogloss:0.335511\n",
      "[97]\ttrain-mlogloss:0.291575\ttest-mlogloss:0.335258\n",
      "[98]\ttrain-mlogloss:0.291008\ttest-mlogloss:0.335038\n",
      "[99]\ttrain-mlogloss:0.290621\ttest-mlogloss:0.334904\n",
      "[100]\ttrain-mlogloss:0.290158\ttest-mlogloss:0.334801\n",
      "[101]\ttrain-mlogloss:0.289555\ttest-mlogloss:0.334543\n",
      "[102]\ttrain-mlogloss:0.28906\ttest-mlogloss:0.334467\n",
      "[103]\ttrain-mlogloss:0.288533\ttest-mlogloss:0.334321\n",
      "[104]\ttrain-mlogloss:0.287963\ttest-mlogloss:0.334069\n",
      "[105]\ttrain-mlogloss:0.287434\ttest-mlogloss:0.333859\n",
      "[106]\ttrain-mlogloss:0.287002\ttest-mlogloss:0.333796\n",
      "[107]\ttrain-mlogloss:0.286375\ttest-mlogloss:0.333588\n",
      "[108]\ttrain-mlogloss:0.285917\ttest-mlogloss:0.333486\n",
      "[109]\ttrain-mlogloss:0.285371\ttest-mlogloss:0.333315\n",
      "[110]\ttrain-mlogloss:0.285011\ttest-mlogloss:0.333114\n",
      "[111]\ttrain-mlogloss:0.284494\ttest-mlogloss:0.333023\n",
      "[112]\ttrain-mlogloss:0.283938\ttest-mlogloss:0.332901\n",
      "[113]\ttrain-mlogloss:0.283406\ttest-mlogloss:0.332726\n",
      "[114]\ttrain-mlogloss:0.283001\ttest-mlogloss:0.332596\n",
      "[115]\ttrain-mlogloss:0.282554\ttest-mlogloss:0.332448\n",
      "[116]\ttrain-mlogloss:0.282152\ttest-mlogloss:0.332277\n",
      "[117]\ttrain-mlogloss:0.281537\ttest-mlogloss:0.332124\n",
      "[118]\ttrain-mlogloss:0.281093\ttest-mlogloss:0.331925\n",
      "[119]\ttrain-mlogloss:0.280709\ttest-mlogloss:0.331762\n",
      "[120]\ttrain-mlogloss:0.280217\ttest-mlogloss:0.331616\n",
      "[121]\ttrain-mlogloss:0.27983\ttest-mlogloss:0.33155\n",
      "[122]\ttrain-mlogloss:0.279443\ttest-mlogloss:0.331384\n",
      "[123]\ttrain-mlogloss:0.279089\ttest-mlogloss:0.331406\n",
      "[124]\ttrain-mlogloss:0.278686\ttest-mlogloss:0.331292\n",
      "[125]\ttrain-mlogloss:0.278252\ttest-mlogloss:0.331209\n",
      "[126]\ttrain-mlogloss:0.277738\ttest-mlogloss:0.330999\n",
      "[127]\ttrain-mlogloss:0.277313\ttest-mlogloss:0.330964\n",
      "[128]\ttrain-mlogloss:0.276792\ttest-mlogloss:0.330778\n",
      "[129]\ttrain-mlogloss:0.276405\ttest-mlogloss:0.330739\n",
      "[130]\ttrain-mlogloss:0.276034\ttest-mlogloss:0.330857\n",
      "[131]\ttrain-mlogloss:0.275636\ttest-mlogloss:0.330819\n",
      "[132]\ttrain-mlogloss:0.275207\ttest-mlogloss:0.330757\n",
      "[133]\ttrain-mlogloss:0.274798\ttest-mlogloss:0.330501\n",
      "[134]\ttrain-mlogloss:0.274407\ttest-mlogloss:0.330361\n",
      "[135]\ttrain-mlogloss:0.273933\ttest-mlogloss:0.33038\n",
      "[136]\ttrain-mlogloss:0.273449\ttest-mlogloss:0.330412\n",
      "[137]\ttrain-mlogloss:0.273136\ttest-mlogloss:0.330359\n",
      "[138]\ttrain-mlogloss:0.272723\ttest-mlogloss:0.330404\n",
      "[139]\ttrain-mlogloss:0.272367\ttest-mlogloss:0.330456\n",
      "[140]\ttrain-mlogloss:0.27197\ttest-mlogloss:0.330482\n",
      "[141]\ttrain-mlogloss:0.271603\ttest-mlogloss:0.330529\n",
      "[142]\ttrain-mlogloss:0.271184\ttest-mlogloss:0.330509\n",
      "[143]\ttrain-mlogloss:0.270792\ttest-mlogloss:0.330523\n",
      "[144]\ttrain-mlogloss:0.270361\ttest-mlogloss:0.330293\n",
      "[145]\ttrain-mlogloss:0.269941\ttest-mlogloss:0.33011\n",
      "[146]\ttrain-mlogloss:0.269596\ttest-mlogloss:0.329972\n",
      "[147]\ttrain-mlogloss:0.269206\ttest-mlogloss:0.329806\n",
      "[148]\ttrain-mlogloss:0.268801\ttest-mlogloss:0.329703\n",
      "[149]\ttrain-mlogloss:0.268407\ttest-mlogloss:0.329689\n",
      "[150]\ttrain-mlogloss:0.268025\ttest-mlogloss:0.329696\n",
      "[151]\ttrain-mlogloss:0.267601\ttest-mlogloss:0.329559\n",
      "[152]\ttrain-mlogloss:0.267266\ttest-mlogloss:0.329505\n",
      "[153]\ttrain-mlogloss:0.266843\ttest-mlogloss:0.329441\n",
      "[154]\ttrain-mlogloss:0.266478\ttest-mlogloss:0.329432\n",
      "[155]\ttrain-mlogloss:0.266197\ttest-mlogloss:0.329371\n",
      "[156]\ttrain-mlogloss:0.265811\ttest-mlogloss:0.329354\n",
      "[157]\ttrain-mlogloss:0.265426\ttest-mlogloss:0.329231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158]\ttrain-mlogloss:0.265062\ttest-mlogloss:0.329184\n",
      "[159]\ttrain-mlogloss:0.264783\ttest-mlogloss:0.329138\n",
      "[160]\ttrain-mlogloss:0.264363\ttest-mlogloss:0.329166\n",
      "[161]\ttrain-mlogloss:0.264102\ttest-mlogloss:0.329087\n",
      "[162]\ttrain-mlogloss:0.263768\ttest-mlogloss:0.329008\n",
      "[163]\ttrain-mlogloss:0.263426\ttest-mlogloss:0.328956\n",
      "[164]\ttrain-mlogloss:0.262933\ttest-mlogloss:0.328963\n",
      "[165]\ttrain-mlogloss:0.262586\ttest-mlogloss:0.328765\n",
      "[166]\ttrain-mlogloss:0.262196\ttest-mlogloss:0.328762\n",
      "[167]\ttrain-mlogloss:0.261851\ttest-mlogloss:0.328684\n",
      "[168]\ttrain-mlogloss:0.261571\ttest-mlogloss:0.328725\n",
      "[169]\ttrain-mlogloss:0.26122\ttest-mlogloss:0.328756\n",
      "[170]\ttrain-mlogloss:0.260942\ttest-mlogloss:0.328775\n",
      "[171]\ttrain-mlogloss:0.260638\ttest-mlogloss:0.328852\n",
      "[172]\ttrain-mlogloss:0.260242\ttest-mlogloss:0.328796\n",
      "[173]\ttrain-mlogloss:0.259864\ttest-mlogloss:0.328758\n",
      "[174]\ttrain-mlogloss:0.25949\ttest-mlogloss:0.328719\n",
      "[175]\ttrain-mlogloss:0.259133\ttest-mlogloss:0.328689\n",
      "[176]\ttrain-mlogloss:0.258801\ttest-mlogloss:0.328684\n",
      "[177]\ttrain-mlogloss:0.258406\ttest-mlogloss:0.328656\n",
      "[178]\ttrain-mlogloss:0.258058\ttest-mlogloss:0.328553\n",
      "[179]\ttrain-mlogloss:0.257809\ttest-mlogloss:0.328537\n",
      "[180]\ttrain-mlogloss:0.257542\ttest-mlogloss:0.328509\n",
      "[181]\ttrain-mlogloss:0.257138\ttest-mlogloss:0.328314\n",
      "[182]\ttrain-mlogloss:0.256819\ttest-mlogloss:0.328378\n",
      "[183]\ttrain-mlogloss:0.256489\ttest-mlogloss:0.328309\n",
      "[184]\ttrain-mlogloss:0.256191\ttest-mlogloss:0.328303\n",
      "[185]\ttrain-mlogloss:0.255863\ttest-mlogloss:0.328286\n",
      "[186]\ttrain-mlogloss:0.255521\ttest-mlogloss:0.328164\n",
      "[187]\ttrain-mlogloss:0.255207\ttest-mlogloss:0.328122\n",
      "[188]\ttrain-mlogloss:0.254821\ttest-mlogloss:0.327938\n",
      "[189]\ttrain-mlogloss:0.254453\ttest-mlogloss:0.327873\n",
      "[190]\ttrain-mlogloss:0.254038\ttest-mlogloss:0.327738\n",
      "[191]\ttrain-mlogloss:0.253683\ttest-mlogloss:0.327678\n",
      "[192]\ttrain-mlogloss:0.253394\ttest-mlogloss:0.327645\n",
      "[193]\ttrain-mlogloss:0.25312\ttest-mlogloss:0.327614\n",
      "[194]\ttrain-mlogloss:0.252756\ttest-mlogloss:0.327683\n",
      "[195]\ttrain-mlogloss:0.252433\ttest-mlogloss:0.327634\n",
      "[196]\ttrain-mlogloss:0.252152\ttest-mlogloss:0.327689\n",
      "[197]\ttrain-mlogloss:0.251768\ttest-mlogloss:0.327594\n",
      "[198]\ttrain-mlogloss:0.251428\ttest-mlogloss:0.327529\n",
      "[199]\ttrain-mlogloss:0.251015\ttest-mlogloss:0.327386\n",
      "[200]\ttrain-mlogloss:0.250701\ttest-mlogloss:0.327445\n",
      "[201]\ttrain-mlogloss:0.25041\ttest-mlogloss:0.32743\n",
      "[202]\ttrain-mlogloss:0.250129\ttest-mlogloss:0.32728\n",
      "[203]\ttrain-mlogloss:0.249814\ttest-mlogloss:0.327287\n",
      "[204]\ttrain-mlogloss:0.249528\ttest-mlogloss:0.327227\n",
      "[205]\ttrain-mlogloss:0.249218\ttest-mlogloss:0.327203\n",
      "[206]\ttrain-mlogloss:0.248865\ttest-mlogloss:0.327242\n",
      "[207]\ttrain-mlogloss:0.24862\ttest-mlogloss:0.327242\n",
      "[208]\ttrain-mlogloss:0.248402\ttest-mlogloss:0.327198\n",
      "[209]\ttrain-mlogloss:0.248142\ttest-mlogloss:0.327112\n",
      "[210]\ttrain-mlogloss:0.247833\ttest-mlogloss:0.327158\n",
      "[211]\ttrain-mlogloss:0.247543\ttest-mlogloss:0.327034\n",
      "[212]\ttrain-mlogloss:0.247225\ttest-mlogloss:0.32705\n",
      "[213]\ttrain-mlogloss:0.246954\ttest-mlogloss:0.326999\n",
      "[214]\ttrain-mlogloss:0.246732\ttest-mlogloss:0.326953\n",
      "[215]\ttrain-mlogloss:0.246392\ttest-mlogloss:0.326941\n",
      "[216]\ttrain-mlogloss:0.246048\ttest-mlogloss:0.326884\n",
      "[217]\ttrain-mlogloss:0.245797\ttest-mlogloss:0.326849\n",
      "[218]\ttrain-mlogloss:0.24548\ttest-mlogloss:0.326684\n",
      "[219]\ttrain-mlogloss:0.245182\ttest-mlogloss:0.326643\n",
      "[220]\ttrain-mlogloss:0.244847\ttest-mlogloss:0.326581\n",
      "[221]\ttrain-mlogloss:0.244566\ttest-mlogloss:0.326536\n",
      "[222]\ttrain-mlogloss:0.244272\ttest-mlogloss:0.326433\n",
      "[223]\ttrain-mlogloss:0.243941\ttest-mlogloss:0.326509\n",
      "[224]\ttrain-mlogloss:0.243616\ttest-mlogloss:0.32657\n",
      "[225]\ttrain-mlogloss:0.243313\ttest-mlogloss:0.326535\n",
      "[226]\ttrain-mlogloss:0.243044\ttest-mlogloss:0.326591\n",
      "[227]\ttrain-mlogloss:0.242821\ttest-mlogloss:0.326696\n",
      "[228]\ttrain-mlogloss:0.242637\ttest-mlogloss:0.326652\n",
      "[229]\ttrain-mlogloss:0.24241\ttest-mlogloss:0.326645\n",
      "[230]\ttrain-mlogloss:0.242069\ttest-mlogloss:0.326719\n",
      "[231]\ttrain-mlogloss:0.241806\ttest-mlogloss:0.326576\n",
      "[232]\ttrain-mlogloss:0.241552\ttest-mlogloss:0.326578\n",
      "[233]\ttrain-mlogloss:0.241308\ttest-mlogloss:0.32661\n",
      "[234]\ttrain-mlogloss:0.241067\ttest-mlogloss:0.326631\n",
      "[235]\ttrain-mlogloss:0.2408\ttest-mlogloss:0.32665\n",
      "[236]\ttrain-mlogloss:0.240528\ttest-mlogloss:0.326686\n",
      "[237]\ttrain-mlogloss:0.24025\ttest-mlogloss:0.326748\n",
      "[238]\ttrain-mlogloss:0.239962\ttest-mlogloss:0.326707\n",
      "[239]\ttrain-mlogloss:0.239748\ttest-mlogloss:0.326819\n",
      "Test error using softprob = 0.12751106571331292\n"
     ]
    }
   ],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_test).reshape(y_test.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != y_test) / y_test.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.921818\ttest-mlogloss:0.921863\n",
      "[1]\ttrain-mlogloss:0.79919\ttest-mlogloss:0.799002\n",
      "[2]\ttrain-mlogloss:0.709528\ttest-mlogloss:0.70911\n",
      "[3]\ttrain-mlogloss:0.641847\ttest-mlogloss:0.641432\n",
      "[4]\ttrain-mlogloss:0.58975\ttest-mlogloss:0.589307\n",
      "[5]\ttrain-mlogloss:0.549532\ttest-mlogloss:0.548942\n",
      "[6]\ttrain-mlogloss:0.51771\ttest-mlogloss:0.516905\n",
      "[7]\ttrain-mlogloss:0.492063\ttest-mlogloss:0.491249\n",
      "[8]\ttrain-mlogloss:0.470918\ttest-mlogloss:0.470304\n",
      "[9]\ttrain-mlogloss:0.453839\ttest-mlogloss:0.453268\n",
      "[10]\ttrain-mlogloss:0.439633\ttest-mlogloss:0.439011\n",
      "[11]\ttrain-mlogloss:0.42799\ttest-mlogloss:0.427375\n",
      "[12]\ttrain-mlogloss:0.418099\ttest-mlogloss:0.41718\n",
      "[13]\ttrain-mlogloss:0.409744\ttest-mlogloss:0.408586\n",
      "[14]\ttrain-mlogloss:0.402601\ttest-mlogloss:0.401298\n",
      "[15]\ttrain-mlogloss:0.396384\ttest-mlogloss:0.394931\n",
      "[16]\ttrain-mlogloss:0.391226\ttest-mlogloss:0.389685\n",
      "[17]\ttrain-mlogloss:0.386594\ttest-mlogloss:0.384904\n",
      "[18]\ttrain-mlogloss:0.382438\ttest-mlogloss:0.380924\n",
      "[19]\ttrain-mlogloss:0.378784\ttest-mlogloss:0.377231\n",
      "[20]\ttrain-mlogloss:0.375561\ttest-mlogloss:0.374344\n",
      "[21]\ttrain-mlogloss:0.372713\ttest-mlogloss:0.371459\n",
      "[22]\ttrain-mlogloss:0.369994\ttest-mlogloss:0.368769\n",
      "[23]\ttrain-mlogloss:0.367495\ttest-mlogloss:0.366415\n",
      "[24]\ttrain-mlogloss:0.365086\ttest-mlogloss:0.364174\n",
      "[25]\ttrain-mlogloss:0.363041\ttest-mlogloss:0.362072\n",
      "[26]\ttrain-mlogloss:0.360894\ttest-mlogloss:0.360286\n",
      "[27]\ttrain-mlogloss:0.358427\ttest-mlogloss:0.357889\n",
      "[28]\ttrain-mlogloss:0.356686\ttest-mlogloss:0.356236\n",
      "[29]\ttrain-mlogloss:0.354958\ttest-mlogloss:0.354772\n",
      "[30]\ttrain-mlogloss:0.353042\ttest-mlogloss:0.3529\n",
      "[31]\ttrain-mlogloss:0.351534\ttest-mlogloss:0.351453\n",
      "[32]\ttrain-mlogloss:0.350061\ttest-mlogloss:0.349966\n",
      "[33]\ttrain-mlogloss:0.348746\ttest-mlogloss:0.348559\n",
      "[34]\ttrain-mlogloss:0.347125\ttest-mlogloss:0.347208\n",
      "[35]\ttrain-mlogloss:0.345824\ttest-mlogloss:0.346096\n",
      "[36]\ttrain-mlogloss:0.344629\ttest-mlogloss:0.344921\n",
      "[37]\ttrain-mlogloss:0.343449\ttest-mlogloss:0.3438\n",
      "[38]\ttrain-mlogloss:0.342153\ttest-mlogloss:0.342593\n",
      "[39]\ttrain-mlogloss:0.340765\ttest-mlogloss:0.341417\n",
      "[40]\ttrain-mlogloss:0.339714\ttest-mlogloss:0.340368\n",
      "[41]\ttrain-mlogloss:0.338548\ttest-mlogloss:0.339423\n",
      "[42]\ttrain-mlogloss:0.337546\ttest-mlogloss:0.338412\n",
      "[43]\ttrain-mlogloss:0.336309\ttest-mlogloss:0.337171\n",
      "[44]\ttrain-mlogloss:0.335438\ttest-mlogloss:0.33627\n",
      "[45]\ttrain-mlogloss:0.334524\ttest-mlogloss:0.335482\n",
      "[46]\ttrain-mlogloss:0.333505\ttest-mlogloss:0.334478\n",
      "[47]\ttrain-mlogloss:0.332605\ttest-mlogloss:0.333578\n",
      "[48]\ttrain-mlogloss:0.331549\ttest-mlogloss:0.332439\n",
      "[49]\ttrain-mlogloss:0.330689\ttest-mlogloss:0.331511\n",
      "[50]\ttrain-mlogloss:0.329738\ttest-mlogloss:0.330761\n",
      "[51]\ttrain-mlogloss:0.328878\ttest-mlogloss:0.329782\n",
      "[52]\ttrain-mlogloss:0.328087\ttest-mlogloss:0.329092\n",
      "[53]\ttrain-mlogloss:0.327228\ttest-mlogloss:0.32823\n",
      "[54]\ttrain-mlogloss:0.326369\ttest-mlogloss:0.327479\n",
      "[55]\ttrain-mlogloss:0.325674\ttest-mlogloss:0.326782\n",
      "[56]\ttrain-mlogloss:0.324773\ttest-mlogloss:0.325986\n",
      "[57]\ttrain-mlogloss:0.324038\ttest-mlogloss:0.325258\n",
      "[58]\ttrain-mlogloss:0.323447\ttest-mlogloss:0.324665\n",
      "[59]\ttrain-mlogloss:0.322686\ttest-mlogloss:0.323971\n",
      "[60]\ttrain-mlogloss:0.321956\ttest-mlogloss:0.323325\n",
      "[61]\ttrain-mlogloss:0.321187\ttest-mlogloss:0.322695\n",
      "[62]\ttrain-mlogloss:0.320563\ttest-mlogloss:0.32214\n",
      "[63]\ttrain-mlogloss:0.319943\ttest-mlogloss:0.321597\n",
      "[64]\ttrain-mlogloss:0.31947\ttest-mlogloss:0.321042\n",
      "[65]\ttrain-mlogloss:0.318853\ttest-mlogloss:0.320398\n",
      "[66]\ttrain-mlogloss:0.318079\ttest-mlogloss:0.319816\n",
      "[67]\ttrain-mlogloss:0.317387\ttest-mlogloss:0.31921\n",
      "[68]\ttrain-mlogloss:0.316848\ttest-mlogloss:0.318509\n",
      "[69]\ttrain-mlogloss:0.316166\ttest-mlogloss:0.317913\n",
      "[70]\ttrain-mlogloss:0.31537\ttest-mlogloss:0.317283\n",
      "[71]\ttrain-mlogloss:0.314847\ttest-mlogloss:0.316713\n",
      "[72]\ttrain-mlogloss:0.314291\ttest-mlogloss:0.316135\n",
      "[73]\ttrain-mlogloss:0.313745\ttest-mlogloss:0.315715\n",
      "[74]\ttrain-mlogloss:0.313133\ttest-mlogloss:0.315104\n",
      "[75]\ttrain-mlogloss:0.312497\ttest-mlogloss:0.314502\n",
      "[76]\ttrain-mlogloss:0.311727\ttest-mlogloss:0.313871\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-dc91d9f41da0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# do the same thing again, but output probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'objective'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'multi:softprob'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbstp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxg_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwatchlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Note: this convention has been changed since xgboost-unity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# get prediction, this is in 1D array, need reshape to (ndata, nclass)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_t, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_t).reshape(Y.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != Y) / Y.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_43</th>\n",
       "      <th>svd_word_44</th>\n",
       "      <th>svd_word_45</th>\n",
       "      <th>svd_word_46</th>\n",
       "      <th>svd_word_47</th>\n",
       "      <th>svd_word_48</th>\n",
       "      <th>svd_word_49</th>\n",
       "      <th>nb_cvec_eap</th>\n",
       "      <th>nb_cvec_hpl</th>\n",
       "      <th>nb_cvec_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006320</td>\n",
       "      <td>0.019999</td>\n",
       "      <td>-0.006687</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>-0.046846</td>\n",
       "      <td>-0.042702</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.978387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>-0.000610</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>-0.001725</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>-0.006745</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007646</td>\n",
       "      <td>-0.004253</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>-0.012464</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>0.217325</td>\n",
       "      <td>0.782527</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7     ...       svd_word_43  svd_word_44  svd_word_45  \\\n",
       "0    0.003240     ...         -0.006320     0.019999    -0.006687   \n",
       "1   -0.000215     ...          0.003998    -0.000610     0.001042   \n",
       "2   -0.004598     ...          0.007646    -0.004253     0.031135   \n",
       "\n",
       "   svd_word_46  svd_word_47  svd_word_48  svd_word_49  nb_cvec_eap  \\\n",
       "0     0.008188    -0.046846    -0.042702     0.000003     0.021018   \n",
       "1    -0.000802    -0.001725     0.004586    -0.006745     0.999985   \n",
       "2     0.000088    -0.001402    -0.012464     0.007291     0.217325   \n",
       "\n",
       "   nb_cvec_hpl  nb_cvec_mws  \n",
       "0     0.000595     0.978387  \n",
       "1     0.000009     0.000006  \n",
       "2     0.782527     0.000148  \n",
       "\n",
       "[3 rows x 55 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_45</th>\n",
       "      <th>svd_word_46</th>\n",
       "      <th>svd_word_47</th>\n",
       "      <th>svd_word_48</th>\n",
       "      <th>svd_word_49</th>\n",
       "      <th>nb_cvec_eap</th>\n",
       "      <th>nb_cvec_hpl</th>\n",
       "      <th>nb_cvec_mws</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006687</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>-0.046846</td>\n",
       "      <td>-0.042702</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.978387</td>\n",
       "      <td>[still, i, urg, leav, ireland, inquietud, impa...</td>\n",
       "      <td>still i urg leav ireland inquietud impati fath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>-0.001725</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>-0.006745</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>[if, fire, want, fan, could, readili, fan, new...</td>\n",
       "      <td>if fire want fan could readili fan newspap gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>-0.012464</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>0.217325</td>\n",
       "      <td>0.782527</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>[and, broken, frail, door, found, two, clean, ...</td>\n",
       "      <td>and broken frail door found two clean pick hum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7                        ...                          svd_word_45  \\\n",
       "0    0.003240                        ...                            -0.006687   \n",
       "1   -0.000215                        ...                             0.001042   \n",
       "2   -0.004598                        ...                             0.031135   \n",
       "\n",
       "   svd_word_46  svd_word_47  svd_word_48  svd_word_49  nb_cvec_eap  \\\n",
       "0     0.008188    -0.046846    -0.042702     0.000003     0.021018   \n",
       "1    -0.000802    -0.001725     0.004586    -0.006745     0.999985   \n",
       "2     0.000088    -0.001402    -0.012464     0.007291     0.217325   \n",
       "\n",
       "   nb_cvec_hpl  nb_cvec_mws  \\\n",
       "0     0.000595     0.978387   \n",
       "1     0.000009     0.000006   \n",
       "2     0.782527     0.000148   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [still, i, urg, leav, ireland, inquietud, impa...   \n",
       "1  [if, fire, want, fan, could, readili, fan, new...   \n",
       "2  [and, broken, frail, door, found, two, clean, ...   \n",
       "\n",
       "                                 cleaned_text_string  \n",
       "0  still i urg leav ireland inquietud impati fath...  \n",
       "1  if fire want fan could readili fan newspap gov...  \n",
       "2  and broken frail door found two clean pick hum...  \n",
       "\n",
       "[3 rows x 57 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['cleaned_text'] = df_test.text.apply(tokenize_stem)\n",
    "df_test['cleaned_text_string'] = df_test.cleaned_text.apply(' '.join)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>2.470797</td>\n",
       "      <td>0.148157</td>\n",
       "      <td>4.902270</td>\n",
       "      <td>0.550620</td>\n",
       "      <td>0.620488</td>\n",
       "      <td>0.293736</td>\n",
       "      <td>0.499431</td>\n",
       "      <td>-3.060433</td>\n",
       "      <td>-1.231265</td>\n",
       "      <td>1.104716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>6.992887</td>\n",
       "      <td>-0.361119</td>\n",
       "      <td>10.840488</td>\n",
       "      <td>0.832574</td>\n",
       "      <td>1.976278</td>\n",
       "      <td>1.081327</td>\n",
       "      <td>-0.232783</td>\n",
       "      <td>-7.698443</td>\n",
       "      <td>-2.857432</td>\n",
       "      <td>1.991308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>5.884607</td>\n",
       "      <td>-1.001613</td>\n",
       "      <td>6.743172</td>\n",
       "      <td>0.213241</td>\n",
       "      <td>1.738426</td>\n",
       "      <td>0.930032</td>\n",
       "      <td>-1.557037</td>\n",
       "      <td>-5.864972</td>\n",
       "      <td>-1.940434</td>\n",
       "      <td>0.822325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7       ...        w2v_feature_90  w2v_feature_91  w2v_feature_92  \\\n",
       "0    0.003240       ...              2.470797        0.148157        4.902270   \n",
       "1   -0.000215       ...              6.992887       -0.361119       10.840488   \n",
       "2   -0.004598       ...              5.884607       -1.001613        6.743172   \n",
       "\n",
       "   w2v_feature_93  w2v_feature_94  w2v_feature_95  w2v_feature_96  \\\n",
       "0        0.550620        0.620488        0.293736        0.499431   \n",
       "1        0.832574        1.976278        1.081327       -0.232783   \n",
       "2        0.213241        1.738426        0.930032       -1.557037   \n",
       "\n",
       "   w2v_feature_97  w2v_feature_98  w2v_feature_99  \n",
       "0       -3.060433       -1.231265        1.104716  \n",
       "1       -7.698443       -2.857432        1.991308  \n",
       "2       -5.864972       -1.940434        0.822325  \n",
       "\n",
       "[3 rows x 247 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['length']=df_test['cleaned_text_string'].apply(len)\n",
    "df_test[\"num_words\"] = df_test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_test[\"num_unique_words\"] = df_test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_test[\"num_punctuations\"] =df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test[\"num_words_upper\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_test[\"num_words_title\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_test[\"mean_word_len\"] = df_test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test[\"num_stopwords\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_test['lexical_diversity'] = df_test.text.apply(lexical_diversity)\n",
    "df_test['w2v_array'] = df_test.cleaned_text.apply(sum_up_word2vec_array)\n",
    "count_topwords(df_test)\n",
    "create_w2v_columns(df_test) \n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550620</td>\n",
       "      <td>0.620488</td>\n",
       "      <td>0.293736</td>\n",
       "      <td>0.499431</td>\n",
       "      <td>-3.060433</td>\n",
       "      <td>-1.231265</td>\n",
       "      <td>1.104716</td>\n",
       "      <td>0.071473</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>0.026541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832574</td>\n",
       "      <td>1.976278</td>\n",
       "      <td>1.081327</td>\n",
       "      <td>-0.232783</td>\n",
       "      <td>-7.698443</td>\n",
       "      <td>-2.857432</td>\n",
       "      <td>1.991308</td>\n",
       "      <td>0.035932</td>\n",
       "      <td>0.062884</td>\n",
       "      <td>0.039306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213241</td>\n",
       "      <td>1.738426</td>\n",
       "      <td>0.930032</td>\n",
       "      <td>-1.557037</td>\n",
       "      <td>-5.864972</td>\n",
       "      <td>-1.940434</td>\n",
       "      <td>0.822325</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.057723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7    ...      w2v_feature_93  w2v_feature_94  w2v_feature_95  \\\n",
       "0    0.003240    ...            0.550620        0.620488        0.293736   \n",
       "1   -0.000215    ...            0.832574        1.976278        1.081327   \n",
       "2   -0.004598    ...            0.213241        1.738426        0.930032   \n",
       "\n",
       "   w2v_feature_96  w2v_feature_97  w2v_feature_98  w2v_feature_99  mws_index  \\\n",
       "0        0.499431       -3.060433       -1.231265        1.104716   0.071473   \n",
       "1       -0.232783       -7.698443       -2.857432        1.991308   0.035932   \n",
       "2       -1.557037       -5.864972       -1.940434        0.822325   0.027486   \n",
       "\n",
       "   eap_index  hpl_index  \n",
       "0   0.036315   0.026541  \n",
       "1   0.062884   0.039306  \n",
       "2   0.055142   0.057723  \n",
       "\n",
       "[3 rows x 250 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['mws_index']=df_test['cleaned_text'].apply(ind_val_mws)/df_test['length']\n",
    "df_test['eap_index']=df_test['cleaned_text'].apply(ind_val_eap)/df_test['length']\n",
    "df_test['hpl_index']=df_test['cleaned_text'].apply(ind_val_hpl)/df_test['length']\n",
    "df_test.head(n=3)\n",
    "# df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_test['cleaned_text']\n",
    "del df_test['cleaned_text_string']\n",
    "del df_test['w2v_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author2', 'author', 'cleaned_text', 'cleaned_text_string']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=(df_train.columns.tolist())[6:]\n",
    "[item for item in df_train.columns.tolist() if item not in df_test.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550620</td>\n",
       "      <td>0.620488</td>\n",
       "      <td>0.293736</td>\n",
       "      <td>0.499431</td>\n",
       "      <td>-3.060433</td>\n",
       "      <td>-1.231265</td>\n",
       "      <td>1.104716</td>\n",
       "      <td>0.071473</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>0.026541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832574</td>\n",
       "      <td>1.976278</td>\n",
       "      <td>1.081327</td>\n",
       "      <td>-0.232783</td>\n",
       "      <td>-7.698443</td>\n",
       "      <td>-2.857432</td>\n",
       "      <td>1.991308</td>\n",
       "      <td>0.035932</td>\n",
       "      <td>0.062884</td>\n",
       "      <td>0.039306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213241</td>\n",
       "      <td>1.738426</td>\n",
       "      <td>0.930032</td>\n",
       "      <td>-1.557037</td>\n",
       "      <td>-5.864972</td>\n",
       "      <td>-1.940434</td>\n",
       "      <td>0.822325</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.057723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>-0.007515</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.004020</td>\n",
       "      <td>-0.004521</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>-0.002882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739516</td>\n",
       "      <td>1.780698</td>\n",
       "      <td>1.040653</td>\n",
       "      <td>-0.705218</td>\n",
       "      <td>-7.231385</td>\n",
       "      <td>-2.711493</td>\n",
       "      <td>1.857348</td>\n",
       "      <td>0.024369</td>\n",
       "      <td>0.065057</td>\n",
       "      <td>0.055736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>-0.003185</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.001152</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>-0.006110</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>-0.002149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410158</td>\n",
       "      <td>0.281921</td>\n",
       "      <td>0.229973</td>\n",
       "      <td>0.561596</td>\n",
       "      <td>-1.859675</td>\n",
       "      <td>-0.814361</td>\n",
       "      <td>0.731755</td>\n",
       "      <td>0.025615</td>\n",
       "      <td>0.070948</td>\n",
       "      <td>0.028437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "3  id27757  While I was thinking how I should possibly man...    0.013408   \n",
       "4  id04081  I am not sure to what limit his knowledge may ...    0.012565   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "3   -0.007515   -0.000154   -0.004020   -0.004521    0.002712   -0.004220   \n",
       "4   -0.003185   -0.000719   -0.001152    0.000930   -0.006110    0.001690   \n",
       "\n",
       "   svd_word_7    ...      w2v_feature_93  w2v_feature_94  w2v_feature_95  \\\n",
       "0    0.003240    ...            0.550620        0.620488        0.293736   \n",
       "1   -0.000215    ...            0.832574        1.976278        1.081327   \n",
       "2   -0.004598    ...            0.213241        1.738426        0.930032   \n",
       "3   -0.002882    ...            0.739516        1.780698        1.040653   \n",
       "4   -0.002149    ...            0.410158        0.281921        0.229973   \n",
       "\n",
       "   w2v_feature_96  w2v_feature_97  w2v_feature_98  w2v_feature_99  mws_index  \\\n",
       "0        0.499431       -3.060433       -1.231265        1.104716   0.071473   \n",
       "1       -0.232783       -7.698443       -2.857432        1.991308   0.035932   \n",
       "2       -1.557037       -5.864972       -1.940434        0.822325   0.027486   \n",
       "3       -0.705218       -7.231385       -2.711493        1.857348   0.024369   \n",
       "4        0.561596       -1.859675       -0.814361        0.731755   0.025615   \n",
       "\n",
       "   eap_index  hpl_index  \n",
       "0   0.036315   0.026541  \n",
       "1   0.062884   0.039306  \n",
       "2   0.055142   0.057723  \n",
       "3   0.065057   0.055736  \n",
       "4   0.070948   0.028437  \n",
       "\n",
       "[5 rows x 247 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'text', 'length', 'num_words', 'num_unique_words',\n",
       "       'num_punctuations', 'num_words_upper', 'num_words_title',\n",
       "       'mean_word_len', 'num_stopwords', 'lexical_diversity', 'long',\n",
       "       'see', 'year', 'happi', 'word', 'feel', 'death', 'fear', 'old',\n",
       "       'face', 'even', 'father', 'look', 'chang', 'came', 'yet', 'ever',\n",
       "       'door', 'first', 'made', 'place', 'raymond', 'heard', 'hope',\n",
       "       'time', 'littl', 'say', 'heart', 'thus', 'life', 'make', 'whose',\n",
       "       'pass', 'thought', 'never', 'room', 'men', 'great', 'still', 'come',\n",
       "       'dream', 'upon', 'found', 'love', 'well', 'mani', 'know', 'point',\n",
       "       'one', 'mind', 'howev', 'everi', 'two', 'seem', 'saw', 'appear',\n",
       "       'die', 'certain', 'light', 'man', 'like', 'shall', 'eye', 'thing',\n",
       "       'might', 'return', 'said', 'hous', 'friend', 'hand', 'street',\n",
       "       'night', 'may', 'much', 'strang', 'day', 'natur', 'must', 'though',\n",
       "       'us', 'w2v_feature_0', 'w2v_feature_1', 'w2v_feature_2',\n",
       "       'w2v_feature_3', 'w2v_feature_4', 'w2v_feature_5', 'w2v_feature_6',\n",
       "       'w2v_feature_7', 'w2v_feature_8', 'w2v_feature_9', 'w2v_feature_10',\n",
       "       'w2v_feature_11', 'w2v_feature_12', 'w2v_feature_13',\n",
       "       'w2v_feature_14', 'w2v_feature_15', 'w2v_feature_16',\n",
       "       'w2v_feature_17', 'w2v_feature_18', 'w2v_feature_19',\n",
       "       'w2v_feature_20', 'w2v_feature_21', 'w2v_feature_22',\n",
       "       'w2v_feature_23', 'w2v_feature_24', 'w2v_feature_25',\n",
       "       'w2v_feature_26', 'w2v_feature_27', 'w2v_feature_28',\n",
       "       'w2v_feature_29', 'w2v_feature_30', 'w2v_feature_31',\n",
       "       'w2v_feature_32', 'w2v_feature_33', 'w2v_feature_34',\n",
       "       'w2v_feature_35', 'w2v_feature_36', 'w2v_feature_37',\n",
       "       'w2v_feature_38', 'w2v_feature_39', 'w2v_feature_40',\n",
       "       'w2v_feature_41', 'w2v_feature_42', 'w2v_feature_43',\n",
       "       'w2v_feature_44', 'w2v_feature_45', 'w2v_feature_46',\n",
       "       'w2v_feature_47', 'w2v_feature_48', 'w2v_feature_49',\n",
       "       'w2v_feature_50', 'w2v_feature_51', 'w2v_feature_52',\n",
       "       'w2v_feature_53', 'w2v_feature_54', 'w2v_feature_55',\n",
       "       'w2v_feature_56', 'w2v_feature_57', 'w2v_feature_58',\n",
       "       'w2v_feature_59', 'w2v_feature_60', 'w2v_feature_61',\n",
       "       'w2v_feature_62', 'w2v_feature_63', 'w2v_feature_64',\n",
       "       'w2v_feature_65', 'w2v_feature_66', 'w2v_feature_67',\n",
       "       'w2v_feature_68', 'w2v_feature_69', 'w2v_feature_70',\n",
       "       'w2v_feature_71', 'w2v_feature_72', 'w2v_feature_73',\n",
       "       'w2v_feature_74', 'w2v_feature_75', 'w2v_feature_76',\n",
       "       'w2v_feature_77', 'w2v_feature_78', 'w2v_feature_79',\n",
       "       'w2v_feature_80', 'w2v_feature_81', 'w2v_feature_82',\n",
       "       'w2v_feature_83', 'w2v_feature_84', 'w2v_feature_85',\n",
       "       'w2v_feature_86', 'w2v_feature_87', 'w2v_feature_88',\n",
       "       'w2v_feature_89', 'w2v_feature_90', 'w2v_feature_91',\n",
       "       'w2v_feature_92', 'w2v_feature_93', 'w2v_feature_94',\n",
       "       'w2v_feature_95', 'w2v_feature_96', 'w2v_feature_97',\n",
       "       'w2v_feature_98', 'w2v_feature_99', 'mws_index', 'eap_index',\n",
       "       'hpl_index', 'svd_word_0', 'svd_word_1', 'svd_word_2', 'svd_word_3',\n",
       "       'svd_word_4', 'svd_word_5', 'svd_word_6', 'svd_word_7',\n",
       "       'svd_word_8', 'svd_word_9', 'svd_word_10', 'svd_word_11',\n",
       "       'svd_word_12', 'svd_word_13', 'svd_word_14', 'svd_word_15',\n",
       "       'svd_word_16', 'svd_word_17', 'svd_word_18', 'svd_word_19',\n",
       "       'svd_word_20', 'svd_word_21', 'svd_word_22', 'svd_word_23',\n",
       "       'svd_word_24', 'svd_word_25', 'svd_word_26', 'svd_word_27',\n",
       "       'svd_word_28', 'svd_word_29', 'svd_word_30', 'svd_word_31',\n",
       "       'svd_word_32', 'svd_word_33', 'svd_word_34', 'svd_word_35',\n",
       "       'svd_word_36', 'svd_word_37', 'svd_word_38', 'svd_word_39',\n",
       "       'svd_word_40', 'svd_word_41', 'svd_word_42', 'svd_word_43',\n",
       "       'svd_word_44', 'svd_word_45', 'svd_word_46', 'svd_word_47',\n",
       "       'svd_word_48', 'svd_word_49', 'nb_cvec_eap', 'nb_cvec_hpl',\n",
       "       'nb_cvec_mws'], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['author2', 'id', 'text', 'author', 'cleaned_text',\n",
       "       'cleaned_text_string', 'length', 'num_words', 'num_unique_words',\n",
       "       'num_punctuations', 'num_words_upper', 'num_words_title',\n",
       "       'mean_word_len', 'num_stopwords', 'lexical_diversity', 'long',\n",
       "       'see', 'year', 'happi', 'word', 'feel', 'death', 'fear', 'old',\n",
       "       'face', 'even', 'father', 'look', 'chang', 'came', 'yet', 'ever',\n",
       "       'door', 'first', 'made', 'place', 'raymond', 'heard', 'hope',\n",
       "       'time', 'littl', 'say', 'heart', 'thus', 'life', 'make', 'whose',\n",
       "       'pass', 'thought', 'never', 'room', 'men', 'great', 'still', 'come',\n",
       "       'dream', 'upon', 'found', 'love', 'well', 'mani', 'know', 'point',\n",
       "       'one', 'mind', 'howev', 'everi', 'two', 'seem', 'saw', 'appear',\n",
       "       'die', 'certain', 'light', 'man', 'like', 'shall', 'eye', 'thing',\n",
       "       'might', 'return', 'said', 'hous', 'friend', 'hand', 'street',\n",
       "       'night', 'may', 'much', 'strang', 'day', 'natur', 'must', 'though',\n",
       "       'us', 'w2v_feature_0', 'w2v_feature_1', 'w2v_feature_2',\n",
       "       'w2v_feature_3', 'w2v_feature_4', 'w2v_feature_5', 'w2v_feature_6',\n",
       "       'w2v_feature_7', 'w2v_feature_8', 'w2v_feature_9', 'w2v_feature_10',\n",
       "       'w2v_feature_11', 'w2v_feature_12', 'w2v_feature_13',\n",
       "       'w2v_feature_14', 'w2v_feature_15', 'w2v_feature_16',\n",
       "       'w2v_feature_17', 'w2v_feature_18', 'w2v_feature_19',\n",
       "       'w2v_feature_20', 'w2v_feature_21', 'w2v_feature_22',\n",
       "       'w2v_feature_23', 'w2v_feature_24', 'w2v_feature_25',\n",
       "       'w2v_feature_26', 'w2v_feature_27', 'w2v_feature_28',\n",
       "       'w2v_feature_29', 'w2v_feature_30', 'w2v_feature_31',\n",
       "       'w2v_feature_32', 'w2v_feature_33', 'w2v_feature_34',\n",
       "       'w2v_feature_35', 'w2v_feature_36', 'w2v_feature_37',\n",
       "       'w2v_feature_38', 'w2v_feature_39', 'w2v_feature_40',\n",
       "       'w2v_feature_41', 'w2v_feature_42', 'w2v_feature_43',\n",
       "       'w2v_feature_44', 'w2v_feature_45', 'w2v_feature_46',\n",
       "       'w2v_feature_47', 'w2v_feature_48', 'w2v_feature_49',\n",
       "       'w2v_feature_50', 'w2v_feature_51', 'w2v_feature_52',\n",
       "       'w2v_feature_53', 'w2v_feature_54', 'w2v_feature_55',\n",
       "       'w2v_feature_56', 'w2v_feature_57', 'w2v_feature_58',\n",
       "       'w2v_feature_59', 'w2v_feature_60', 'w2v_feature_61',\n",
       "       'w2v_feature_62', 'w2v_feature_63', 'w2v_feature_64',\n",
       "       'w2v_feature_65', 'w2v_feature_66', 'w2v_feature_67',\n",
       "       'w2v_feature_68', 'w2v_feature_69', 'w2v_feature_70',\n",
       "       'w2v_feature_71', 'w2v_feature_72', 'w2v_feature_73',\n",
       "       'w2v_feature_74', 'w2v_feature_75', 'w2v_feature_76',\n",
       "       'w2v_feature_77', 'w2v_feature_78', 'w2v_feature_79',\n",
       "       'w2v_feature_80', 'w2v_feature_81', 'w2v_feature_82',\n",
       "       'w2v_feature_83', 'w2v_feature_84', 'w2v_feature_85',\n",
       "       'w2v_feature_86', 'w2v_feature_87', 'w2v_feature_88',\n",
       "       'w2v_feature_89', 'w2v_feature_90', 'w2v_feature_91',\n",
       "       'w2v_feature_92', 'w2v_feature_93', 'w2v_feature_94',\n",
       "       'w2v_feature_95', 'w2v_feature_96', 'w2v_feature_97',\n",
       "       'w2v_feature_98', 'w2v_feature_99', 'mws_index', 'eap_index',\n",
       "       'hpl_index', 'svd_word_0', 'svd_word_1', 'svd_word_2', 'svd_word_3',\n",
       "       'svd_word_4', 'svd_word_5', 'svd_word_6', 'svd_word_7',\n",
       "       'svd_word_8', 'svd_word_9', 'svd_word_10', 'svd_word_11',\n",
       "       'svd_word_12', 'svd_word_13', 'svd_word_14', 'svd_word_15',\n",
       "       'svd_word_16', 'svd_word_17', 'svd_word_18', 'svd_word_19',\n",
       "       'svd_word_20', 'svd_word_21', 'svd_word_22', 'svd_word_23',\n",
       "       'svd_word_24', 'svd_word_25', 'svd_word_26', 'svd_word_27',\n",
       "       'svd_word_28', 'svd_word_29', 'svd_word_30', 'svd_word_31',\n",
       "       'svd_word_32', 'svd_word_33', 'svd_word_34', 'svd_word_35',\n",
       "       'svd_word_36', 'svd_word_37', 'svd_word_38', 'svd_word_39',\n",
       "       'svd_word_40', 'svd_word_41', 'svd_word_42', 'svd_word_43',\n",
       "       'svd_word_44', 'svd_word_45', 'svd_word_46', 'svd_word_47',\n",
       "       'svd_word_48', 'svd_word_49', 'nb_cvec_eap', 'nb_cvec_hpl',\n",
       "       'nb_cvec_mws', 'lda_topic_0', 'lda_topic_1', 'lda_topic_2',\n",
       "       'lda_topic_3', 'lda_topic_4', 'lda_topic_5', 'lda_topic_6',\n",
       "       'lda_topic_7', 'lda_topic_8', 'lda_topic_9', 'lda_topic_10',\n",
       "       'lda_topic_11', 'lda_topic_12', 'lda_topic_13', 'lda_topic_14',\n",
       "       'lda_topic_15', 'lda_topic_16', 'lda_topic_17', 'lda_topic_18',\n",
       "       'lda_topic_19', 'lda_topic_20', 'lda_topic_21', 'lda_topic_22',\n",
       "       'lda_topic_23', 'lda_topic_24', 'svd_word_0', 'svd_word_1',\n",
       "       'svd_word_2', 'svd_word_3', 'svd_word_4', 'svd_word_5',\n",
       "       'svd_word_6', 'svd_word_7', 'svd_word_8', 'svd_word_9',\n",
       "       'svd_word_10', 'svd_word_11', 'svd_word_12', 'svd_word_13',\n",
       "       'svd_word_14', 'svd_word_15', 'svd_word_16', 'svd_word_17',\n",
       "       'svd_word_18', 'svd_word_19', 'svd_word_20', 'svd_word_21',\n",
       "       'svd_word_22', 'svd_word_23', 'svd_word_24', 'svd_word_25',\n",
       "       'svd_word_26', 'svd_word_27', 'svd_word_28', 'svd_word_29',\n",
       "       'svd_word_30', 'svd_word_31', 'svd_word_32', 'svd_word_33',\n",
       "       'svd_word_34', 'svd_word_35', 'svd_word_36', 'svd_word_37',\n",
       "       'svd_word_38', 'svd_word_39', 'svd_word_40', 'svd_word_41',\n",
       "       'svd_word_42', 'svd_word_43', 'svd_word_44', 'svd_word_45',\n",
       "       'svd_word_46', 'svd_word_47', 'svd_word_48', 'svd_word_49'], dtype=object)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_test=df_test[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.70000000e+01,   1.90000000e+01,   1.90000000e+01, ...,\n",
       "          2.10181734e-02,   5.94701527e-04,   9.78387125e-01],\n",
       "       [  1.81000000e+02,   6.20000000e+01,   4.90000000e+01, ...,\n",
       "          9.99985252e-01,   8.92562104e-06,   5.82205815e-06],\n",
       "       [  1.14000000e+02,   3.30000000e+01,   3.00000000e+01, ...,\n",
       "          2.17325173e-01,   7.82527035e-01,   1.47792342e-04],\n",
       "       ..., \n",
       "       [  9.90000000e+01,   2.50000000e+01,   2.40000000e+01, ...,\n",
       "          9.99708837e-01,   1.20498159e-04,   1.70664359e-04],\n",
       "       [  1.15000000e+02,   3.80000000e+01,   3.40000000e+01, ...,\n",
       "          6.88692927e-04,   4.59749594e-06,   9.99306710e-01],\n",
       "       [  1.34000000e+02,   3.80000000e+01,   3.30000000e+01, ...,\n",
       "          2.47053560e-02,   9.75286711e-01,   7.93279044e-06]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f311', 'f312', 'f313', 'f314', 'f315', 'f316', 'f317', 'f318', 'f319'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294']\nexpected f295, f296, f302, f314, f297, f312, f313, f305, f307, f319, f311, f309, f301, f304, f300, f318, f308, f306, f298, f315, f299, f317, f310, f303, f316 in input data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-37848931b58e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxg_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbstp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf)\u001b[0m\n\u001b[1;32m    937\u001b[0m             \u001b[0moption_mask\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0;36m0x02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0;32m-> 1179\u001b[0;31m                                             data.feature_names))\n\u001b[0m\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294', 'f295', 'f296', 'f297', 'f298', 'f299', 'f300', 'f301', 'f302', 'f303', 'f304', 'f305', 'f306', 'f307', 'f308', 'f309', 'f310', 'f311', 'f312', 'f313', 'f314', 'f315', 'f316', 'f317', 'f318', 'f319'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194', 'f195', 'f196', 'f197', 'f198', 'f199', 'f200', 'f201', 'f202', 'f203', 'f204', 'f205', 'f206', 'f207', 'f208', 'f209', 'f210', 'f211', 'f212', 'f213', 'f214', 'f215', 'f216', 'f217', 'f218', 'f219', 'f220', 'f221', 'f222', 'f223', 'f224', 'f225', 'f226', 'f227', 'f228', 'f229', 'f230', 'f231', 'f232', 'f233', 'f234', 'f235', 'f236', 'f237', 'f238', 'f239', 'f240', 'f241', 'f242', 'f243', 'f244', 'f245', 'f246', 'f247', 'f248', 'f249', 'f250', 'f251', 'f252', 'f253', 'f254', 'f255', 'f256', 'f257', 'f258', 'f259', 'f260', 'f261', 'f262', 'f263', 'f264', 'f265', 'f266', 'f267', 'f268', 'f269', 'f270', 'f271', 'f272', 'f273', 'f274', 'f275', 'f276', 'f277', 'f278', 'f279', 'f280', 'f281', 'f282', 'f283', 'f284', 'f285', 'f286', 'f287', 'f288', 'f289', 'f290', 'f291', 'f292', 'f293', 'f294']\nexpected f295, f296, f302, f314, f297, f312, f313, f305, f307, f319, f311, f309, f301, f304, f300, f318, f308, f306, f298, f315, f299, f317, f310, f303, f316 in input data"
     ]
    }
   ],
   "source": [
    "x_t=ds_test[:, :]\n",
    "y_t=df_test['id'].values\n",
    "xg_t=xgb.DMatrix(x_t)\n",
    "pred_prob = bstp.predict(xg_t).reshape(y_t.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.44851837e-02   5.50276274e-03   9.60012019e-01]\n",
      " [  9.96701777e-01   2.07544537e-03   1.22278044e-03]\n",
      " [  6.28928840e-02   9.35769796e-01   1.33732741e-03]\n",
      " ..., \n",
      " [  9.06760871e-01   5.87458573e-02   3.44932452e-02]\n",
      " [  2.11261455e-02   1.50194473e-03   9.77371871e-01]\n",
      " [  1.41863478e-02   9.85448301e-01   3.65293323e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.034485</td>\n",
       "      <td>0.005503</td>\n",
       "      <td>0.960012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.996702</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.001223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.062893</td>\n",
       "      <td>0.935770</td>\n",
       "      <td>0.001337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.809090</td>\n",
       "      <td>0.188739</td>\n",
       "      <td>0.002171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.978529</td>\n",
       "      <td>0.016969</td>\n",
       "      <td>0.004502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.034485  0.005503  0.960012\n",
       "1  id24541  0.996702  0.002075  0.001223\n",
       "2  id00134  0.062893  0.935770  0.001337\n",
       "3  id27757  0.809090  0.188739  0.002171\n",
       "4  id04081  0.978529  0.016969  0.004502"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export=pd.DataFrame(pred_prob)\n",
    "export.insert(loc=0, column='id', value=y_t)\n",
    "export.columns=['id','EAP', 'HPL', 'MWS']\n",
    "export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8392"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>id23301</td>\n",
       "      <td>0.337495</td>\n",
       "      <td>0.166845</td>\n",
       "      <td>0.49566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       EAP       HPL      MWS\n",
       "6106  id23301  0.337495  0.166845  0.49566"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export[export['id']=='id23301']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export.to_csv(path_or_buf=\"../data/export.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# попробуем теперь генсимовский лда\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set(stopwords.words('english')).union(set(get_stop_words('english')))\n",
    "\n",
    "\n",
    "words_exclude = set(eng_stopwords).union(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем сделать корпус как список от всех строк\n",
    "\n",
    "train_corpse = df_train['cleaned_text_string'].tolist()\n",
    "\n",
    "texts = [[word for word in document.lower().split() if word not in words_exclude] for document in train_corpse]\n",
    "dicccs = Dictionary(texts)\n",
    "\n",
    "\n",
    "corpus = [dicccs.doc2bow(text) for text in texts]\n",
    "\n",
    "# print(corpse[0:5])\n",
    "\n",
    "\n",
    "lda_model = LdaModel(corpus, id2word=dicccs, num_topics=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6,\n",
       "  '0.016*\"feel\" + 0.015*\"heart\" + 0.015*\"look\" + 0.014*\"love\" + 0.012*\"hope\" + 0.012*\"passion\" + 0.010*\"eye\" + 0.010*\"yet\" + 0.009*\"seem\" + 0.009*\"evil\"'),\n",
       " (15,\n",
       "  '0.141*\"``\" + 0.122*\"\\'\\'\" + 0.024*\"said\" + 0.013*\"say\" + 0.011*\"shall\" + 0.011*\"\\'s\" + 0.010*\"repli\" + 0.007*\"n\\'t\" + 0.006*\"sure\" + 0.006*\"held\"'),\n",
       " (9,\n",
       "  '0.019*\"\\'s\" + 0.011*\"one\" + 0.009*\"larg\" + 0.009*\"hand\" + 0.009*\"content\" + 0.008*\"success\" + 0.008*\"man\" + 0.008*\"piec\" + 0.008*\"seem\" + 0.007*\"fulli\"'),\n",
       " (3,\n",
       "  '0.017*\"one\" + 0.016*\"citi\" + 0.014*\"sea\" + 0.013*\"lay\" + 0.011*\"door\" + 0.011*\"saw\" + 0.011*\"enter\" + 0.010*\"anoth\" + 0.010*\"moon\" + 0.010*\"white\"'),\n",
       " (16,\n",
       "  '0.013*\"sleep\" + 0.009*\"length\" + 0.009*\"castl\" + 0.009*\"angl\" + 0.009*\"life\" + 0.009*\"must\" + 0.009*\"night\" + 0.008*\"sourc\" + 0.008*\"struck\" + 0.008*\"forget\"'),\n",
       " (14,\n",
       "  '0.018*\"object\" + 0.017*\"thus\" + 0.010*\"satisfi\" + 0.010*\"companion\" + 0.009*\"minut\" + 0.008*\"letter\" + 0.008*\"pass\" + 0.008*\"thou\" + 0.008*\"convey\" + 0.008*\"dwell\"'),\n",
       " (17,\n",
       "  '0.018*\"interest\" + 0.015*\"\\'s\" + 0.015*\"plan\" + 0.013*\"small\" + 0.011*\"alway\" + 0.010*\"unusu\" + 0.009*\"human\" + 0.009*\"woman\" + 0.008*\"uncl\" + 0.008*\"saw\"'),\n",
       " (13,\n",
       "  '0.013*\"year\" + 0.011*\"hundr\" + 0.010*\"even\" + 0.009*\"enemi\" + 0.009*\"tomb\" + 0.007*\"three\" + 0.007*\"agit\" + 0.007*\"detail\" + 0.007*\"hors\" + 0.007*\"two\"'),\n",
       " (10,\n",
       "  '0.016*\"never\" + 0.013*\"read\" + 0.012*\"like\" + 0.012*\"leav\" + 0.012*\"ye\" + 0.012*\"n\\'t\" + 0.011*\"love\" + 0.010*\"yet\" + 0.009*\"god\" + 0.009*\"eye\"'),\n",
       " (1,\n",
       "  '0.011*\"might\" + 0.009*\"term\" + 0.009*\"employ\" + 0.009*\"ask\" + 0.008*\"becam\" + 0.008*\"went\" + 0.008*\"haunt\" + 0.008*\"togeth\" + 0.008*\"night\" + 0.008*\"thing\"'),\n",
       " (11,\n",
       "  '0.061*\"``\" + 0.044*\"\\'\\'\" + 0.028*\"said\" + 0.013*\"miseri\" + 0.012*\"dear\" + 0.010*\"father\" + 0.010*\"know\" + 0.008*\"heart\" + 0.008*\"love\" + 0.008*\"matter\"'),\n",
       " (2,\n",
       "  '0.023*\"upon\" + 0.012*\"time\" + 0.011*\"open\" + 0.011*\"door\" + 0.009*\"rush\" + 0.009*\"main\" + 0.009*\"room\" + 0.009*\"street\" + 0.009*\"occas\" + 0.008*\"window\"'),\n",
       " (7,\n",
       "  '0.012*\"upon\" + 0.012*\"voic\" + 0.011*\"one\" + 0.009*\"box\" + 0.008*\"hand\" + 0.008*\"seem\" + 0.007*\"hous\" + 0.007*\"ice\" + 0.007*\"everi\" + 0.007*\"start\"'),\n",
       " (21,\n",
       "  '0.011*\"move\" + 0.009*\"seen\" + 0.008*\"one\" + 0.008*\"answer\" + 0.007*\"last\" + 0.007*\"hung\" + 0.007*\"black\" + 0.007*\"horribl\" + 0.007*\"like\" + 0.007*\"time\"'),\n",
       " (22,\n",
       "  '0.019*\"come\" + 0.015*\"hill\" + 0.011*\"old\" + 0.011*\"heard\" + 0.011*\"one\" + 0.009*\"fit\" + 0.009*\"tale\" + 0.009*\"whether\" + 0.009*\"peopl\" + 0.008*\"left\"')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['process', 'howev', 'afford', 'mean', 'ascertain', 'dimens', 'dungeon', 'might', 'make', 'circuit', 'return', 'point', 'whenc', 'set', 'without', 'awar', 'fact', 'perfect', 'uniform', 'seem', 'wall']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.081501738548664951),\n",
       " (18, 0.39479289459575573),\n",
       " (19, 0.050642737227831379),\n",
       " (20, 0.37754174325765977),\n",
       " (24, 0.059157250001896047)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = dicccs.doc2bow(texts[0])\n",
    "\n",
    "lda_model.get_document_topics(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.60075588554034731),\n",
       " (5, 0.12909575248303104),\n",
       " (6, 0.086607033567281319),\n",
       " (7, 0.062167704750211973),\n",
       " (9, 0.052128071578546056),\n",
       " (23, 0.036202073815929887)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = dicccs.doc2bow(texts[15])\n",
    "\n",
    "lda_model.get_document_topics(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# поехали. времени мало\n",
    "\n",
    "def topicify_cleane_text_list(cl_tex_lst):\n",
    "    \n",
    "    cl_lst = [i for i in cl_tex_lst if i not in words_exclude]\n",
    "    \n",
    "    doc = dicccs.doc2bow(cl_lst)\n",
    "    \n",
    "    return lda_model.get_document_topics(doc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train['cleaned_text_string'] = df_train.cleaned_text.apply(' '.join)\n",
    "\n",
    "\n",
    "\n",
    "df_train['topic_probs_lda'] = df_train.cleaned_text.apply(topicify_cleane_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "def split_topicdata(frm, topicnum):\n",
    "    \n",
    "    # получаем колонку как лист\n",
    "    topic_probs = frm['topic_probs_lda'].tolist()\n",
    "    \n",
    "    rownum = len(topic_probs)\n",
    "    \n",
    "    generate_zeroes = [[0 for j in range(rownum)] for i in range(topicnum)]\n",
    "    \n",
    "    for tuplistnum, tuplist in enumerate(topic_probs):\n",
    "        for tup in tuplist:\n",
    "            \n",
    "            generate_zeroes[tup[0]][tuplistnum] = tup[1]\n",
    "            \n",
    "    return generate_zeroes\n",
    "\n",
    "\n",
    "test_run = split_topicdata(df_train, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colnum, col in enumerate(test_run):\n",
    "    # print(len(col))\n",
    "    \n",
    "    df_train['lda_topic_' + str(colnum)] = col\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "# теперь повторим это для теста\n",
    "df_test['cleaned_text'] = df_test.text.apply(tokenize_stem)\n",
    "\n",
    "df_test['topic_probs_lda'] = df_test.cleaned_text.apply(topicify_cleane_text_list)\n",
    "\n",
    "test_test_run = split_topicdata(df_test, 25)\n",
    "\n",
    "for colnum, col in enumerate(test_test_run):\n",
    "    # print(len(col))\n",
    "    \n",
    "    df_test['lda_topic_' + str(colnum)] = col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'text', 'length', 'num_words', 'num_unique_words',\n",
       "       'num_punctuations', 'num_words_upper', 'num_words_title',\n",
       "       'mean_word_len', 'num_stopwords', 'lexical_diversity', 'long',\n",
       "       'see', 'year', 'happi', 'word', 'feel', 'death', 'fear', 'old',\n",
       "       'face', 'even', 'father', 'look', 'chang', 'came', 'yet', 'ever',\n",
       "       'door', 'first', 'made', 'place', 'raymond', 'heard', 'hope',\n",
       "       'time', 'littl', 'say', 'heart', 'thus', 'life', 'make', 'whose',\n",
       "       'pass', 'thought', 'never', 'room', 'men', 'great', 'still', 'come',\n",
       "       'dream', 'upon', 'found', 'love', 'well', 'mani', 'know', 'point',\n",
       "       'one', 'mind', 'howev', 'everi', 'two', 'seem', 'saw', 'appear',\n",
       "       'die', 'certain', 'light', 'man', 'like', 'shall', 'eye', 'thing',\n",
       "       'might', 'return', 'said', 'hous', 'friend', 'hand', 'street',\n",
       "       'night', 'may', 'much', 'strang', 'day', 'natur', 'must', 'though',\n",
       "       'us', 'w2v_feature_0', 'w2v_feature_1', 'w2v_feature_2',\n",
       "       'w2v_feature_3', 'w2v_feature_4', 'w2v_feature_5', 'w2v_feature_6',\n",
       "       'w2v_feature_7', 'w2v_feature_8', 'w2v_feature_9', 'w2v_feature_10',\n",
       "       'w2v_feature_11', 'w2v_feature_12', 'w2v_feature_13',\n",
       "       'w2v_feature_14', 'w2v_feature_15', 'w2v_feature_16',\n",
       "       'w2v_feature_17', 'w2v_feature_18', 'w2v_feature_19',\n",
       "       'w2v_feature_20', 'w2v_feature_21', 'w2v_feature_22',\n",
       "       'w2v_feature_23', 'w2v_feature_24', 'w2v_feature_25',\n",
       "       'w2v_feature_26', 'w2v_feature_27', 'w2v_feature_28',\n",
       "       'w2v_feature_29', 'w2v_feature_30', 'w2v_feature_31',\n",
       "       'w2v_feature_32', 'w2v_feature_33', 'w2v_feature_34',\n",
       "       'w2v_feature_35', 'w2v_feature_36', 'w2v_feature_37',\n",
       "       'w2v_feature_38', 'w2v_feature_39', 'w2v_feature_40',\n",
       "       'w2v_feature_41', 'w2v_feature_42', 'w2v_feature_43',\n",
       "       'w2v_feature_44', 'w2v_feature_45', 'w2v_feature_46',\n",
       "       'w2v_feature_47', 'w2v_feature_48', 'w2v_feature_49',\n",
       "       'w2v_feature_50', 'w2v_feature_51', 'w2v_feature_52',\n",
       "       'w2v_feature_53', 'w2v_feature_54', 'w2v_feature_55',\n",
       "       'w2v_feature_56', 'w2v_feature_57', 'w2v_feature_58',\n",
       "       'w2v_feature_59', 'w2v_feature_60', 'w2v_feature_61',\n",
       "       'w2v_feature_62', 'w2v_feature_63', 'w2v_feature_64',\n",
       "       'w2v_feature_65', 'w2v_feature_66', 'w2v_feature_67',\n",
       "       'w2v_feature_68', 'w2v_feature_69', 'w2v_feature_70',\n",
       "       'w2v_feature_71', 'w2v_feature_72', 'w2v_feature_73',\n",
       "       'w2v_feature_74', 'w2v_feature_75', 'w2v_feature_76',\n",
       "       'w2v_feature_77', 'w2v_feature_78', 'w2v_feature_79',\n",
       "       'w2v_feature_80', 'w2v_feature_81', 'w2v_feature_82',\n",
       "       'w2v_feature_83', 'w2v_feature_84', 'w2v_feature_85',\n",
       "       'w2v_feature_86', 'w2v_feature_87', 'w2v_feature_88',\n",
       "       'w2v_feature_89', 'w2v_feature_90', 'w2v_feature_91',\n",
       "       'w2v_feature_92', 'w2v_feature_93', 'w2v_feature_94',\n",
       "       'w2v_feature_95', 'w2v_feature_96', 'w2v_feature_97',\n",
       "       'w2v_feature_98', 'w2v_feature_99', 'mws_index', 'eap_index',\n",
       "       'hpl_index', 'svd_word_0', 'svd_word_1', 'svd_word_2', 'svd_word_3',\n",
       "       'svd_word_4', 'svd_word_5', 'svd_word_6', 'svd_word_7',\n",
       "       'svd_word_8', 'svd_word_9', 'svd_word_10', 'svd_word_11',\n",
       "       'svd_word_12', 'svd_word_13', 'svd_word_14', 'svd_word_15',\n",
       "       'svd_word_16', 'svd_word_17', 'svd_word_18', 'svd_word_19',\n",
       "       'svd_word_20', 'svd_word_21', 'svd_word_22', 'svd_word_23',\n",
       "       'svd_word_24', 'svd_word_25', 'svd_word_26', 'svd_word_27',\n",
       "       'svd_word_28', 'svd_word_29', 'svd_word_30', 'svd_word_31',\n",
       "       'svd_word_32', 'svd_word_33', 'svd_word_34', 'svd_word_35',\n",
       "       'svd_word_36', 'svd_word_37', 'svd_word_38', 'svd_word_39',\n",
       "       'svd_word_40', 'svd_word_41', 'svd_word_42', 'svd_word_43',\n",
       "       'svd_word_44', 'svd_word_45', 'svd_word_46', 'svd_word_47',\n",
       "       'svd_word_48', 'svd_word_49', 'nb_cvec_eap', 'nb_cvec_hpl',\n",
       "       'nb_cvec_mws', 'cleaned_text', 'topic_probs_lda', 'lda_topic_0',\n",
       "       'lda_topic_1', 'lda_topic_2', 'lda_topic_3', 'lda_topic_4',\n",
       "       'lda_topic_5', 'lda_topic_6', 'lda_topic_7', 'lda_topic_8',\n",
       "       'lda_topic_9', 'lda_topic_10', 'lda_topic_11', 'lda_topic_12',\n",
       "       'lda_topic_13', 'lda_topic_14', 'lda_topic_15', 'lda_topic_16',\n",
       "       'lda_topic_17', 'lda_topic_18', 'lda_topic_19', 'lda_topic_20',\n",
       "       'lda_topic_21', 'lda_topic_22', 'lda_topic_23', 'lda_topic_24'], dtype=object)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['author2', 'id', 'text', 'author', 'cleaned_text',\n",
       "       'cleaned_text_string', 'length', 'num_words', 'num_unique_words',\n",
       "       'num_punctuations', 'num_words_upper', 'num_words_title',\n",
       "       'mean_word_len', 'num_stopwords', 'lexical_diversity', 'long',\n",
       "       'see', 'year', 'happi', 'word', 'feel', 'death', 'fear', 'old',\n",
       "       'face', 'even', 'father', 'look', 'chang', 'came', 'yet', 'ever',\n",
       "       'door', 'first', 'made', 'place', 'raymond', 'heard', 'hope',\n",
       "       'time', 'littl', 'say', 'heart', 'thus', 'life', 'make', 'whose',\n",
       "       'pass', 'thought', 'never', 'room', 'men', 'great', 'still', 'come',\n",
       "       'dream', 'upon', 'found', 'love', 'well', 'mani', 'know', 'point',\n",
       "       'one', 'mind', 'howev', 'everi', 'two', 'seem', 'saw', 'appear',\n",
       "       'die', 'certain', 'light', 'man', 'like', 'shall', 'eye', 'thing',\n",
       "       'might', 'return', 'said', 'hous', 'friend', 'hand', 'street',\n",
       "       'night', 'may', 'much', 'strang', 'day', 'natur', 'must', 'though',\n",
       "       'us', 'w2v_feature_0', 'w2v_feature_1', 'w2v_feature_2',\n",
       "       'w2v_feature_3', 'w2v_feature_4', 'w2v_feature_5', 'w2v_feature_6',\n",
       "       'w2v_feature_7', 'w2v_feature_8', 'w2v_feature_9', 'w2v_feature_10',\n",
       "       'w2v_feature_11', 'w2v_feature_12', 'w2v_feature_13',\n",
       "       'w2v_feature_14', 'w2v_feature_15', 'w2v_feature_16',\n",
       "       'w2v_feature_17', 'w2v_feature_18', 'w2v_feature_19',\n",
       "       'w2v_feature_20', 'w2v_feature_21', 'w2v_feature_22',\n",
       "       'w2v_feature_23', 'w2v_feature_24', 'w2v_feature_25',\n",
       "       'w2v_feature_26', 'w2v_feature_27', 'w2v_feature_28',\n",
       "       'w2v_feature_29', 'w2v_feature_30', 'w2v_feature_31',\n",
       "       'w2v_feature_32', 'w2v_feature_33', 'w2v_feature_34',\n",
       "       'w2v_feature_35', 'w2v_feature_36', 'w2v_feature_37',\n",
       "       'w2v_feature_38', 'w2v_feature_39', 'w2v_feature_40',\n",
       "       'w2v_feature_41', 'w2v_feature_42', 'w2v_feature_43',\n",
       "       'w2v_feature_44', 'w2v_feature_45', 'w2v_feature_46',\n",
       "       'w2v_feature_47', 'w2v_feature_48', 'w2v_feature_49',\n",
       "       'w2v_feature_50', 'w2v_feature_51', 'w2v_feature_52',\n",
       "       'w2v_feature_53', 'w2v_feature_54', 'w2v_feature_55',\n",
       "       'w2v_feature_56', 'w2v_feature_57', 'w2v_feature_58',\n",
       "       'w2v_feature_59', 'w2v_feature_60', 'w2v_feature_61',\n",
       "       'w2v_feature_62', 'w2v_feature_63', 'w2v_feature_64',\n",
       "       'w2v_feature_65', 'w2v_feature_66', 'w2v_feature_67',\n",
       "       'w2v_feature_68', 'w2v_feature_69', 'w2v_feature_70',\n",
       "       'w2v_feature_71', 'w2v_feature_72', 'w2v_feature_73',\n",
       "       'w2v_feature_74', 'w2v_feature_75', 'w2v_feature_76',\n",
       "       'w2v_feature_77', 'w2v_feature_78', 'w2v_feature_79',\n",
       "       'w2v_feature_80', 'w2v_feature_81', 'w2v_feature_82',\n",
       "       'w2v_feature_83', 'w2v_feature_84', 'w2v_feature_85',\n",
       "       'w2v_feature_86', 'w2v_feature_87', 'w2v_feature_88',\n",
       "       'w2v_feature_89', 'w2v_feature_90', 'w2v_feature_91',\n",
       "       'w2v_feature_92', 'w2v_feature_93', 'w2v_feature_94',\n",
       "       'w2v_feature_95', 'w2v_feature_96', 'w2v_feature_97',\n",
       "       'w2v_feature_98', 'w2v_feature_99', 'mws_index', 'eap_index',\n",
       "       'hpl_index', 'svd_word_0', 'svd_word_1', 'svd_word_2', 'svd_word_3',\n",
       "       'svd_word_4', 'svd_word_5', 'svd_word_6', 'svd_word_7',\n",
       "       'svd_word_8', 'svd_word_9', 'svd_word_10', 'svd_word_11',\n",
       "       'svd_word_12', 'svd_word_13', 'svd_word_14', 'svd_word_15',\n",
       "       'svd_word_16', 'svd_word_17', 'svd_word_18', 'svd_word_19',\n",
       "       'svd_word_20', 'svd_word_21', 'svd_word_22', 'svd_word_23',\n",
       "       'svd_word_24', 'svd_word_25', 'svd_word_26', 'svd_word_27',\n",
       "       'svd_word_28', 'svd_word_29', 'svd_word_30', 'svd_word_31',\n",
       "       'svd_word_32', 'svd_word_33', 'svd_word_34', 'svd_word_35',\n",
       "       'svd_word_36', 'svd_word_37', 'svd_word_38', 'svd_word_39',\n",
       "       'svd_word_40', 'svd_word_41', 'svd_word_42', 'svd_word_43',\n",
       "       'svd_word_44', 'svd_word_45', 'svd_word_46', 'svd_word_47',\n",
       "       'svd_word_48', 'svd_word_49', 'nb_cvec_eap', 'nb_cvec_hpl',\n",
       "       'nb_cvec_mws', 'topic_probs_lda', 'lda_topic_0', 'lda_topic_1',\n",
       "       'lda_topic_2', 'lda_topic_3', 'lda_topic_4', 'lda_topic_5',\n",
       "       'lda_topic_6', 'lda_topic_7', 'lda_topic_8', 'lda_topic_9',\n",
       "       'lda_topic_10', 'lda_topic_11', 'lda_topic_12', 'lda_topic_13',\n",
       "       'lda_topic_14', 'lda_topic_15', 'lda_topic_16', 'lda_topic_17',\n",
       "       'lda_topic_18', 'lda_topic_19', 'lda_topic_20', 'lda_topic_21',\n",
       "       'lda_topic_22', 'lda_topic_23', 'lda_topic_24'], dtype=object)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_test['topic_probs_lda']\n",
    "del df_train['topic_probs_lda']\n",
    "del df_test['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# готово! время прохонять"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
