{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\radames\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Radames\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Radames\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Radames\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from string import digits\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes, pipeline\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from stop_words import get_stop_words\n",
    "import gensim\n",
    "import re\n",
    "from gensim.models import ldamodel as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "# 3 столбца - id, text, author\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нулей не должно быть!\n",
    "\n",
    "\n",
    "def sum_up_word2vec_array(clnd_text):\n",
    "    \n",
    "    total = None\n",
    "    \n",
    "    for word in clnd_text:\n",
    "        if word in w2v:\n",
    "            if total is None:\n",
    "                total = w2v[word]\n",
    "            else:\n",
    "                total = np.add(total, w2v[word])\n",
    "                \n",
    "    if total is None:\n",
    "        return np.zeros(w2v_array_len)\n",
    "    else:\n",
    "        return total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нулей не должно быть!\n",
    "\n",
    "\n",
    "def avg_up_word2vec_array(clnd_text):\n",
    "    \n",
    "    total = None\n",
    "    \n",
    "    for word in clnd_text:\n",
    "        if word in w2v:\n",
    "            if total is None:\n",
    "                total = w2v[word]\n",
    "            else:                \n",
    "                total = np.mean([total, w2v[word]], axis=0)\n",
    "                \n",
    "                \n",
    "    if total is None:\n",
    "        return np.zeros(w2v_array_len)\n",
    "    else:\n",
    "        return total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "def tokenize_stem(file_text):\n",
    "    #firstly let's apply nltk tokenization\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #deleting stop_words\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_text'] = df_train.text.apply(tokenize_stem)\n",
    "df_train['cleaned_text_string'] = df_train.cleaned_text.apply(' '.join)\n",
    "df_train.head(n=3)\n",
    "eng_stopwords = set(stopwords.words('english')).union(set(get_stop_words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(file_text):\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "    except:\n",
    "        nltk.download('punkt')\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "        \n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return len(set(tokens))/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[matrix([[ 0.00053369,  0.00053369,  0.00106738, ...,  0.00053369,\n",
      "          0.        ,  0.        ]]), matrix([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "          0.00135189,  0.00067595]]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "# вытаскиваем \"значимые\" слова\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "raw_documents_authors = ['', '', '']\n",
    "\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    if row['author'] == 'EAP':\n",
    "        raw_documents_authors[0] += row['cleaned_text_string'] + ' '\n",
    "    elif row['author'] == 'HPL':\n",
    "        raw_documents_authors[1] += row['cleaned_text_string'] + ' '\n",
    "    else:\n",
    "        raw_documents_authors[2] += row['cleaned_text_string'] + ' '\n",
    "        \n",
    "\n",
    "# удалим уникальные слова, не встречающиеся у других писателей\n",
    "\n",
    "eap_only = set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "hpl_only = set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "msh_only = set(raw_documents_authors[2].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' '))\n",
    "\n",
    "unique_words = eap_only.union(hpl_only).union(msh_only)\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word')\n",
    "idf_matrix =  tf.fit_transform(raw_documents_authors)\n",
    "feature_names = tf.get_feature_names()\n",
    "# dictionary_word = dict(zip(feature_names, idf_matrix))\n",
    "\n",
    "dense_idf = [i.todense() for i in idf_matrix]\n",
    "print(dense_idf)\n",
    "\n",
    "max_weighted_term = []\n",
    "\n",
    "eap_dense_list = dense_idf[0].tolist()[0]\n",
    "hpl_dense_list = dense_idf[1].tolist()[0]\n",
    "mws_dense_list = dense_idf[2].tolist()[0]\n",
    "\n",
    "for inum, i in enumerate(eap_dense_list):\n",
    "    max_weighted_term.append(max(hpl_dense_list[inum], mws_dense_list[inum], \n",
    "                             i))\n",
    "\n",
    "max_tf_dict = dict(zip(feature_names, max_weighted_term))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(df_train['cleaned_text'], size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_array_len = list(w2v.items())[0][1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_words(tfidfdict, numwrd):\n",
    "\n",
    "    top_word_dict, min_value, min_key = {}, 99, ''\n",
    "    \n",
    "\n",
    "    for k, v in max_tf_dict.items():\n",
    "        # print(top_word_dict.values())\n",
    "        # print(v)\n",
    "        if k not in unique_words and k not in eng_stopwords:\n",
    "        \n",
    "            if len(top_word_dict) < numwrd:\n",
    "                top_word_dict[k] = v\n",
    "                if v <= min_value:\n",
    "                    min_key = k\n",
    "            else:\n",
    "                # print(v, min(list(top_word_dict.values())))\n",
    "                if v > min(list(top_word_dict.values())) and k not in eng_stopwords:\n",
    "\n",
    "                    min_value = min(top_word_dict.values())\n",
    "\n",
    "                    for ky, va in top_word_dict.items():\n",
    "                        if va == min_value:\n",
    "                            min_key = ky\n",
    "\n",
    "                    top_word_dict.pop(min_key)\n",
    "                    top_word_dict[k] = v\n",
    "                \n",
    "    return top_word_dict\n",
    "another_top_words_dict = extract_top_words(max_tf_dict, 80)\n",
    "high_tf_idf_words_columns = list(another_top_words_dict.keys())\n",
    "\n",
    "\n",
    "def count_topwords(target_df):\n",
    "\n",
    "    for word in high_tf_idf_words_columns:\n",
    "        \n",
    "        # TODO: костыль, нужен, когда у нас уже есть такие столбцы\n",
    "        # в датасете\n",
    "#         try:\n",
    "#             target_df = target_df.drop(word, 1)\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "        \n",
    "\n",
    "        def count_numwords(collist):\n",
    "            value = 0\n",
    "\n",
    "            for wd in collist:\n",
    "                if wd == word:\n",
    "                    value += 1\n",
    "            return value\n",
    "\n",
    "\n",
    "        target_df[word] = target_df.cleaned_text.apply(count_numwords)\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>chang</th>\n",
       "      <th>mani</th>\n",
       "      <th>fear</th>\n",
       "      <th>one</th>\n",
       "      <th>thus</th>\n",
       "      <th>certain</th>\n",
       "      <th>still</th>\n",
       "      <th>return</th>\n",
       "      <th>saw</th>\n",
       "      <th>see</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper ...   chang  mani  \\\n",
       "0                35                 7                2 ...       0     0   \n",
       "1                14                 1                0 ...       0     0   \n",
       "2                32                 5                0 ...       0     0   \n",
       "\n",
       "   fear  one thus  certain  still  return  saw  see  \n",
       "0     0    0    0        0      0       1    0    0  \n",
       "1     0    0    0        0      0       0    0    0  \n",
       "2     0    0    0        0      0       0    0    0  \n",
       "\n",
       "[3 rows x 95 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['length']=df_train['cleaned_text_string'].apply(len)\n",
    "df_train[\"num_words\"] = df_train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_train[\"num_unique_words\"] = df_train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_train[\"num_punctuations\"] =df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_train[\"num_words_upper\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_train[\"num_words_title\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_train[\"mean_word_len\"] = df_train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_train[\"num_stopwords\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_train['lexical_diversity'] = df_train.text.apply(lexical_diversity)\n",
    "df_train['w2v_array'] = df_train.cleaned_text.apply(sum_up_word2vec_array)\n",
    "count_topwords(df_train)\n",
    "\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_w2v_columns(target_df):\n",
    "    \n",
    "    # сначала вытаскиваем колонку как список списков\n",
    "    \n",
    "    w2v_array = target_df['w2v_array'].tolist()\n",
    "    \n",
    "    for i in range(100):\n",
    "        \n",
    "        target_df['w2v_feature_' + str(i)] = [j[i] for j in w2v_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_w2v_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.663512</td>\n",
       "      <td>-1.303735</td>\n",
       "      <td>-0.877374</td>\n",
       "      <td>-7.937459</td>\n",
       "      <td>-4.798588</td>\n",
       "      <td>-7.241487</td>\n",
       "      <td>-9.307872</td>\n",
       "      <td>-3.895346</td>\n",
       "      <td>3.014773</td>\n",
       "      <td>10.633761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.008078</td>\n",
       "      <td>-0.375688</td>\n",
       "      <td>-0.311576</td>\n",
       "      <td>-2.121509</td>\n",
       "      <td>-1.593851</td>\n",
       "      <td>-1.819666</td>\n",
       "      <td>-2.784693</td>\n",
       "      <td>-1.001561</td>\n",
       "      <td>0.877215</td>\n",
       "      <td>3.173744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152554</td>\n",
       "      <td>-1.109466</td>\n",
       "      <td>-0.952268</td>\n",
       "      <td>-7.125866</td>\n",
       "      <td>-2.325616</td>\n",
       "      <td>-7.382561</td>\n",
       "      <td>-6.925181</td>\n",
       "      <td>-4.383532</td>\n",
       "      <td>2.261675</td>\n",
       "      <td>8.054424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, love, spring, as, look, windsor, terrac,...</td>\n",
       "      <td>how love spring as look windsor terrac sixteen...</td>\n",
       "      <td>144</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.126091</td>\n",
       "      <td>-1.244493</td>\n",
       "      <td>-1.154061</td>\n",
       "      <td>-7.403440</td>\n",
       "      <td>-3.271107</td>\n",
       "      <td>-7.417303</td>\n",
       "      <td>-8.028737</td>\n",
       "      <td>-4.295195</td>\n",
       "      <td>2.497749</td>\n",
       "      <td>9.283039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[find, noth, els, even, gold, superintend, aba...</td>\n",
       "      <td>find noth els even gold superintend abandon at...</td>\n",
       "      <td>102</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.407288</td>\n",
       "      <td>-0.927115</td>\n",
       "      <td>-0.816135</td>\n",
       "      <td>-4.990958</td>\n",
       "      <td>-2.828885</td>\n",
       "      <td>-4.729518</td>\n",
       "      <td>-5.940284</td>\n",
       "      <td>-2.799661</td>\n",
       "      <td>1.966485</td>\n",
       "      <td>6.781491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "3  [how, love, spring, as, look, windsor, terrac,...   \n",
       "4  [find, noth, els, even, gold, superintend, aba...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "3  how love spring as look windsor terrac sixteen...     144         34   \n",
       "4  find noth els even gold superintend abandon at...     102         27   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper       ...        \\\n",
       "0                35                 7                2       ...         \n",
       "1                14                 1                0       ...         \n",
       "2                32                 5                0       ...         \n",
       "3                32                 4                0       ...         \n",
       "4                25                 4                0       ...         \n",
       "\n",
       "   w2v_feature_90  w2v_feature_91  w2v_feature_92  w2v_feature_93  \\\n",
       "0       -2.663512       -1.303735       -0.877374       -7.937459   \n",
       "1       -1.008078       -0.375688       -0.311576       -2.121509   \n",
       "2       -0.152554       -1.109466       -0.952268       -7.125866   \n",
       "3       -1.126091       -1.244493       -1.154061       -7.403440   \n",
       "4       -1.407288       -0.927115       -0.816135       -4.990958   \n",
       "\n",
       "  w2v_feature_94  w2v_feature_95  w2v_feature_96  w2v_feature_97  \\\n",
       "0      -4.798588       -7.241487       -9.307872       -3.895346   \n",
       "1      -1.593851       -1.819666       -2.784693       -1.001561   \n",
       "2      -2.325616       -7.382561       -6.925181       -4.383532   \n",
       "3      -3.271107       -7.417303       -8.028737       -4.295195   \n",
       "4      -2.828885       -4.729518       -5.940284       -2.799661   \n",
       "\n",
       "   w2v_feature_98  w2v_feature_99  \n",
       "0        3.014773       10.633761  \n",
       "1        0.877215        3.173744  \n",
       "2        2.261675        8.054424  \n",
       "3        2.497749        9.283039  \n",
       "4        1.966485        6.781491  \n",
       "\n",
       "[5 rows x 195 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>happi</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>81.543165</td>\n",
       "      <td>25.442405</td>\n",
       "      <td>21.894937</td>\n",
       "      <td>4.096329</td>\n",
       "      <td>0.553291</td>\n",
       "      <td>2.102405</td>\n",
       "      <td>4.644952</td>\n",
       "      <td>12.747595</td>\n",
       "      <td>0.886060</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.597563</td>\n",
       "      <td>-0.711742</td>\n",
       "      <td>-0.616946</td>\n",
       "      <td>-4.337550</td>\n",
       "      <td>-2.765478</td>\n",
       "      <td>-3.670272</td>\n",
       "      <td>-4.922817</td>\n",
       "      <td>-2.245777</td>\n",
       "      <td>1.494961</td>\n",
       "      <td>6.133319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>60.100183</td>\n",
       "      <td>18.567706</td>\n",
       "      <td>13.727397</td>\n",
       "      <td>3.573788</td>\n",
       "      <td>0.892966</td>\n",
       "      <td>2.052241</td>\n",
       "      <td>0.631340</td>\n",
       "      <td>9.619779</td>\n",
       "      <td>0.097354</td>\n",
       "      <td>0.070110</td>\n",
       "      <td>...</td>\n",
       "      <td>2.060546</td>\n",
       "      <td>0.492144</td>\n",
       "      <td>0.494714</td>\n",
       "      <td>3.051287</td>\n",
       "      <td>2.432590</td>\n",
       "      <td>3.045421</td>\n",
       "      <td>3.386442</td>\n",
       "      <td>1.728497</td>\n",
       "      <td>1.034629</td>\n",
       "      <td>4.427708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.132179</td>\n",
       "      <td>-7.327130</td>\n",
       "      <td>-10.641767</td>\n",
       "      <td>-45.374699</td>\n",
       "      <td>-54.478886</td>\n",
       "      <td>-32.246254</td>\n",
       "      <td>-54.783573</td>\n",
       "      <td>-19.000381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.045488</td>\n",
       "      <td>-0.911061</td>\n",
       "      <td>-0.793333</td>\n",
       "      <td>-5.618233</td>\n",
       "      <td>-3.482996</td>\n",
       "      <td>-5.050953</td>\n",
       "      <td>-6.285369</td>\n",
       "      <td>-2.985216</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>3.167969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.998157</td>\n",
       "      <td>-0.594339</td>\n",
       "      <td>-0.502430</td>\n",
       "      <td>-3.578559</td>\n",
       "      <td>-2.088254</td>\n",
       "      <td>-3.033720</td>\n",
       "      <td>-4.106851</td>\n",
       "      <td>-1.822431</td>\n",
       "      <td>1.238525</td>\n",
       "      <td>5.069916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.402371</td>\n",
       "      <td>-0.378131</td>\n",
       "      <td>-0.298529</td>\n",
       "      <td>-2.227310</td>\n",
       "      <td>-1.267485</td>\n",
       "      <td>-1.652519</td>\n",
       "      <td>-2.615695</td>\n",
       "      <td>-1.049155</td>\n",
       "      <td>1.916356</td>\n",
       "      <td>7.795946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>925.000000</td>\n",
       "      <td>267.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.529351</td>\n",
       "      <td>2.105792</td>\n",
       "      <td>4.669187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.967430</td>\n",
       "      <td>8.583466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408612</td>\n",
       "      <td>13.935845</td>\n",
       "      <td>91.209198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            length    num_words  num_unique_words  num_punctuations  \\\n",
       "count  7900.000000  7900.000000       7900.000000       7900.000000   \n",
       "mean     81.543165    25.442405         21.894937          4.096329   \n",
       "std      60.100183    18.567706         13.727397          3.573788   \n",
       "min       5.000000     2.000000          2.000000          1.000000   \n",
       "25%      40.000000    12.000000         12.000000          2.000000   \n",
       "50%      66.000000    21.000000         19.000000          3.000000   \n",
       "75%     106.000000    33.000000         29.000000          5.000000   \n",
       "max     925.000000   267.000000        155.000000         71.000000   \n",
       "\n",
       "       num_words_upper  num_words_title  mean_word_len  num_stopwords  \\\n",
       "count      7900.000000      7900.000000    7900.000000    7900.000000   \n",
       "mean          0.553291         2.102405       4.644952      12.747595   \n",
       "std           0.892966         2.052241       0.631340       9.619779   \n",
       "min           0.000000         0.000000       2.000000       0.000000   \n",
       "25%           0.000000         1.000000       4.250000       6.000000   \n",
       "50%           0.000000         1.000000       4.600000      10.000000   \n",
       "75%           1.000000         2.000000       5.000000      17.000000   \n",
       "max          15.000000        43.000000      11.000000     135.000000   \n",
       "\n",
       "       lexical_diversity        happi       ...        w2v_feature_90  \\\n",
       "count        7900.000000  7900.000000       ...           7900.000000   \n",
       "mean            0.886060     0.004684       ...             -1.597563   \n",
       "std             0.097354     0.070110       ...              2.060546   \n",
       "min             0.333333     0.000000       ...            -43.132179   \n",
       "25%             0.821429     0.000000       ...             -2.045488   \n",
       "50%             0.894737     0.000000       ...             -0.998157   \n",
       "75%             1.000000     0.000000       ...             -0.402371   \n",
       "max             1.000000     2.000000       ...              2.529351   \n",
       "\n",
       "       w2v_feature_91  w2v_feature_92  w2v_feature_93  w2v_feature_94  \\\n",
       "count     7900.000000     7900.000000     7900.000000     7900.000000   \n",
       "mean        -0.711742       -0.616946       -4.337550       -2.765478   \n",
       "std          0.492144        0.494714        3.051287        2.432590   \n",
       "min         -7.327130      -10.641767      -45.374699      -54.478886   \n",
       "25%         -0.911061       -0.793333       -5.618233       -3.482996   \n",
       "50%         -0.594339       -0.502430       -3.578559       -2.088254   \n",
       "75%         -0.378131       -0.298529       -2.227310       -1.267485   \n",
       "max          2.105792        4.669187        0.000000        2.967430   \n",
       "\n",
       "       w2v_feature_95  w2v_feature_96  w2v_feature_97  w2v_feature_98  \\\n",
       "count     7900.000000     7900.000000     7900.000000     7900.000000   \n",
       "mean        -3.670272       -4.922817       -2.245777        1.494961   \n",
       "std          3.045421        3.386442        1.728497        1.034629   \n",
       "min        -32.246254      -54.783573      -19.000381        0.000000   \n",
       "25%         -5.050953       -6.285369       -2.985216        0.788991   \n",
       "50%         -3.033720       -4.106851       -1.822431        1.238525   \n",
       "75%         -1.652519       -2.615695       -1.049155        1.916356   \n",
       "max          8.583466        0.000000        0.408612       13.935845   \n",
       "\n",
       "       w2v_feature_99  \n",
       "count     7900.000000  \n",
       "mean         6.133319  \n",
       "std          4.427708  \n",
       "min          0.000000  \n",
       "25%          3.167969  \n",
       "50%          5.069916  \n",
       "75%          7.795946  \n",
       "max         91.209198  \n",
       "\n",
       "[8 rows x 189 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eap=df_train[df_train['author']=='EAP']\n",
    "df_eap.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>happi</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>86.124586</td>\n",
       "      <td>27.417273</td>\n",
       "      <td>23.544672</td>\n",
       "      <td>3.833719</td>\n",
       "      <td>0.751489</td>\n",
       "      <td>2.124255</td>\n",
       "      <td>4.598182</td>\n",
       "      <td>13.896923</td>\n",
       "      <td>0.883407</td>\n",
       "      <td>0.030609</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.803665</td>\n",
       "      <td>-0.785075</td>\n",
       "      <td>-0.668287</td>\n",
       "      <td>-4.735346</td>\n",
       "      <td>-3.037104</td>\n",
       "      <td>-4.115414</td>\n",
       "      <td>-5.532511</td>\n",
       "      <td>-2.404975</td>\n",
       "      <td>1.703766</td>\n",
       "      <td>6.679250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>71.976281</td>\n",
       "      <td>23.134440</td>\n",
       "      <td>14.925835</td>\n",
       "      <td>2.840625</td>\n",
       "      <td>1.203636</td>\n",
       "      <td>1.759572</td>\n",
       "      <td>0.561558</td>\n",
       "      <td>12.196599</td>\n",
       "      <td>0.086804</td>\n",
       "      <td>0.180708</td>\n",
       "      <td>...</td>\n",
       "      <td>2.058326</td>\n",
       "      <td>0.667723</td>\n",
       "      <td>0.593003</td>\n",
       "      <td>4.007942</td>\n",
       "      <td>2.736596</td>\n",
       "      <td>3.838224</td>\n",
       "      <td>4.640517</td>\n",
       "      <td>2.154190</td>\n",
       "      <td>1.456653</td>\n",
       "      <td>5.568243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.951378</td>\n",
       "      <td>-26.515741</td>\n",
       "      <td>-22.135078</td>\n",
       "      <td>-158.751236</td>\n",
       "      <td>-85.285522</td>\n",
       "      <td>-150.461166</td>\n",
       "      <td>-179.890717</td>\n",
       "      <td>-85.041237</td>\n",
       "      <td>0.026838</td>\n",
       "      <td>0.104845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.252982</td>\n",
       "      <td>-0.979568</td>\n",
       "      <td>-0.849959</td>\n",
       "      <td>-5.920853</td>\n",
       "      <td>-3.670478</td>\n",
       "      <td>-5.349403</td>\n",
       "      <td>-6.842672</td>\n",
       "      <td>-3.101918</td>\n",
       "      <td>0.957934</td>\n",
       "      <td>3.778657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.560791</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.237344</td>\n",
       "      <td>-0.672247</td>\n",
       "      <td>-0.558439</td>\n",
       "      <td>-4.049242</td>\n",
       "      <td>-2.419662</td>\n",
       "      <td>-3.521115</td>\n",
       "      <td>-4.757585</td>\n",
       "      <td>-2.037106</td>\n",
       "      <td>1.459436</td>\n",
       "      <td>5.636068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>108.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.907156</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.615417</td>\n",
       "      <td>-0.445167</td>\n",
       "      <td>-0.350724</td>\n",
       "      <td>-2.676190</td>\n",
       "      <td>-1.562614</td>\n",
       "      <td>-2.122534</td>\n",
       "      <td>-3.140221</td>\n",
       "      <td>-1.265842</td>\n",
       "      <td>2.113236</td>\n",
       "      <td>8.213998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2715.000000</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.166953</td>\n",
       "      <td>-0.015917</td>\n",
       "      <td>0.125284</td>\n",
       "      <td>-0.069200</td>\n",
       "      <td>-0.032010</td>\n",
       "      <td>3.329655</td>\n",
       "      <td>-0.079661</td>\n",
       "      <td>0.309729</td>\n",
       "      <td>57.268726</td>\n",
       "      <td>208.190536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            length    num_words  num_unique_words  num_punctuations  \\\n",
       "count  6044.000000  6044.000000       6044.000000       6044.000000   \n",
       "mean     86.124586    27.417273         23.544672          3.833719   \n",
       "std      71.976281    23.134440         14.925835          2.840625   \n",
       "min       4.000000     2.000000          2.000000          1.000000   \n",
       "25%      48.000000    15.000000         14.000000          2.000000   \n",
       "50%      74.000000    23.000000         21.000000          3.000000   \n",
       "75%     108.000000    34.000000         30.000000          5.000000   \n",
       "max    2715.000000   861.000000        429.000000         59.000000   \n",
       "\n",
       "       num_words_upper  num_words_title  mean_word_len  num_stopwords  \\\n",
       "count      6044.000000      6044.000000    6044.000000    6044.000000   \n",
       "mean          0.751489         2.124255       4.598182      13.896923   \n",
       "std           1.203636         1.759572       0.561558      12.196599   \n",
       "min           0.000000         0.000000       2.666667       0.000000   \n",
       "25%           0.000000         1.000000       4.250000       7.000000   \n",
       "50%           0.000000         2.000000       4.560791      12.000000   \n",
       "75%           1.000000         3.000000       4.907156      18.000000   \n",
       "max          27.000000        46.000000      10.500000     437.000000   \n",
       "\n",
       "       lexical_diversity        happi       ...        w2v_feature_90  \\\n",
       "count        6044.000000  6044.000000       ...           6044.000000   \n",
       "mean            0.883407     0.030609       ...             -1.803665   \n",
       "std             0.086804     0.180708       ...              2.058326   \n",
       "min             0.398990     0.000000       ...            -44.951378   \n",
       "25%             0.823529     0.000000       ...             -2.252982   \n",
       "50%             0.885714     0.000000       ...             -1.237344   \n",
       "75%             0.950000     0.000000       ...             -0.615417   \n",
       "max             1.000000     2.000000       ...              1.166953   \n",
       "\n",
       "       w2v_feature_91  w2v_feature_92  w2v_feature_93  w2v_feature_94  \\\n",
       "count     6044.000000     6044.000000     6044.000000     6044.000000   \n",
       "mean        -0.785075       -0.668287       -4.735346       -3.037104   \n",
       "std          0.667723        0.593003        4.007942        2.736596   \n",
       "min        -26.515741      -22.135078     -158.751236      -85.285522   \n",
       "25%         -0.979568       -0.849959       -5.920853       -3.670478   \n",
       "50%         -0.672247       -0.558439       -4.049242       -2.419662   \n",
       "75%         -0.445167       -0.350724       -2.676190       -1.562614   \n",
       "max         -0.015917        0.125284       -0.069200       -0.032010   \n",
       "\n",
       "       w2v_feature_95  w2v_feature_96  w2v_feature_97  w2v_feature_98  \\\n",
       "count     6044.000000     6044.000000     6044.000000     6044.000000   \n",
       "mean        -4.115414       -5.532511       -2.404975        1.703766   \n",
       "std          3.838224        4.640517        2.154190        1.456653   \n",
       "min       -150.461166     -179.890717      -85.041237        0.026838   \n",
       "25%         -5.349403       -6.842672       -3.101918        0.957934   \n",
       "50%         -3.521115       -4.757585       -2.037106        1.459436   \n",
       "75%         -2.122534       -3.140221       -1.265842        2.113236   \n",
       "max          3.329655       -0.079661        0.309729       57.268726   \n",
       "\n",
       "       w2v_feature_99  \n",
       "count     6044.000000  \n",
       "mean         6.679250  \n",
       "std          5.568243  \n",
       "min          0.104845  \n",
       "25%          3.778657  \n",
       "50%          5.636068  \n",
       "75%          8.213998  \n",
       "max        208.190536  \n",
       "\n",
       "[8 rows x 189 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mws=df_train[df_train['author']=='MWS']\n",
    "df_mws.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordset=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#делаю сет со всеми словами\n",
    "for i in df_train.index:\n",
    "    wordset |= set(df_train['cleaned_text'][i])\n",
    "wordlist=list(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>mws</th>\n",
       "      <th>eap</th>\n",
       "      <th>hpl</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bland</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ph'nglui</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bacon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sail</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tudor</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  mws  eap  hpl  all\n",
       "0     bland    0    0    0    0\n",
       "1  ph'nglui    0    0    0    0\n",
       "2     bacon    0    0    0    0\n",
       "3      sail    0    0    0    0\n",
       "4     tudor    0    0    0    0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#делаю фрейм со словами\n",
    "df_word=pd.DataFrame(columns=[\"word\", \"mws\", \"eap\", \"hpl\", \"all\"])\n",
    "df_word[\"word\"]=wordlist\n",
    "df_word[\"mws\"]=0\n",
    "df_word[\"eap\"]=0\n",
    "df_word[\"hpl\"]=0\n",
    "df_word[\"all\"]=0\n",
    "df_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# как мы будем эту штуку правильнее делать (возможно это жуткий костыль), я хз\n",
    "# сначала создаем словарь где ключ - уникальное слово, а значение - его порядковый номер\n",
    "# затем создаем разреженную матрицу, которую заполняем в зависимости от порядковых номеров \n",
    "word_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#делаю сет со всеми словами\n",
    "# и сразу заготовку под шапку(потом увидишь зачем)\n",
    "counter = 0\n",
    "head = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    for word in wordlist:\n",
    "        if word not in word_dict:\n",
    "            head.append(word)\n",
    "            word_dict[word] = counter\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# видоизменять колонки в pandas руками по одному значению в строке или столбце - очень плохая идея\n",
    "# колонка это numpy.ndarray, а значит при каждой итерации она будет пересоздаваться\n",
    "# что угробит производительность\n",
    "# делаем значит так. считаем где сколько и где встречались отдельные слова, затем создаем строку за строкой для \n",
    "# каждого предложения\n",
    "\n",
    "list_of_lists = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    row = [0 for i in range(len(word_dict))]\n",
    "    for word in wordlist:\n",
    "        row[word_dict[word]] += 1\n",
    "    list_of_lists.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... и для того чтобы посмотреть встречаемость того или иного слова по авторам добавим такую колонку\n",
    "\n",
    "count_frame = pd.DataFrame(list_of_lists)\n",
    "count_frame['author'] = df_train['author']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... и теперь нормальную шапку делаем\n",
    "\n",
    "count_frame.columns = head + ['author']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   this  process  howev  afford  mean  ascertain  dimens  dungeon  i  might  \\\n",
      "0     1        1      1       1     1          1       1        1  2      1   \n",
      "1     0        0      0       0     0          0       0        0  0      1   \n",
      "2     0        0      0       0     0          0       0        0  0      0   \n",
      "3     0        0      0       0     0          0       0        0  0      0   \n",
      "4     0        0      0       0     0          0       0        0  0      0   \n",
      "\n",
      "    ...    aegidus  burr  bentley  waltzer  binder  brusqueri  adriat  ancona  \\\n",
      "0   ...          0     0        0        0       0          0       0       0   \n",
      "1   ...          0     0        0        0       0          0       0       0   \n",
      "2   ...          0     0        0        0       0          0       0       0   \n",
      "3   ...          0     0        0        0       0          0       0       0   \n",
      "4   ...          0     0        0        0       0          0       0       0   \n",
      "\n",
      "   agir  author  \n",
      "0     0     EAP  \n",
      "1     0     HPL  \n",
      "2     0     EAP  \n",
      "3     0     MWS  \n",
      "4     0     HPL  \n",
      "\n",
      "[5 rows x 15231 columns]\n"
     ]
    }
   ],
   "source": [
    "print(count_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пока объединим все, потом может быть будем использовать\n",
    "col=list(count_frame.columns)\n",
    "col[-1]='author_name'\n",
    "count_frame.columns=col\n",
    "pivot_col=pd.pivot_table(count_frame, aggfunc=np.sum, values=col, index=['author_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 14351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  \\\n",
       "author_name                                                                  \n",
       "EAP             1      2      0       22       0     2      1     2      3   \n",
       "HPL             0      0      0       17      24     0      1     3      0   \n",
       "MWS             0      0      1        9       0     0      0     1      2   \n",
       "\n",
       "             abbrevi  ...    æmilianus  æneid  ærial  æronaut  ærostat  \\\n",
       "author_name           ...                                                \n",
       "EAP                2  ...            0      0      1        3        1   \n",
       "HPL                0  ...            2      1      0        0        0   \n",
       "MWS                0  ...            0      0      0        0        0   \n",
       "\n",
       "             æschylus  élite  émeut  οἶδα  υπνος  \n",
       "author_name                                       \n",
       "EAP                 1      1      1     0      0  \n",
       "HPL                 0      0      0     2      1  \n",
       "MWS                 0      0      0     0      0  \n",
       "\n",
       "[3 rows x 14351 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Убираем лишние слова, которые не учли раньше\n",
    "col=list(pivot_col.columns)\n",
    "col2=[string for string in col if (string[0]!='\"' and string[0]!=\"'\"\n",
    "                                  and string[0]!='.' and string[0]!='`'\n",
    "                                   and len(string)>3 and '.' not in string)]\n",
    "col=[]\n",
    "pivot_col=pivot_col[col2]\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUMA</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 14351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  abbrevi  \\\n",
       "EAP      1      2      0       22       0     2      1     2      3        2   \n",
       "HPL      0      0      0       17      24     0      1     3      0        0   \n",
       "MWS      0      0      1        9       0     0      0     1      2        0   \n",
       "SUMA     1      2      1       48      24     2      2     6      5        2   \n",
       "\n",
       "      ...    æmilianus  æneid  ærial  æronaut  ærostat  æschylus  élite  \\\n",
       "EAP   ...            0      0      1        3        1         1      1   \n",
       "HPL   ...            2      1      0        0        0         0      0   \n",
       "MWS   ...            0      0      0        0        0         0      0   \n",
       "SUMA  ...            2      1      1        3        1         1      1   \n",
       "\n",
       "      émeut  οἶδα  υπνος  \n",
       "EAP       1     0      0  \n",
       "HPL       0     2      1  \n",
       "MWS       0     0      0  \n",
       "SUMA      1     2      1  \n",
       "\n",
       "[4 rows x 14351 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create pivot\n",
    "pivot_col=pivot_col.append(pivot_col.sum(), ignore_index=True)\n",
    "pivot_col.index=['EAP', 'HPL', 'MWS', 'SUMA']\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "      <th>summa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>74269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUMA</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>235194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 14352 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  abbrevi  \\\n",
       "EAP      1      2      0       22       0     2      1     2      3        2   \n",
       "HPL      0      0      0       17      24     0      1     3      0        0   \n",
       "MWS      0      0      1        9       0     0      0     1      2        0   \n",
       "SUMA     1      2      1       48      24     2      2     6      5        2   \n",
       "\n",
       "       ...    æneid  ærial  æronaut  ærostat  æschylus  élite  émeut  οἶδα  \\\n",
       "EAP    ...        0      1        3        1         1      1      1     0   \n",
       "HPL    ...        1      0        0        0         0      0      0     2   \n",
       "MWS    ...        0      0        0        0         0      0      0     0   \n",
       "SUMA   ...        1      1        3        1         1      1      1     2   \n",
       "\n",
       "      υπνος   summa  \n",
       "EAP       0   87765  \n",
       "HPL       1   74269  \n",
       "MWS       0   73160  \n",
       "SUMA      1  235194  \n",
       "\n",
       "[4 rows x 14352 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa=[pivot_col.loc['EAP'].sum(), pivot_col.loc['HPL'].sum(), \n",
    "       pivot_col.loc['MWS'].sum(), pivot_col.loc['SUMA'].sum()]\n",
    "pivot_col['summa']=summa\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdic</th>\n",
       "      <th>aberr</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abhorr</th>\n",
       "      <th>abil</th>\n",
       "      <th>abject</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>your</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zenith</th>\n",
       "      <th>zest</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zone</th>\n",
       "      <th>summa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.373160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.315778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 6619 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandon  abash      abat  abbey     abdic     aberr     abhor    abhorr  \\\n",
       "EAP  0.458333    0.5  0.333333    0.6  0.142857  0.166667  0.058824  0.111111   \n",
       "HPL  0.354167    0.5  0.500000    0.0  0.000000  0.666667  0.235294  0.555556   \n",
       "MWS  0.187500    0.0  0.166667    0.4  0.857143  0.166667  0.705882  0.333333   \n",
       "\n",
       "         abil    abject    ...      younger  youngest      your     youth  \\\n",
       "EAP  0.789474  0.333333    ...     0.272727       0.2  0.534884  0.101562   \n",
       "HPL  0.052632  0.000000    ...     0.000000       0.4  0.069767  0.429688   \n",
       "MWS  0.157895  0.666667    ...     0.727273       0.4  0.395349  0.468750   \n",
       "\n",
       "         zeal  zenith  zest  zigzag      zone     summa  \n",
       "EAP  0.117647     0.4   0.2     0.4  0.666667  0.373160  \n",
       "HPL  0.470588     0.6   0.2     0.6  0.333333  0.315778  \n",
       "MWS  0.411765     0.0   0.6     0.0  0.000000  0.311062  \n",
       "\n",
       "[3 rows x 6619 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create probability of author text knowing that a word was used\n",
    "pivot_part=pivot_col\n",
    "pivot_part.loc['EAP']=pivot_col.loc['EAP']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['HPL']=pivot_col.loc['HPL']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['MWS']=pivot_col.loc['MWS']/pivot_col.loc['SUMA']\n",
    "pivot_part=pivot_part.loc[['EAP', 'HPL', 'MWS']]\n",
    "# Delete unique words\n",
    "pivot_part=pivot_part.loc[:, (pivot_part!=1).all(axis=0)]\n",
    "pivot_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44859813084112149"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will be easier to work this way\n",
    "eap_dict=pivot_part.loc['EAP'].to_dict()\n",
    "hpl_dict=pivot_part.loc['HPL'].to_dict()\n",
    "mws_dict=pivot_part.loc['MWS'].to_dict()\n",
    "eap_dict['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create author score \n",
    "def ind_val_eap(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=eap_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_hpl(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=hpl_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_mws(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=mws_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.937459</td>\n",
       "      <td>-4.798588</td>\n",
       "      <td>-7.241487</td>\n",
       "      <td>-9.307872</td>\n",
       "      <td>-3.895346</td>\n",
       "      <td>3.014773</td>\n",
       "      <td>10.633761</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.074388</td>\n",
       "      <td>0.034504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.121509</td>\n",
       "      <td>-1.593851</td>\n",
       "      <td>-1.819666</td>\n",
       "      <td>-2.784693</td>\n",
       "      <td>-1.001561</td>\n",
       "      <td>0.877215</td>\n",
       "      <td>3.173744</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.034739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.125866</td>\n",
       "      <td>-2.325616</td>\n",
       "      <td>-7.382561</td>\n",
       "      <td>-6.925181</td>\n",
       "      <td>-4.383532</td>\n",
       "      <td>2.261675</td>\n",
       "      <td>8.054424</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper    ...      \\\n",
       "0                35                 7                2    ...       \n",
       "1                14                 1                0    ...       \n",
       "2                32                 5                0    ...       \n",
       "\n",
       "   w2v_feature_93  w2v_feature_94  w2v_feature_95  w2v_feature_96  \\\n",
       "0       -7.937459       -4.798588       -7.241487       -9.307872   \n",
       "1       -2.121509       -1.593851       -1.819666       -2.784693   \n",
       "2       -7.125866       -2.325616       -7.382561       -6.925181   \n",
       "\n",
       "  w2v_feature_97  w2v_feature_98  w2v_feature_99  mws_index  eap_index  \\\n",
       "0      -3.895346        3.014773       10.633761   0.035935   0.074388   \n",
       "1      -1.001561        0.877215        3.173744   0.035860   0.060980   \n",
       "2      -4.383532        2.261675        8.054424   0.026900   0.047832   \n",
       "\n",
       "   hpl_index  \n",
       "0   0.034504  \n",
       "1   0.034739  \n",
       "2   0.037337  \n",
       "\n",
       "[3 rows x 198 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add index of author\n",
    "df_train['mws_index']=df_train['cleaned_text'].apply(ind_val_mws)/df_train['length']\n",
    "df_train['eap_index']=df_train['cleaned_text'].apply(ind_val_eap)/df_train['length']\n",
    "df_train['hpl_index']=df_train['cleaned_text'].apply(ind_val_hpl)/df_train['length']\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author2</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.937459</td>\n",
       "      <td>-4.798588</td>\n",
       "      <td>-7.241487</td>\n",
       "      <td>-9.307872</td>\n",
       "      <td>-3.895346</td>\n",
       "      <td>3.014773</td>\n",
       "      <td>10.633761</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.074388</td>\n",
       "      <td>0.034504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.121509</td>\n",
       "      <td>-1.593851</td>\n",
       "      <td>-1.819666</td>\n",
       "      <td>-2.784693</td>\n",
       "      <td>-1.001561</td>\n",
       "      <td>0.877215</td>\n",
       "      <td>3.173744</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.034739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.125866</td>\n",
       "      <td>-2.325616</td>\n",
       "      <td>-7.382561</td>\n",
       "      <td>-6.925181</td>\n",
       "      <td>-4.383532</td>\n",
       "      <td>2.261675</td>\n",
       "      <td>8.054424</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, love, spring, as, look, windsor, terrac,...</td>\n",
       "      <td>how love spring as look windsor terrac sixteen...</td>\n",
       "      <td>144</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.403440</td>\n",
       "      <td>-3.271107</td>\n",
       "      <td>-7.417303</td>\n",
       "      <td>-8.028737</td>\n",
       "      <td>-4.295195</td>\n",
       "      <td>2.497749</td>\n",
       "      <td>9.283039</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.033438</td>\n",
       "      <td>0.033601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[find, noth, els, even, gold, superintend, aba...</td>\n",
       "      <td>find noth els even gold superintend abandon at...</td>\n",
       "      <td>102</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.990958</td>\n",
       "      <td>-2.828885</td>\n",
       "      <td>-4.729518</td>\n",
       "      <td>-5.940284</td>\n",
       "      <td>-2.799661</td>\n",
       "      <td>1.966485</td>\n",
       "      <td>6.781491</td>\n",
       "      <td>0.036859</td>\n",
       "      <td>0.056661</td>\n",
       "      <td>0.043735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 199 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author2       id                                               text author  \\\n",
       "0        0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1        1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2        0  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3        2  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4        1  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "3  [how, love, spring, as, look, windsor, terrac,...   \n",
       "4  [find, noth, els, even, gold, superintend, aba...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "3  how love spring as look windsor terrac sixteen...     144         34   \n",
       "4  find noth els even gold superintend abandon at...     102         27   \n",
       "\n",
       "   num_unique_words  num_punctuations    ...      w2v_feature_93  \\\n",
       "0                35                 7    ...           -7.937459   \n",
       "1                14                 1    ...           -2.121509   \n",
       "2                32                 5    ...           -7.125866   \n",
       "3                32                 4    ...           -7.403440   \n",
       "4                25                 4    ...           -4.990958   \n",
       "\n",
       "   w2v_feature_94  w2v_feature_95  w2v_feature_96  w2v_feature_97  \\\n",
       "0       -4.798588       -7.241487       -9.307872       -3.895346   \n",
       "1       -1.593851       -1.819666       -2.784693       -1.001561   \n",
       "2       -2.325616       -7.382561       -6.925181       -4.383532   \n",
       "3       -3.271107       -7.417303       -8.028737       -4.295195   \n",
       "4       -2.828885       -4.729518       -5.940284       -2.799661   \n",
       "\n",
       "  w2v_feature_98  w2v_feature_99  mws_index  eap_index  hpl_index  \n",
       "0       3.014773       10.633761   0.035935   0.074388   0.034504  \n",
       "1       0.877215        3.173744   0.035860   0.060980   0.034739  \n",
       "2       2.261675        8.054424   0.026900   0.047832   0.037337  \n",
       "3       2.497749        9.283039   0.071850   0.033438   0.033601  \n",
       "4       1.966485        6.781491   0.036859   0.056661   0.043735  \n",
       "\n",
       "[5 rows x 199 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform authors' names to numeric\n",
    "df_train['author']=df_train['author'].astype('category')\n",
    "df_train['author2']=df_train['author'].cat.codes\n",
    "# Create different features \n",
    "df_train.head(n=3)\n",
    "mid = df_train['author2']\n",
    "df_train.drop(labels=['author2'], axis=1,inplace = True)\n",
    "df_train.insert(0, 'author2', mid)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = df_train['author'].map(author_mapping_dict)\n",
    "train_id = df_train['id'].values\n",
    "test_id = df_test['id'].values\n",
    "cols_to_drop = ['id', 'text']\n",
    "train_X = df_train.drop(cols_to_drop+['author'], axis=1)\n",
    "test_X = df_test.drop(cols_to_drop, axis=1)\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf = tfidf_vec.fit_transform(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.862729297781\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=3, shuffle=True, random_state=194)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 50\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "df_test = pd.concat([df_test, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the count vectorizer ###\n",
    "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "tfidf_vec.fit(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.450918416166\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# add the predictions as new features #\n",
    "df_train[\"nb_cvec_eap\"] = pred_train[:,0]\n",
    "df_train[\"nb_cvec_hpl\"] = pred_train[:,1]\n",
    "df_train[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "df_test[\"nb_cvec_eap\"] = pred_full_test[:,0]\n",
    "df_test[\"nb_cvec_hpl\"] = pred_full_test[:,1]\n",
    "df_test[\"nb_cvec_mws\"] = pred_full_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author2</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_43</th>\n",
       "      <th>svd_word_44</th>\n",
       "      <th>svd_word_45</th>\n",
       "      <th>svd_word_46</th>\n",
       "      <th>svd_word_47</th>\n",
       "      <th>svd_word_48</th>\n",
       "      <th>svd_word_49</th>\n",
       "      <th>nb_cvec_eap</th>\n",
       "      <th>nb_cvec_hpl</th>\n",
       "      <th>nb_cvec_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060822</td>\n",
       "      <td>-0.019470</td>\n",
       "      <td>0.022374</td>\n",
       "      <td>0.019055</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.037074</td>\n",
       "      <td>0.002873</td>\n",
       "      <td>9.999933e-01</td>\n",
       "      <td>2.752790e-06</td>\n",
       "      <td>3.990111e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>-0.001032</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>-0.004602</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>-0.002629</td>\n",
       "      <td>8.226820e-01</td>\n",
       "      <td>1.492107e-01</td>\n",
       "      <td>2.810727e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031303</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>-0.014951</td>\n",
       "      <td>-0.028148</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.024380</td>\n",
       "      <td>-0.027564</td>\n",
       "      <td>9.999918e-01</td>\n",
       "      <td>8.206128e-06</td>\n",
       "      <td>1.064720e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, love, spring, as, look, windsor, terrac,...</td>\n",
       "      <td>how love spring as look windsor terrac sixteen...</td>\n",
       "      <td>144</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>-0.009910</td>\n",
       "      <td>0.015214</td>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.017966</td>\n",
       "      <td>-0.011112</td>\n",
       "      <td>1.436890e-09</td>\n",
       "      <td>7.472578e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[find, noth, els, even, gold, superintend, aba...</td>\n",
       "      <td>find noth els even gold superintend abandon at...</td>\n",
       "      <td>102</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>-0.001091</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.009183</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>8.960309e-01</td>\n",
       "      <td>1.016456e-01</td>\n",
       "      <td>2.323469e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author2       id                                               text author  \\\n",
       "0        0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1        1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2        0  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3        2  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4        1  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "3  [how, love, spring, as, look, windsor, terrac,...   \n",
       "4  [find, noth, els, even, gold, superintend, aba...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "3  how love spring as look windsor terrac sixteen...     144         34   \n",
       "4  find noth els even gold superintend abandon at...     102         27   \n",
       "\n",
       "   num_unique_words  num_punctuations      ...       svd_word_43  svd_word_44  \\\n",
       "0                35                 7      ...          0.060822    -0.019470   \n",
       "1                14                 1      ...          0.001035     0.000631   \n",
       "2                32                 5      ...         -0.031303     0.011510   \n",
       "3                32                 4      ...          0.009119    -0.009910   \n",
       "4                25                 4      ...          0.001328    -0.000561   \n",
       "\n",
       "   svd_word_45  svd_word_46  svd_word_47 svd_word_48  svd_word_49  \\\n",
       "0     0.022374     0.019055     0.001587    0.037074     0.002873   \n",
       "1    -0.001032     0.003187    -0.004602    0.004110    -0.002629   \n",
       "2    -0.014951    -0.028148     0.010121    0.024380    -0.027564   \n",
       "3     0.015214     0.013138     0.002048    0.017966    -0.011112   \n",
       "4     0.003316    -0.001091     0.000047   -0.009183    -0.002713   \n",
       "\n",
       "    nb_cvec_eap   nb_cvec_hpl   nb_cvec_mws  \n",
       "0  9.999933e-01  2.752790e-06  3.990111e-06  \n",
       "1  8.226820e-01  1.492107e-01  2.810727e-02  \n",
       "2  9.999918e-01  8.206128e-06  1.064720e-08  \n",
       "3  1.436890e-09  7.472578e-10  1.000000e+00  \n",
       "4  8.960309e-01  1.016456e-01  2.323469e-03  \n",
       "\n",
       "[5 rows x 252 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['author2', 'id', 'text', 'author', 'cleaned_text',\n",
       "       'cleaned_text_string', 'length', 'num_words', 'num_unique_words',\n",
       "       'num_punctuations', 'num_words_upper', 'num_words_title',\n",
       "       'mean_word_len', 'num_stopwords', 'lexical_diversity', 'happi',\n",
       "       'hand', 'mind', 'like', 'found', 'death', 'time', 'night', 'though',\n",
       "       'hous', 'two', 'ever', 'much', 'year', 'never', 'come', 'even',\n",
       "       'day', 'yet', 'know', 'life', 'natur', 'men', 'upon', 'well',\n",
       "       'look', 'raymond', 'man', 'feel', 'shall', 'father', 'heart',\n",
       "       'point', 'hope', 'great', 'dream', 'littl', 'howev', 'came',\n",
       "       'strang', 'eye', 'street', 'room', 'door', 'friend', 'face',\n",
       "       'heard', 'say', 'thing', 'made', 'must', 'place', 'word', 'appear',\n",
       "       'thought', 'make', 'may', 'love', 'us', 'light', 'everi', 'die',\n",
       "       'old', 'whose', 'might', 'pass', 'long', 'said', 'first', 'seem',\n",
       "       'chang', 'mani', 'fear', 'one', 'thus', 'certain', 'still',\n",
       "       'return', 'saw', 'see', 'w2v_feature_0', 'w2v_feature_1',\n",
       "       'w2v_feature_2', 'w2v_feature_3', 'w2v_feature_4', 'w2v_feature_5',\n",
       "       'w2v_feature_6', 'w2v_feature_7', 'w2v_feature_8', 'w2v_feature_9',\n",
       "       'w2v_feature_10', 'w2v_feature_11', 'w2v_feature_12',\n",
       "       'w2v_feature_13', 'w2v_feature_14', 'w2v_feature_15',\n",
       "       'w2v_feature_16', 'w2v_feature_17', 'w2v_feature_18',\n",
       "       'w2v_feature_19', 'w2v_feature_20', 'w2v_feature_21',\n",
       "       'w2v_feature_22', 'w2v_feature_23', 'w2v_feature_24',\n",
       "       'w2v_feature_25', 'w2v_feature_26', 'w2v_feature_27',\n",
       "       'w2v_feature_28', 'w2v_feature_29', 'w2v_feature_30',\n",
       "       'w2v_feature_31', 'w2v_feature_32', 'w2v_feature_33',\n",
       "       'w2v_feature_34', 'w2v_feature_35', 'w2v_feature_36',\n",
       "       'w2v_feature_37', 'w2v_feature_38', 'w2v_feature_39',\n",
       "       'w2v_feature_40', 'w2v_feature_41', 'w2v_feature_42',\n",
       "       'w2v_feature_43', 'w2v_feature_44', 'w2v_feature_45',\n",
       "       'w2v_feature_46', 'w2v_feature_47', 'w2v_feature_48',\n",
       "       'w2v_feature_49', 'w2v_feature_50', 'w2v_feature_51',\n",
       "       'w2v_feature_52', 'w2v_feature_53', 'w2v_feature_54',\n",
       "       'w2v_feature_55', 'w2v_feature_56', 'w2v_feature_57',\n",
       "       'w2v_feature_58', 'w2v_feature_59', 'w2v_feature_60',\n",
       "       'w2v_feature_61', 'w2v_feature_62', 'w2v_feature_63',\n",
       "       'w2v_feature_64', 'w2v_feature_65', 'w2v_feature_66',\n",
       "       'w2v_feature_67', 'w2v_feature_68', 'w2v_feature_69',\n",
       "       'w2v_feature_70', 'w2v_feature_71', 'w2v_feature_72',\n",
       "       'w2v_feature_73', 'w2v_feature_74', 'w2v_feature_75',\n",
       "       'w2v_feature_76', 'w2v_feature_77', 'w2v_feature_78',\n",
       "       'w2v_feature_79', 'w2v_feature_80', 'w2v_feature_81',\n",
       "       'w2v_feature_82', 'w2v_feature_83', 'w2v_feature_84',\n",
       "       'w2v_feature_85', 'w2v_feature_86', 'w2v_feature_87',\n",
       "       'w2v_feature_88', 'w2v_feature_89', 'w2v_feature_90',\n",
       "       'w2v_feature_91', 'w2v_feature_92', 'w2v_feature_93',\n",
       "       'w2v_feature_94', 'w2v_feature_95', 'w2v_feature_96',\n",
       "       'w2v_feature_97', 'w2v_feature_98', 'w2v_feature_99', 'mws_index',\n",
       "       'eap_index', 'hpl_index', 'svd_word_0', 'svd_word_1', 'svd_word_2',\n",
       "       'svd_word_3', 'svd_word_4', 'svd_word_5', 'svd_word_6',\n",
       "       'svd_word_7', 'svd_word_8', 'svd_word_9', 'svd_word_10',\n",
       "       'svd_word_11', 'svd_word_12', 'svd_word_13', 'svd_word_14',\n",
       "       'svd_word_15', 'svd_word_16', 'svd_word_17', 'svd_word_18',\n",
       "       'svd_word_19', 'svd_word_20', 'svd_word_21', 'svd_word_22',\n",
       "       'svd_word_23', 'svd_word_24', 'svd_word_25', 'svd_word_26',\n",
       "       'svd_word_27', 'svd_word_28', 'svd_word_29', 'svd_word_30',\n",
       "       'svd_word_31', 'svd_word_32', 'svd_word_33', 'svd_word_34',\n",
       "       'svd_word_35', 'svd_word_36', 'svd_word_37', 'svd_word_38',\n",
       "       'svd_word_39', 'svd_word_40', 'svd_word_41', 'svd_word_42',\n",
       "       'svd_word_43', 'svd_word_44', 'svd_word_45', 'svd_word_46',\n",
       "       'svd_word_47', 'svd_word_48', 'svd_word_49', 'nb_cvec_eap',\n",
       "       'nb_cvec_hpl', 'nb_cvec_mws'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_train['w2v_array']\n",
    "df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n",
    "        print(message)\n",
    "        print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "class LemmaCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the entire training text in a list\n",
    "text = list(df_train.text.values)\n",
    "# Calling our overwritten Count vectorizer\n",
    "tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n",
    "                                     min_df=2,\n",
    "                                     stop_words='english',\n",
    "                                     decode_error='ignore')\n",
    "tf = tf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=13, max_iter=5,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=13, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00452489,  0.00452489,  0.00452489, ...,  0.00452499,\n",
       "         0.06415156,  0.00452489],\n",
       "       [ 0.01538462,  0.01538462,  0.01538467, ...,  0.01538462,\n",
       "         0.01538462,  0.01538462],\n",
       "       [ 0.00404858,  0.00404859,  0.89772543, ...,  0.00404861,\n",
       "         0.00404865,  0.00404867],\n",
       "       ..., \n",
       "       [ 0.00961538,  0.00961538,  0.00961556, ...,  0.0096154 ,\n",
       "         0.00961538,  0.00961538],\n",
       "       [ 0.01098901,  0.21741942,  0.01098901, ...,  0.01098901,\n",
       "         0.01098901,  0.01098906],\n",
       "       [ 0.00961538,  0.00961538,  0.13461525, ...,  0.00961563,\n",
       "         0.00961538,  0.00961544]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data set\n",
    "ds_train=df_train.values\n",
    "X=ds_train[:, 6:]\n",
    "Y=ds_train[:, 0]\n",
    "seed=7\n",
    "test_size=0.3\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X,Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.921491\ttest-mlogloss:0.922556\n",
      "[1]\ttrain-mlogloss:0.798604\ttest-mlogloss:0.800504\n",
      "[2]\ttrain-mlogloss:0.708918\ttest-mlogloss:0.711609\n",
      "[3]\ttrain-mlogloss:0.641138\ttest-mlogloss:0.644234\n",
      "[4]\ttrain-mlogloss:0.589194\ttest-mlogloss:0.592531\n",
      "[5]\ttrain-mlogloss:0.54839\ttest-mlogloss:0.552153\n",
      "[6]\ttrain-mlogloss:0.516373\ttest-mlogloss:0.520772\n",
      "[7]\ttrain-mlogloss:0.490757\ttest-mlogloss:0.495507\n",
      "[8]\ttrain-mlogloss:0.469581\ttest-mlogloss:0.475061\n",
      "[9]\ttrain-mlogloss:0.452595\ttest-mlogloss:0.458734\n",
      "[10]\ttrain-mlogloss:0.438175\ttest-mlogloss:0.444398\n",
      "[11]\ttrain-mlogloss:0.426337\ttest-mlogloss:0.432876\n",
      "[12]\ttrain-mlogloss:0.416557\ttest-mlogloss:0.423428\n",
      "[13]\ttrain-mlogloss:0.407965\ttest-mlogloss:0.415124\n",
      "[14]\ttrain-mlogloss:0.400703\ttest-mlogloss:0.408265\n",
      "[15]\ttrain-mlogloss:0.394627\ttest-mlogloss:0.402537\n",
      "[16]\ttrain-mlogloss:0.389215\ttest-mlogloss:0.397656\n",
      "[17]\ttrain-mlogloss:0.384517\ttest-mlogloss:0.393553\n",
      "[18]\ttrain-mlogloss:0.380119\ttest-mlogloss:0.389497\n",
      "[19]\ttrain-mlogloss:0.376339\ttest-mlogloss:0.386265\n",
      "[20]\ttrain-mlogloss:0.373003\ttest-mlogloss:0.383372\n",
      "[21]\ttrain-mlogloss:0.369852\ttest-mlogloss:0.380874\n",
      "[22]\ttrain-mlogloss:0.367032\ttest-mlogloss:0.378512\n",
      "[23]\ttrain-mlogloss:0.364473\ttest-mlogloss:0.376194\n",
      "[24]\ttrain-mlogloss:0.361987\ttest-mlogloss:0.374323\n",
      "[25]\ttrain-mlogloss:0.359752\ttest-mlogloss:0.372509\n",
      "[26]\ttrain-mlogloss:0.35731\ttest-mlogloss:0.37058\n",
      "[27]\ttrain-mlogloss:0.355001\ttest-mlogloss:0.36888\n",
      "[28]\ttrain-mlogloss:0.353209\ttest-mlogloss:0.367526\n",
      "[29]\ttrain-mlogloss:0.351443\ttest-mlogloss:0.366339\n",
      "[30]\ttrain-mlogloss:0.349732\ttest-mlogloss:0.365216\n",
      "[31]\ttrain-mlogloss:0.34793\ttest-mlogloss:0.364066\n",
      "[32]\ttrain-mlogloss:0.346486\ttest-mlogloss:0.362865\n",
      "[33]\ttrain-mlogloss:0.344916\ttest-mlogloss:0.361413\n",
      "[34]\ttrain-mlogloss:0.343312\ttest-mlogloss:0.360501\n",
      "[35]\ttrain-mlogloss:0.342088\ttest-mlogloss:0.359569\n",
      "[36]\ttrain-mlogloss:0.340832\ttest-mlogloss:0.358784\n",
      "[37]\ttrain-mlogloss:0.339411\ttest-mlogloss:0.357766\n",
      "[38]\ttrain-mlogloss:0.33796\ttest-mlogloss:0.356679\n",
      "[39]\ttrain-mlogloss:0.336822\ttest-mlogloss:0.35595\n",
      "[40]\ttrain-mlogloss:0.335627\ttest-mlogloss:0.355273\n",
      "[41]\ttrain-mlogloss:0.334562\ttest-mlogloss:0.354544\n",
      "[42]\ttrain-mlogloss:0.333201\ttest-mlogloss:0.353637\n",
      "[43]\ttrain-mlogloss:0.332214\ttest-mlogloss:0.353072\n",
      "[44]\ttrain-mlogloss:0.331152\ttest-mlogloss:0.352664\n",
      "[45]\ttrain-mlogloss:0.32995\ttest-mlogloss:0.352098\n",
      "[46]\ttrain-mlogloss:0.328979\ttest-mlogloss:0.351735\n",
      "[47]\ttrain-mlogloss:0.327961\ttest-mlogloss:0.351248\n",
      "[48]\ttrain-mlogloss:0.326792\ttest-mlogloss:0.350668\n",
      "[49]\ttrain-mlogloss:0.325934\ttest-mlogloss:0.350116\n",
      "[50]\ttrain-mlogloss:0.32497\ttest-mlogloss:0.349307\n",
      "[51]\ttrain-mlogloss:0.323981\ttest-mlogloss:0.348794\n",
      "[52]\ttrain-mlogloss:0.322938\ttest-mlogloss:0.348455\n",
      "[53]\ttrain-mlogloss:0.322151\ttest-mlogloss:0.348063\n",
      "[54]\ttrain-mlogloss:0.321035\ttest-mlogloss:0.347364\n",
      "[55]\ttrain-mlogloss:0.320271\ttest-mlogloss:0.347001\n",
      "[56]\ttrain-mlogloss:0.319384\ttest-mlogloss:0.346473\n",
      "[57]\ttrain-mlogloss:0.318499\ttest-mlogloss:0.346043\n",
      "[58]\ttrain-mlogloss:0.317565\ttest-mlogloss:0.345643\n",
      "[59]\ttrain-mlogloss:0.316725\ttest-mlogloss:0.34522\n",
      "[60]\ttrain-mlogloss:0.315887\ttest-mlogloss:0.344853\n",
      "[61]\ttrain-mlogloss:0.315036\ttest-mlogloss:0.344399\n",
      "[62]\ttrain-mlogloss:0.314132\ttest-mlogloss:0.3439\n",
      "[63]\ttrain-mlogloss:0.313251\ttest-mlogloss:0.343622\n",
      "[64]\ttrain-mlogloss:0.312407\ttest-mlogloss:0.342988\n",
      "[65]\ttrain-mlogloss:0.311669\ttest-mlogloss:0.342539\n",
      "[66]\ttrain-mlogloss:0.310917\ttest-mlogloss:0.342071\n",
      "[67]\ttrain-mlogloss:0.310219\ttest-mlogloss:0.341697\n",
      "[68]\ttrain-mlogloss:0.309405\ttest-mlogloss:0.341255\n",
      "[69]\ttrain-mlogloss:0.308668\ttest-mlogloss:0.341162\n",
      "[70]\ttrain-mlogloss:0.307972\ttest-mlogloss:0.340689\n",
      "[71]\ttrain-mlogloss:0.307263\ttest-mlogloss:0.340296\n",
      "[72]\ttrain-mlogloss:0.306546\ttest-mlogloss:0.33977\n",
      "[73]\ttrain-mlogloss:0.305863\ttest-mlogloss:0.339459\n",
      "[74]\ttrain-mlogloss:0.305196\ttest-mlogloss:0.339296\n",
      "[75]\ttrain-mlogloss:0.304613\ttest-mlogloss:0.338948\n",
      "[76]\ttrain-mlogloss:0.303832\ttest-mlogloss:0.338724\n",
      "[77]\ttrain-mlogloss:0.303099\ttest-mlogloss:0.338351\n",
      "[78]\ttrain-mlogloss:0.302467\ttest-mlogloss:0.338277\n",
      "[79]\ttrain-mlogloss:0.301813\ttest-mlogloss:0.338105\n",
      "[80]\ttrain-mlogloss:0.301214\ttest-mlogloss:0.337928\n",
      "[81]\ttrain-mlogloss:0.300542\ttest-mlogloss:0.337662\n",
      "[82]\ttrain-mlogloss:0.29984\ttest-mlogloss:0.337262\n",
      "[83]\ttrain-mlogloss:0.299243\ttest-mlogloss:0.337131\n",
      "[84]\ttrain-mlogloss:0.298685\ttest-mlogloss:0.336775\n",
      "[85]\ttrain-mlogloss:0.298188\ttest-mlogloss:0.336529\n",
      "[86]\ttrain-mlogloss:0.29755\ttest-mlogloss:0.336461\n",
      "[87]\ttrain-mlogloss:0.296991\ttest-mlogloss:0.336163\n",
      "[88]\ttrain-mlogloss:0.296451\ttest-mlogloss:0.3359\n",
      "[89]\ttrain-mlogloss:0.29588\ttest-mlogloss:0.335625\n",
      "[90]\ttrain-mlogloss:0.295331\ttest-mlogloss:0.335585\n",
      "[91]\ttrain-mlogloss:0.294838\ttest-mlogloss:0.335327\n",
      "[92]\ttrain-mlogloss:0.294171\ttest-mlogloss:0.335183\n",
      "[93]\ttrain-mlogloss:0.293675\ttest-mlogloss:0.334889\n",
      "[94]\ttrain-mlogloss:0.293175\ttest-mlogloss:0.334684\n",
      "[95]\ttrain-mlogloss:0.292642\ttest-mlogloss:0.334349\n",
      "[96]\ttrain-mlogloss:0.29203\ttest-mlogloss:0.334126\n",
      "[97]\ttrain-mlogloss:0.291632\ttest-mlogloss:0.334055\n",
      "[98]\ttrain-mlogloss:0.291122\ttest-mlogloss:0.333976\n",
      "[99]\ttrain-mlogloss:0.290615\ttest-mlogloss:0.333779\n",
      "[100]\ttrain-mlogloss:0.290095\ttest-mlogloss:0.33373\n",
      "[101]\ttrain-mlogloss:0.289514\ttest-mlogloss:0.333451\n",
      "[102]\ttrain-mlogloss:0.289036\ttest-mlogloss:0.333333\n",
      "[103]\ttrain-mlogloss:0.288483\ttest-mlogloss:0.333153\n",
      "[104]\ttrain-mlogloss:0.287991\ttest-mlogloss:0.332959\n",
      "[105]\ttrain-mlogloss:0.287466\ttest-mlogloss:0.332994\n",
      "[106]\ttrain-mlogloss:0.286923\ttest-mlogloss:0.333016\n",
      "[107]\ttrain-mlogloss:0.286481\ttest-mlogloss:0.332899\n",
      "[108]\ttrain-mlogloss:0.285903\ttest-mlogloss:0.332885\n",
      "[109]\ttrain-mlogloss:0.285424\ttest-mlogloss:0.332814\n",
      "[110]\ttrain-mlogloss:0.285073\ttest-mlogloss:0.332754\n",
      "[111]\ttrain-mlogloss:0.284666\ttest-mlogloss:0.332611\n",
      "[112]\ttrain-mlogloss:0.284271\ttest-mlogloss:0.33245\n",
      "[113]\ttrain-mlogloss:0.2838\ttest-mlogloss:0.3324\n",
      "[114]\ttrain-mlogloss:0.283293\ttest-mlogloss:0.332437\n",
      "[115]\ttrain-mlogloss:0.282801\ttest-mlogloss:0.332153\n",
      "[116]\ttrain-mlogloss:0.282344\ttest-mlogloss:0.33197\n",
      "[117]\ttrain-mlogloss:0.281915\ttest-mlogloss:0.331695\n",
      "[118]\ttrain-mlogloss:0.281565\ttest-mlogloss:0.331588\n",
      "[119]\ttrain-mlogloss:0.281037\ttest-mlogloss:0.331676\n",
      "[120]\ttrain-mlogloss:0.280565\ttest-mlogloss:0.331632\n",
      "[121]\ttrain-mlogloss:0.280128\ttest-mlogloss:0.331713\n",
      "[122]\ttrain-mlogloss:0.279776\ttest-mlogloss:0.331621\n",
      "[123]\ttrain-mlogloss:0.279406\ttest-mlogloss:0.331469\n",
      "[124]\ttrain-mlogloss:0.278843\ttest-mlogloss:0.331355\n",
      "[125]\ttrain-mlogloss:0.27852\ttest-mlogloss:0.331326\n",
      "[126]\ttrain-mlogloss:0.278102\ttest-mlogloss:0.331335\n",
      "[127]\ttrain-mlogloss:0.27768\ttest-mlogloss:0.331283\n",
      "[128]\ttrain-mlogloss:0.277158\ttest-mlogloss:0.331035\n",
      "[129]\ttrain-mlogloss:0.276737\ttest-mlogloss:0.330893\n",
      "[130]\ttrain-mlogloss:0.27621\ttest-mlogloss:0.33074\n",
      "[131]\ttrain-mlogloss:0.275768\ttest-mlogloss:0.330702\n",
      "[132]\ttrain-mlogloss:0.275389\ttest-mlogloss:0.330541\n",
      "[133]\ttrain-mlogloss:0.27503\ttest-mlogloss:0.330571\n",
      "[134]\ttrain-mlogloss:0.274533\ttest-mlogloss:0.330276\n",
      "[135]\ttrain-mlogloss:0.274213\ttest-mlogloss:0.330214\n",
      "[136]\ttrain-mlogloss:0.273738\ttest-mlogloss:0.330113\n",
      "[137]\ttrain-mlogloss:0.273289\ttest-mlogloss:0.330081\n",
      "[138]\ttrain-mlogloss:0.272957\ttest-mlogloss:0.329921\n",
      "[139]\ttrain-mlogloss:0.2725\ttest-mlogloss:0.329912\n",
      "[140]\ttrain-mlogloss:0.272068\ttest-mlogloss:0.329934\n",
      "[141]\ttrain-mlogloss:0.271644\ttest-mlogloss:0.329863\n",
      "[142]\ttrain-mlogloss:0.271257\ttest-mlogloss:0.32977\n",
      "[143]\ttrain-mlogloss:0.270945\ttest-mlogloss:0.329667\n",
      "[144]\ttrain-mlogloss:0.270508\ttest-mlogloss:0.329633\n",
      "[145]\ttrain-mlogloss:0.270119\ttest-mlogloss:0.329617\n",
      "[146]\ttrain-mlogloss:0.26968\ttest-mlogloss:0.329628\n",
      "[147]\ttrain-mlogloss:0.269274\ttest-mlogloss:0.329603\n",
      "[148]\ttrain-mlogloss:0.268927\ttest-mlogloss:0.329643\n",
      "[149]\ttrain-mlogloss:0.268572\ttest-mlogloss:0.329489\n",
      "[150]\ttrain-mlogloss:0.268271\ttest-mlogloss:0.329512\n",
      "[151]\ttrain-mlogloss:0.267884\ttest-mlogloss:0.329436\n",
      "[152]\ttrain-mlogloss:0.267516\ttest-mlogloss:0.329264\n",
      "[153]\ttrain-mlogloss:0.267149\ttest-mlogloss:0.329254\n",
      "[154]\ttrain-mlogloss:0.266936\ttest-mlogloss:0.329101\n",
      "[155]\ttrain-mlogloss:0.266547\ttest-mlogloss:0.329063\n",
      "[156]\ttrain-mlogloss:0.266197\ttest-mlogloss:0.328999\n",
      "[157]\ttrain-mlogloss:0.265871\ttest-mlogloss:0.329015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158]\ttrain-mlogloss:0.265446\ttest-mlogloss:0.328779\n",
      "[159]\ttrain-mlogloss:0.265132\ttest-mlogloss:0.328772\n",
      "[160]\ttrain-mlogloss:0.264843\ttest-mlogloss:0.328665\n",
      "[161]\ttrain-mlogloss:0.264411\ttest-mlogloss:0.328436\n",
      "[162]\ttrain-mlogloss:0.264074\ttest-mlogloss:0.328456\n",
      "[163]\ttrain-mlogloss:0.263798\ttest-mlogloss:0.3284\n",
      "[164]\ttrain-mlogloss:0.263471\ttest-mlogloss:0.32849\n",
      "[165]\ttrain-mlogloss:0.263045\ttest-mlogloss:0.328378\n",
      "[166]\ttrain-mlogloss:0.262677\ttest-mlogloss:0.328241\n",
      "[167]\ttrain-mlogloss:0.262418\ttest-mlogloss:0.32808\n",
      "[168]\ttrain-mlogloss:0.262137\ttest-mlogloss:0.328082\n",
      "[169]\ttrain-mlogloss:0.261753\ttest-mlogloss:0.328064\n",
      "[170]\ttrain-mlogloss:0.261391\ttest-mlogloss:0.328045\n",
      "[171]\ttrain-mlogloss:0.261016\ttest-mlogloss:0.327993\n",
      "[172]\ttrain-mlogloss:0.260653\ttest-mlogloss:0.328087\n",
      "[173]\ttrain-mlogloss:0.260296\ttest-mlogloss:0.328146\n",
      "[174]\ttrain-mlogloss:0.259915\ttest-mlogloss:0.327958\n",
      "[175]\ttrain-mlogloss:0.259577\ttest-mlogloss:0.327984\n",
      "[176]\ttrain-mlogloss:0.259189\ttest-mlogloss:0.327867\n",
      "[177]\ttrain-mlogloss:0.258971\ttest-mlogloss:0.327856\n",
      "[178]\ttrain-mlogloss:0.258624\ttest-mlogloss:0.3278\n",
      "[179]\ttrain-mlogloss:0.25825\ttest-mlogloss:0.327791\n",
      "[180]\ttrain-mlogloss:0.257998\ttest-mlogloss:0.327843\n",
      "[181]\ttrain-mlogloss:0.257661\ttest-mlogloss:0.327867\n",
      "[182]\ttrain-mlogloss:0.25725\ttest-mlogloss:0.327886\n",
      "[183]\ttrain-mlogloss:0.256914\ttest-mlogloss:0.327776\n",
      "[184]\ttrain-mlogloss:0.256526\ttest-mlogloss:0.327707\n",
      "[185]\ttrain-mlogloss:0.25627\ttest-mlogloss:0.327596\n",
      "[186]\ttrain-mlogloss:0.255992\ttest-mlogloss:0.3275\n",
      "[187]\ttrain-mlogloss:0.255723\ttest-mlogloss:0.327518\n",
      "[188]\ttrain-mlogloss:0.255447\ttest-mlogloss:0.327426\n",
      "[189]\ttrain-mlogloss:0.25512\ttest-mlogloss:0.327393\n",
      "[190]\ttrain-mlogloss:0.254785\ttest-mlogloss:0.327477\n",
      "[191]\ttrain-mlogloss:0.254461\ttest-mlogloss:0.327302\n",
      "[192]\ttrain-mlogloss:0.254152\ttest-mlogloss:0.327363\n",
      "[193]\ttrain-mlogloss:0.253848\ttest-mlogloss:0.327308\n",
      "[194]\ttrain-mlogloss:0.253665\ttest-mlogloss:0.327261\n",
      "[195]\ttrain-mlogloss:0.253369\ttest-mlogloss:0.327325\n",
      "[196]\ttrain-mlogloss:0.253083\ttest-mlogloss:0.327311\n",
      "[197]\ttrain-mlogloss:0.25278\ttest-mlogloss:0.327234\n",
      "[198]\ttrain-mlogloss:0.252542\ttest-mlogloss:0.327323\n",
      "[199]\ttrain-mlogloss:0.252275\ttest-mlogloss:0.327421\n",
      "[200]\ttrain-mlogloss:0.252007\ttest-mlogloss:0.327425\n",
      "[201]\ttrain-mlogloss:0.251725\ttest-mlogloss:0.327359\n",
      "[202]\ttrain-mlogloss:0.251359\ttest-mlogloss:0.327498\n",
      "[203]\ttrain-mlogloss:0.251052\ttest-mlogloss:0.32738\n",
      "[204]\ttrain-mlogloss:0.250811\ttest-mlogloss:0.327394\n",
      "[205]\ttrain-mlogloss:0.250494\ttest-mlogloss:0.327439\n",
      "[206]\ttrain-mlogloss:0.250139\ttest-mlogloss:0.327391\n",
      "[207]\ttrain-mlogloss:0.249904\ttest-mlogloss:0.327314\n",
      "[208]\ttrain-mlogloss:0.249549\ttest-mlogloss:0.32725\n",
      "[209]\ttrain-mlogloss:0.249187\ttest-mlogloss:0.32733\n",
      "[210]\ttrain-mlogloss:0.248825\ttest-mlogloss:0.32716\n",
      "[211]\ttrain-mlogloss:0.248588\ttest-mlogloss:0.32721\n",
      "[212]\ttrain-mlogloss:0.248297\ttest-mlogloss:0.327119\n",
      "[213]\ttrain-mlogloss:0.247961\ttest-mlogloss:0.327077\n",
      "[214]\ttrain-mlogloss:0.247597\ttest-mlogloss:0.327113\n",
      "[215]\ttrain-mlogloss:0.247339\ttest-mlogloss:0.327023\n",
      "[216]\ttrain-mlogloss:0.247084\ttest-mlogloss:0.327008\n",
      "[217]\ttrain-mlogloss:0.246842\ttest-mlogloss:0.32703\n",
      "[218]\ttrain-mlogloss:0.246547\ttest-mlogloss:0.326936\n",
      "[219]\ttrain-mlogloss:0.246267\ttest-mlogloss:0.326912\n",
      "[220]\ttrain-mlogloss:0.246005\ttest-mlogloss:0.326921\n",
      "[221]\ttrain-mlogloss:0.245697\ttest-mlogloss:0.326963\n",
      "[222]\ttrain-mlogloss:0.245402\ttest-mlogloss:0.32687\n",
      "[223]\ttrain-mlogloss:0.245112\ttest-mlogloss:0.326815\n",
      "[224]\ttrain-mlogloss:0.244876\ttest-mlogloss:0.326887\n",
      "[225]\ttrain-mlogloss:0.244659\ttest-mlogloss:0.3268\n",
      "[226]\ttrain-mlogloss:0.244396\ttest-mlogloss:0.326752\n",
      "[227]\ttrain-mlogloss:0.24412\ttest-mlogloss:0.326835\n",
      "[228]\ttrain-mlogloss:0.243859\ttest-mlogloss:0.326798\n",
      "[229]\ttrain-mlogloss:0.243566\ttest-mlogloss:0.326811\n",
      "[230]\ttrain-mlogloss:0.243308\ttest-mlogloss:0.326896\n",
      "[231]\ttrain-mlogloss:0.243035\ttest-mlogloss:0.326888\n",
      "[232]\ttrain-mlogloss:0.24278\ttest-mlogloss:0.326864\n",
      "[233]\ttrain-mlogloss:0.242515\ttest-mlogloss:0.326939\n",
      "[234]\ttrain-mlogloss:0.242151\ttest-mlogloss:0.326862\n",
      "[235]\ttrain-mlogloss:0.241889\ttest-mlogloss:0.3269\n",
      "[236]\ttrain-mlogloss:0.241645\ttest-mlogloss:0.326766\n",
      "[237]\ttrain-mlogloss:0.241292\ttest-mlogloss:0.326598\n",
      "[238]\ttrain-mlogloss:0.240969\ttest-mlogloss:0.326669\n",
      "[239]\ttrain-mlogloss:0.240618\ttest-mlogloss:0.326738\n",
      "Test error using softmax = 0.127170582226762\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "xg_train=xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test=xgb.DMatrix(X_test, label=y_test)\n",
    "xg_t=xgb.DMatrix(X, label=Y)\n",
    "param={}\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['eta'] = 0.2\n",
    "param['max_depth'] = 2\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 3\n",
    "param['eval_metric']= \"mlogloss\"\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 240\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# get prediction\n",
    "pred = bst.predict(xg_test)\n",
    "error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.921491\ttest-mlogloss:0.922556\n",
      "[1]\ttrain-mlogloss:0.798604\ttest-mlogloss:0.800504\n",
      "[2]\ttrain-mlogloss:0.708918\ttest-mlogloss:0.711609\n",
      "[3]\ttrain-mlogloss:0.641138\ttest-mlogloss:0.644234\n",
      "[4]\ttrain-mlogloss:0.589194\ttest-mlogloss:0.592531\n",
      "[5]\ttrain-mlogloss:0.54839\ttest-mlogloss:0.552153\n",
      "[6]\ttrain-mlogloss:0.516373\ttest-mlogloss:0.520772\n",
      "[7]\ttrain-mlogloss:0.490757\ttest-mlogloss:0.495507\n",
      "[8]\ttrain-mlogloss:0.469581\ttest-mlogloss:0.475061\n",
      "[9]\ttrain-mlogloss:0.452595\ttest-mlogloss:0.458734\n",
      "[10]\ttrain-mlogloss:0.438175\ttest-mlogloss:0.444398\n",
      "[11]\ttrain-mlogloss:0.426337\ttest-mlogloss:0.432876\n",
      "[12]\ttrain-mlogloss:0.416557\ttest-mlogloss:0.423428\n",
      "[13]\ttrain-mlogloss:0.407965\ttest-mlogloss:0.415124\n",
      "[14]\ttrain-mlogloss:0.400703\ttest-mlogloss:0.408265\n",
      "[15]\ttrain-mlogloss:0.394627\ttest-mlogloss:0.402537\n",
      "[16]\ttrain-mlogloss:0.389215\ttest-mlogloss:0.397656\n",
      "[17]\ttrain-mlogloss:0.384517\ttest-mlogloss:0.393553\n",
      "[18]\ttrain-mlogloss:0.380119\ttest-mlogloss:0.389497\n",
      "[19]\ttrain-mlogloss:0.376339\ttest-mlogloss:0.386265\n",
      "[20]\ttrain-mlogloss:0.373003\ttest-mlogloss:0.383372\n",
      "[21]\ttrain-mlogloss:0.369852\ttest-mlogloss:0.380874\n",
      "[22]\ttrain-mlogloss:0.367032\ttest-mlogloss:0.378512\n",
      "[23]\ttrain-mlogloss:0.364473\ttest-mlogloss:0.376194\n",
      "[24]\ttrain-mlogloss:0.361987\ttest-mlogloss:0.374323\n",
      "[25]\ttrain-mlogloss:0.359752\ttest-mlogloss:0.372509\n",
      "[26]\ttrain-mlogloss:0.35731\ttest-mlogloss:0.37058\n",
      "[27]\ttrain-mlogloss:0.355001\ttest-mlogloss:0.36888\n",
      "[28]\ttrain-mlogloss:0.353209\ttest-mlogloss:0.367526\n",
      "[29]\ttrain-mlogloss:0.351443\ttest-mlogloss:0.366339\n",
      "[30]\ttrain-mlogloss:0.349732\ttest-mlogloss:0.365216\n",
      "[31]\ttrain-mlogloss:0.34793\ttest-mlogloss:0.364066\n",
      "[32]\ttrain-mlogloss:0.346486\ttest-mlogloss:0.362865\n",
      "[33]\ttrain-mlogloss:0.344916\ttest-mlogloss:0.361413\n",
      "[34]\ttrain-mlogloss:0.343312\ttest-mlogloss:0.360501\n",
      "[35]\ttrain-mlogloss:0.342088\ttest-mlogloss:0.359569\n",
      "[36]\ttrain-mlogloss:0.340832\ttest-mlogloss:0.358784\n",
      "[37]\ttrain-mlogloss:0.339411\ttest-mlogloss:0.357766\n",
      "[38]\ttrain-mlogloss:0.33796\ttest-mlogloss:0.356679\n",
      "[39]\ttrain-mlogloss:0.336822\ttest-mlogloss:0.35595\n",
      "[40]\ttrain-mlogloss:0.335627\ttest-mlogloss:0.355273\n",
      "[41]\ttrain-mlogloss:0.334562\ttest-mlogloss:0.354544\n",
      "[42]\ttrain-mlogloss:0.333201\ttest-mlogloss:0.353637\n",
      "[43]\ttrain-mlogloss:0.332214\ttest-mlogloss:0.353072\n",
      "[44]\ttrain-mlogloss:0.331152\ttest-mlogloss:0.352664\n",
      "[45]\ttrain-mlogloss:0.32995\ttest-mlogloss:0.352098\n",
      "[46]\ttrain-mlogloss:0.328979\ttest-mlogloss:0.351735\n",
      "[47]\ttrain-mlogloss:0.327961\ttest-mlogloss:0.351248\n",
      "[48]\ttrain-mlogloss:0.326792\ttest-mlogloss:0.350668\n",
      "[49]\ttrain-mlogloss:0.325934\ttest-mlogloss:0.350116\n",
      "[50]\ttrain-mlogloss:0.32497\ttest-mlogloss:0.349307\n",
      "[51]\ttrain-mlogloss:0.323981\ttest-mlogloss:0.348794\n",
      "[52]\ttrain-mlogloss:0.322938\ttest-mlogloss:0.348455\n",
      "[53]\ttrain-mlogloss:0.322151\ttest-mlogloss:0.348063\n",
      "[54]\ttrain-mlogloss:0.321035\ttest-mlogloss:0.347364\n",
      "[55]\ttrain-mlogloss:0.320271\ttest-mlogloss:0.347001\n",
      "[56]\ttrain-mlogloss:0.319384\ttest-mlogloss:0.346473\n",
      "[57]\ttrain-mlogloss:0.318499\ttest-mlogloss:0.346043\n",
      "[58]\ttrain-mlogloss:0.317565\ttest-mlogloss:0.345643\n",
      "[59]\ttrain-mlogloss:0.316725\ttest-mlogloss:0.34522\n",
      "[60]\ttrain-mlogloss:0.315887\ttest-mlogloss:0.344853\n",
      "[61]\ttrain-mlogloss:0.315036\ttest-mlogloss:0.344399\n",
      "[62]\ttrain-mlogloss:0.314132\ttest-mlogloss:0.3439\n",
      "[63]\ttrain-mlogloss:0.313251\ttest-mlogloss:0.343622\n",
      "[64]\ttrain-mlogloss:0.312407\ttest-mlogloss:0.342988\n",
      "[65]\ttrain-mlogloss:0.311669\ttest-mlogloss:0.342539\n",
      "[66]\ttrain-mlogloss:0.310917\ttest-mlogloss:0.342071\n",
      "[67]\ttrain-mlogloss:0.310219\ttest-mlogloss:0.341697\n",
      "[68]\ttrain-mlogloss:0.309405\ttest-mlogloss:0.341255\n",
      "[69]\ttrain-mlogloss:0.308668\ttest-mlogloss:0.341162\n",
      "[70]\ttrain-mlogloss:0.307972\ttest-mlogloss:0.340689\n",
      "[71]\ttrain-mlogloss:0.307263\ttest-mlogloss:0.340296\n",
      "[72]\ttrain-mlogloss:0.306546\ttest-mlogloss:0.33977\n",
      "[73]\ttrain-mlogloss:0.305863\ttest-mlogloss:0.339459\n",
      "[74]\ttrain-mlogloss:0.305196\ttest-mlogloss:0.339296\n",
      "[75]\ttrain-mlogloss:0.304613\ttest-mlogloss:0.338948\n",
      "[76]\ttrain-mlogloss:0.303832\ttest-mlogloss:0.338724\n",
      "[77]\ttrain-mlogloss:0.303099\ttest-mlogloss:0.338351\n",
      "[78]\ttrain-mlogloss:0.302467\ttest-mlogloss:0.338277\n",
      "[79]\ttrain-mlogloss:0.301813\ttest-mlogloss:0.338105\n",
      "[80]\ttrain-mlogloss:0.301214\ttest-mlogloss:0.337928\n",
      "[81]\ttrain-mlogloss:0.300542\ttest-mlogloss:0.337662\n",
      "[82]\ttrain-mlogloss:0.29984\ttest-mlogloss:0.337262\n",
      "[83]\ttrain-mlogloss:0.299243\ttest-mlogloss:0.337131\n",
      "[84]\ttrain-mlogloss:0.298685\ttest-mlogloss:0.336775\n",
      "[85]\ttrain-mlogloss:0.298188\ttest-mlogloss:0.336529\n",
      "[86]\ttrain-mlogloss:0.29755\ttest-mlogloss:0.336461\n",
      "[87]\ttrain-mlogloss:0.296991\ttest-mlogloss:0.336163\n",
      "[88]\ttrain-mlogloss:0.296451\ttest-mlogloss:0.3359\n",
      "[89]\ttrain-mlogloss:0.29588\ttest-mlogloss:0.335625\n",
      "[90]\ttrain-mlogloss:0.295331\ttest-mlogloss:0.335585\n",
      "[91]\ttrain-mlogloss:0.294838\ttest-mlogloss:0.335327\n",
      "[92]\ttrain-mlogloss:0.294171\ttest-mlogloss:0.335183\n",
      "[93]\ttrain-mlogloss:0.293675\ttest-mlogloss:0.334889\n",
      "[94]\ttrain-mlogloss:0.293175\ttest-mlogloss:0.334684\n",
      "[95]\ttrain-mlogloss:0.292642\ttest-mlogloss:0.334349\n",
      "[96]\ttrain-mlogloss:0.29203\ttest-mlogloss:0.334126\n",
      "[97]\ttrain-mlogloss:0.291632\ttest-mlogloss:0.334055\n",
      "[98]\ttrain-mlogloss:0.291122\ttest-mlogloss:0.333976\n",
      "[99]\ttrain-mlogloss:0.290615\ttest-mlogloss:0.333779\n",
      "[100]\ttrain-mlogloss:0.290095\ttest-mlogloss:0.33373\n",
      "[101]\ttrain-mlogloss:0.289514\ttest-mlogloss:0.333451\n",
      "[102]\ttrain-mlogloss:0.289036\ttest-mlogloss:0.333333\n",
      "[103]\ttrain-mlogloss:0.288483\ttest-mlogloss:0.333153\n",
      "[104]\ttrain-mlogloss:0.287991\ttest-mlogloss:0.332959\n",
      "[105]\ttrain-mlogloss:0.287466\ttest-mlogloss:0.332994\n",
      "[106]\ttrain-mlogloss:0.286923\ttest-mlogloss:0.333016\n",
      "[107]\ttrain-mlogloss:0.286481\ttest-mlogloss:0.332899\n",
      "[108]\ttrain-mlogloss:0.285903\ttest-mlogloss:0.332885\n",
      "[109]\ttrain-mlogloss:0.285424\ttest-mlogloss:0.332814\n",
      "[110]\ttrain-mlogloss:0.285073\ttest-mlogloss:0.332754\n",
      "[111]\ttrain-mlogloss:0.284666\ttest-mlogloss:0.332611\n",
      "[112]\ttrain-mlogloss:0.284271\ttest-mlogloss:0.33245\n",
      "[113]\ttrain-mlogloss:0.2838\ttest-mlogloss:0.3324\n",
      "[114]\ttrain-mlogloss:0.283293\ttest-mlogloss:0.332437\n",
      "[115]\ttrain-mlogloss:0.282801\ttest-mlogloss:0.332153\n",
      "[116]\ttrain-mlogloss:0.282344\ttest-mlogloss:0.33197\n",
      "[117]\ttrain-mlogloss:0.281915\ttest-mlogloss:0.331695\n",
      "[118]\ttrain-mlogloss:0.281565\ttest-mlogloss:0.331588\n",
      "[119]\ttrain-mlogloss:0.281037\ttest-mlogloss:0.331676\n",
      "[120]\ttrain-mlogloss:0.280565\ttest-mlogloss:0.331632\n",
      "[121]\ttrain-mlogloss:0.280128\ttest-mlogloss:0.331713\n",
      "[122]\ttrain-mlogloss:0.279776\ttest-mlogloss:0.331621\n",
      "[123]\ttrain-mlogloss:0.279406\ttest-mlogloss:0.331469\n",
      "[124]\ttrain-mlogloss:0.278843\ttest-mlogloss:0.331355\n",
      "[125]\ttrain-mlogloss:0.27852\ttest-mlogloss:0.331326\n",
      "[126]\ttrain-mlogloss:0.278102\ttest-mlogloss:0.331335\n",
      "[127]\ttrain-mlogloss:0.27768\ttest-mlogloss:0.331283\n",
      "[128]\ttrain-mlogloss:0.277158\ttest-mlogloss:0.331035\n",
      "[129]\ttrain-mlogloss:0.276737\ttest-mlogloss:0.330893\n",
      "[130]\ttrain-mlogloss:0.27621\ttest-mlogloss:0.33074\n",
      "[131]\ttrain-mlogloss:0.275768\ttest-mlogloss:0.330702\n",
      "[132]\ttrain-mlogloss:0.275389\ttest-mlogloss:0.330541\n",
      "[133]\ttrain-mlogloss:0.27503\ttest-mlogloss:0.330571\n",
      "[134]\ttrain-mlogloss:0.274533\ttest-mlogloss:0.330276\n",
      "[135]\ttrain-mlogloss:0.274213\ttest-mlogloss:0.330214\n",
      "[136]\ttrain-mlogloss:0.273738\ttest-mlogloss:0.330113\n",
      "[137]\ttrain-mlogloss:0.273289\ttest-mlogloss:0.330081\n",
      "[138]\ttrain-mlogloss:0.272957\ttest-mlogloss:0.329921\n",
      "[139]\ttrain-mlogloss:0.2725\ttest-mlogloss:0.329912\n",
      "[140]\ttrain-mlogloss:0.272068\ttest-mlogloss:0.329934\n",
      "[141]\ttrain-mlogloss:0.271644\ttest-mlogloss:0.329863\n",
      "[142]\ttrain-mlogloss:0.271257\ttest-mlogloss:0.32977\n",
      "[143]\ttrain-mlogloss:0.270945\ttest-mlogloss:0.329667\n",
      "[144]\ttrain-mlogloss:0.270508\ttest-mlogloss:0.329633\n",
      "[145]\ttrain-mlogloss:0.270119\ttest-mlogloss:0.329617\n",
      "[146]\ttrain-mlogloss:0.26968\ttest-mlogloss:0.329628\n",
      "[147]\ttrain-mlogloss:0.269274\ttest-mlogloss:0.329603\n",
      "[148]\ttrain-mlogloss:0.268927\ttest-mlogloss:0.329643\n",
      "[149]\ttrain-mlogloss:0.268572\ttest-mlogloss:0.329489\n",
      "[150]\ttrain-mlogloss:0.268271\ttest-mlogloss:0.329512\n",
      "[151]\ttrain-mlogloss:0.267884\ttest-mlogloss:0.329436\n",
      "[152]\ttrain-mlogloss:0.267516\ttest-mlogloss:0.329264\n",
      "[153]\ttrain-mlogloss:0.267149\ttest-mlogloss:0.329254\n",
      "[154]\ttrain-mlogloss:0.266936\ttest-mlogloss:0.329101\n",
      "[155]\ttrain-mlogloss:0.266547\ttest-mlogloss:0.329063\n",
      "[156]\ttrain-mlogloss:0.266197\ttest-mlogloss:0.328999\n",
      "[157]\ttrain-mlogloss:0.265871\ttest-mlogloss:0.329015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158]\ttrain-mlogloss:0.265446\ttest-mlogloss:0.328779\n",
      "[159]\ttrain-mlogloss:0.265132\ttest-mlogloss:0.328772\n",
      "[160]\ttrain-mlogloss:0.264843\ttest-mlogloss:0.328665\n",
      "[161]\ttrain-mlogloss:0.264411\ttest-mlogloss:0.328436\n",
      "[162]\ttrain-mlogloss:0.264074\ttest-mlogloss:0.328456\n",
      "[163]\ttrain-mlogloss:0.263798\ttest-mlogloss:0.3284\n",
      "[164]\ttrain-mlogloss:0.263471\ttest-mlogloss:0.32849\n",
      "[165]\ttrain-mlogloss:0.263045\ttest-mlogloss:0.328378\n",
      "[166]\ttrain-mlogloss:0.262677\ttest-mlogloss:0.328241\n",
      "[167]\ttrain-mlogloss:0.262418\ttest-mlogloss:0.32808\n",
      "[168]\ttrain-mlogloss:0.262137\ttest-mlogloss:0.328082\n",
      "[169]\ttrain-mlogloss:0.261753\ttest-mlogloss:0.328064\n",
      "[170]\ttrain-mlogloss:0.261391\ttest-mlogloss:0.328045\n",
      "[171]\ttrain-mlogloss:0.261016\ttest-mlogloss:0.327993\n",
      "[172]\ttrain-mlogloss:0.260653\ttest-mlogloss:0.328087\n",
      "[173]\ttrain-mlogloss:0.260296\ttest-mlogloss:0.328146\n",
      "[174]\ttrain-mlogloss:0.259915\ttest-mlogloss:0.327958\n",
      "[175]\ttrain-mlogloss:0.259577\ttest-mlogloss:0.327984\n",
      "[176]\ttrain-mlogloss:0.259189\ttest-mlogloss:0.327867\n",
      "[177]\ttrain-mlogloss:0.258971\ttest-mlogloss:0.327856\n",
      "[178]\ttrain-mlogloss:0.258624\ttest-mlogloss:0.3278\n",
      "[179]\ttrain-mlogloss:0.25825\ttest-mlogloss:0.327791\n",
      "[180]\ttrain-mlogloss:0.257998\ttest-mlogloss:0.327843\n",
      "[181]\ttrain-mlogloss:0.257661\ttest-mlogloss:0.327867\n",
      "[182]\ttrain-mlogloss:0.25725\ttest-mlogloss:0.327886\n",
      "[183]\ttrain-mlogloss:0.256914\ttest-mlogloss:0.327776\n",
      "[184]\ttrain-mlogloss:0.256526\ttest-mlogloss:0.327707\n",
      "[185]\ttrain-mlogloss:0.25627\ttest-mlogloss:0.327596\n",
      "[186]\ttrain-mlogloss:0.255992\ttest-mlogloss:0.3275\n",
      "[187]\ttrain-mlogloss:0.255723\ttest-mlogloss:0.327518\n",
      "[188]\ttrain-mlogloss:0.255447\ttest-mlogloss:0.327426\n",
      "[189]\ttrain-mlogloss:0.25512\ttest-mlogloss:0.327393\n",
      "[190]\ttrain-mlogloss:0.254785\ttest-mlogloss:0.327477\n",
      "[191]\ttrain-mlogloss:0.254461\ttest-mlogloss:0.327302\n",
      "[192]\ttrain-mlogloss:0.254152\ttest-mlogloss:0.327363\n",
      "[193]\ttrain-mlogloss:0.253848\ttest-mlogloss:0.327308\n",
      "[194]\ttrain-mlogloss:0.253665\ttest-mlogloss:0.327261\n",
      "[195]\ttrain-mlogloss:0.253369\ttest-mlogloss:0.327325\n",
      "[196]\ttrain-mlogloss:0.253083\ttest-mlogloss:0.327311\n",
      "[197]\ttrain-mlogloss:0.25278\ttest-mlogloss:0.327234\n",
      "[198]\ttrain-mlogloss:0.252542\ttest-mlogloss:0.327323\n",
      "[199]\ttrain-mlogloss:0.252275\ttest-mlogloss:0.327421\n",
      "[200]\ttrain-mlogloss:0.252007\ttest-mlogloss:0.327425\n",
      "[201]\ttrain-mlogloss:0.251725\ttest-mlogloss:0.327359\n",
      "[202]\ttrain-mlogloss:0.251359\ttest-mlogloss:0.327498\n",
      "[203]\ttrain-mlogloss:0.251052\ttest-mlogloss:0.32738\n",
      "[204]\ttrain-mlogloss:0.250811\ttest-mlogloss:0.327394\n",
      "[205]\ttrain-mlogloss:0.250494\ttest-mlogloss:0.327439\n",
      "[206]\ttrain-mlogloss:0.250139\ttest-mlogloss:0.327391\n",
      "[207]\ttrain-mlogloss:0.249904\ttest-mlogloss:0.327314\n",
      "[208]\ttrain-mlogloss:0.249549\ttest-mlogloss:0.32725\n",
      "[209]\ttrain-mlogloss:0.249187\ttest-mlogloss:0.32733\n",
      "[210]\ttrain-mlogloss:0.248825\ttest-mlogloss:0.32716\n",
      "[211]\ttrain-mlogloss:0.248588\ttest-mlogloss:0.32721\n",
      "[212]\ttrain-mlogloss:0.248297\ttest-mlogloss:0.327119\n",
      "[213]\ttrain-mlogloss:0.247961\ttest-mlogloss:0.327077\n",
      "[214]\ttrain-mlogloss:0.247597\ttest-mlogloss:0.327113\n",
      "[215]\ttrain-mlogloss:0.247339\ttest-mlogloss:0.327023\n",
      "[216]\ttrain-mlogloss:0.247084\ttest-mlogloss:0.327008\n",
      "[217]\ttrain-mlogloss:0.246842\ttest-mlogloss:0.32703\n",
      "[218]\ttrain-mlogloss:0.246547\ttest-mlogloss:0.326936\n",
      "[219]\ttrain-mlogloss:0.246267\ttest-mlogloss:0.326912\n",
      "[220]\ttrain-mlogloss:0.246005\ttest-mlogloss:0.326921\n",
      "[221]\ttrain-mlogloss:0.245697\ttest-mlogloss:0.326963\n",
      "[222]\ttrain-mlogloss:0.245402\ttest-mlogloss:0.32687\n",
      "[223]\ttrain-mlogloss:0.245112\ttest-mlogloss:0.326815\n",
      "[224]\ttrain-mlogloss:0.244876\ttest-mlogloss:0.326887\n",
      "[225]\ttrain-mlogloss:0.244659\ttest-mlogloss:0.3268\n",
      "[226]\ttrain-mlogloss:0.244396\ttest-mlogloss:0.326752\n",
      "[227]\ttrain-mlogloss:0.24412\ttest-mlogloss:0.326835\n",
      "[228]\ttrain-mlogloss:0.243859\ttest-mlogloss:0.326798\n",
      "[229]\ttrain-mlogloss:0.243566\ttest-mlogloss:0.326811\n",
      "[230]\ttrain-mlogloss:0.243308\ttest-mlogloss:0.326896\n",
      "[231]\ttrain-mlogloss:0.243035\ttest-mlogloss:0.326888\n",
      "[232]\ttrain-mlogloss:0.24278\ttest-mlogloss:0.326864\n",
      "[233]\ttrain-mlogloss:0.242515\ttest-mlogloss:0.326939\n",
      "[234]\ttrain-mlogloss:0.242151\ttest-mlogloss:0.326862\n",
      "[235]\ttrain-mlogloss:0.241889\ttest-mlogloss:0.3269\n",
      "[236]\ttrain-mlogloss:0.241645\ttest-mlogloss:0.326766\n",
      "[237]\ttrain-mlogloss:0.241292\ttest-mlogloss:0.326598\n",
      "[238]\ttrain-mlogloss:0.240969\ttest-mlogloss:0.326669\n",
      "[239]\ttrain-mlogloss:0.240618\ttest-mlogloss:0.326738\n",
      "Test error using softprob = 0.127170582226762\n"
     ]
    }
   ],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_test).reshape(y_test.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != y_test) / y_test.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.921818\ttest-mlogloss:0.921863\n",
      "[1]\ttrain-mlogloss:0.79919\ttest-mlogloss:0.799002\n",
      "[2]\ttrain-mlogloss:0.709528\ttest-mlogloss:0.70911\n",
      "[3]\ttrain-mlogloss:0.641847\ttest-mlogloss:0.641432\n",
      "[4]\ttrain-mlogloss:0.58975\ttest-mlogloss:0.589307\n",
      "[5]\ttrain-mlogloss:0.549532\ttest-mlogloss:0.548942\n",
      "[6]\ttrain-mlogloss:0.51771\ttest-mlogloss:0.516905\n",
      "[7]\ttrain-mlogloss:0.492063\ttest-mlogloss:0.491249\n",
      "[8]\ttrain-mlogloss:0.470918\ttest-mlogloss:0.470304\n",
      "[9]\ttrain-mlogloss:0.453839\ttest-mlogloss:0.453268\n",
      "[10]\ttrain-mlogloss:0.439633\ttest-mlogloss:0.439011\n",
      "[11]\ttrain-mlogloss:0.42799\ttest-mlogloss:0.427375\n",
      "[12]\ttrain-mlogloss:0.417996\ttest-mlogloss:0.417243\n",
      "[13]\ttrain-mlogloss:0.409643\ttest-mlogloss:0.408651\n",
      "[14]\ttrain-mlogloss:0.402477\ttest-mlogloss:0.401332\n",
      "[15]\ttrain-mlogloss:0.396253\ttest-mlogloss:0.394994\n",
      "[16]\ttrain-mlogloss:0.39115\ttest-mlogloss:0.389764\n",
      "[17]\ttrain-mlogloss:0.386514\ttest-mlogloss:0.384979\n",
      "[18]\ttrain-mlogloss:0.382236\ttest-mlogloss:0.381012\n",
      "[19]\ttrain-mlogloss:0.378698\ttest-mlogloss:0.377297\n",
      "[20]\ttrain-mlogloss:0.375493\ttest-mlogloss:0.374224\n",
      "[21]\ttrain-mlogloss:0.372612\ttest-mlogloss:0.371334\n",
      "[22]\ttrain-mlogloss:0.369909\ttest-mlogloss:0.368771\n",
      "[23]\ttrain-mlogloss:0.367376\ttest-mlogloss:0.366302\n",
      "[24]\ttrain-mlogloss:0.365082\ttest-mlogloss:0.363974\n",
      "[25]\ttrain-mlogloss:0.362994\ttest-mlogloss:0.361945\n",
      "[26]\ttrain-mlogloss:0.361018\ttest-mlogloss:0.35998\n",
      "[27]\ttrain-mlogloss:0.358856\ttest-mlogloss:0.358078\n",
      "[28]\ttrain-mlogloss:0.357137\ttest-mlogloss:0.356549\n",
      "[29]\ttrain-mlogloss:0.355116\ttest-mlogloss:0.354583\n",
      "[30]\ttrain-mlogloss:0.353556\ttest-mlogloss:0.353199\n",
      "[31]\ttrain-mlogloss:0.351989\ttest-mlogloss:0.351862\n",
      "[32]\ttrain-mlogloss:0.350536\ttest-mlogloss:0.350497\n",
      "[33]\ttrain-mlogloss:0.348914\ttest-mlogloss:0.349012\n",
      "[34]\ttrain-mlogloss:0.347606\ttest-mlogloss:0.347898\n",
      "[35]\ttrain-mlogloss:0.346254\ttest-mlogloss:0.346548\n",
      "[36]\ttrain-mlogloss:0.34482\ttest-mlogloss:0.345255\n",
      "[37]\ttrain-mlogloss:0.343623\ttest-mlogloss:0.344186\n",
      "[38]\ttrain-mlogloss:0.342634\ttest-mlogloss:0.343124\n",
      "[39]\ttrain-mlogloss:0.341603\ttest-mlogloss:0.342188\n",
      "[40]\ttrain-mlogloss:0.340402\ttest-mlogloss:0.341131\n",
      "[41]\ttrain-mlogloss:0.339267\ttest-mlogloss:0.339765\n",
      "[42]\ttrain-mlogloss:0.338403\ttest-mlogloss:0.338871\n",
      "[43]\ttrain-mlogloss:0.337294\ttest-mlogloss:0.33789\n",
      "[44]\ttrain-mlogloss:0.336127\ttest-mlogloss:0.336958\n",
      "[45]\ttrain-mlogloss:0.335138\ttest-mlogloss:0.336008\n",
      "[46]\ttrain-mlogloss:0.334185\ttest-mlogloss:0.335007\n",
      "[47]\ttrain-mlogloss:0.333316\ttest-mlogloss:0.334086\n",
      "[48]\ttrain-mlogloss:0.332357\ttest-mlogloss:0.333093\n",
      "[49]\ttrain-mlogloss:0.331125\ttest-mlogloss:0.332017\n",
      "[50]\ttrain-mlogloss:0.330303\ttest-mlogloss:0.331011\n",
      "[51]\ttrain-mlogloss:0.32944\ttest-mlogloss:0.330348\n",
      "[52]\ttrain-mlogloss:0.328668\ttest-mlogloss:0.329663\n",
      "[53]\ttrain-mlogloss:0.327648\ttest-mlogloss:0.328759\n",
      "[54]\ttrain-mlogloss:0.326716\ttest-mlogloss:0.32795\n",
      "[55]\ttrain-mlogloss:0.325952\ttest-mlogloss:0.327398\n",
      "[56]\ttrain-mlogloss:0.325148\ttest-mlogloss:0.326621\n",
      "[57]\ttrain-mlogloss:0.324428\ttest-mlogloss:0.325872\n",
      "[58]\ttrain-mlogloss:0.323807\ttest-mlogloss:0.325179\n",
      "[59]\ttrain-mlogloss:0.323041\ttest-mlogloss:0.324316\n",
      "[60]\ttrain-mlogloss:0.322197\ttest-mlogloss:0.32367\n",
      "[61]\ttrain-mlogloss:0.321269\ttest-mlogloss:0.322925\n",
      "[62]\ttrain-mlogloss:0.320515\ttest-mlogloss:0.322276\n",
      "[63]\ttrain-mlogloss:0.319789\ttest-mlogloss:0.321566\n",
      "[64]\ttrain-mlogloss:0.31907\ttest-mlogloss:0.320903\n",
      "[65]\ttrain-mlogloss:0.318544\ttest-mlogloss:0.320378\n",
      "[66]\ttrain-mlogloss:0.317875\ttest-mlogloss:0.319599\n",
      "[67]\ttrain-mlogloss:0.317324\ttest-mlogloss:0.319023\n",
      "[68]\ttrain-mlogloss:0.316737\ttest-mlogloss:0.31854\n",
      "[69]\ttrain-mlogloss:0.316002\ttest-mlogloss:0.31787\n",
      "[70]\ttrain-mlogloss:0.315499\ttest-mlogloss:0.317216\n",
      "[71]\ttrain-mlogloss:0.314805\ttest-mlogloss:0.316547\n",
      "[72]\ttrain-mlogloss:0.31408\ttest-mlogloss:0.315994\n",
      "[73]\ttrain-mlogloss:0.313502\ttest-mlogloss:0.315463\n",
      "[74]\ttrain-mlogloss:0.313011\ttest-mlogloss:0.314993\n",
      "[75]\ttrain-mlogloss:0.312481\ttest-mlogloss:0.314434\n",
      "[76]\ttrain-mlogloss:0.311995\ttest-mlogloss:0.313863\n",
      "[77]\ttrain-mlogloss:0.311521\ttest-mlogloss:0.313475\n",
      "[78]\ttrain-mlogloss:0.310813\ttest-mlogloss:0.312986\n",
      "[79]\ttrain-mlogloss:0.310296\ttest-mlogloss:0.312514\n",
      "[80]\ttrain-mlogloss:0.309755\ttest-mlogloss:0.311887\n",
      "[81]\ttrain-mlogloss:0.309204\ttest-mlogloss:0.311287\n",
      "[82]\ttrain-mlogloss:0.308636\ttest-mlogloss:0.310817\n",
      "[83]\ttrain-mlogloss:0.308022\ttest-mlogloss:0.310194\n",
      "[84]\ttrain-mlogloss:0.307609\ttest-mlogloss:0.309807\n",
      "[85]\ttrain-mlogloss:0.307033\ttest-mlogloss:0.30928\n",
      "[86]\ttrain-mlogloss:0.306515\ttest-mlogloss:0.308816\n",
      "[87]\ttrain-mlogloss:0.305939\ttest-mlogloss:0.308255\n",
      "[88]\ttrain-mlogloss:0.305508\ttest-mlogloss:0.307787\n",
      "[89]\ttrain-mlogloss:0.305085\ttest-mlogloss:0.307332\n",
      "[90]\ttrain-mlogloss:0.304577\ttest-mlogloss:0.306829\n",
      "[91]\ttrain-mlogloss:0.303972\ttest-mlogloss:0.306258\n",
      "[92]\ttrain-mlogloss:0.303688\ttest-mlogloss:0.305931\n",
      "[93]\ttrain-mlogloss:0.30326\ttest-mlogloss:0.305499\n",
      "[94]\ttrain-mlogloss:0.30278\ttest-mlogloss:0.305093\n",
      "[95]\ttrain-mlogloss:0.302289\ttest-mlogloss:0.304604\n",
      "[96]\ttrain-mlogloss:0.301814\ttest-mlogloss:0.304132\n",
      "[97]\ttrain-mlogloss:0.301435\ttest-mlogloss:0.303662\n",
      "[98]\ttrain-mlogloss:0.300987\ttest-mlogloss:0.303243\n",
      "[99]\ttrain-mlogloss:0.300586\ttest-mlogloss:0.302823\n",
      "[100]\ttrain-mlogloss:0.300131\ttest-mlogloss:0.302478\n",
      "[101]\ttrain-mlogloss:0.299743\ttest-mlogloss:0.302077\n",
      "[102]\ttrain-mlogloss:0.299448\ttest-mlogloss:0.301626\n",
      "[103]\ttrain-mlogloss:0.299044\ttest-mlogloss:0.301191\n",
      "[104]\ttrain-mlogloss:0.298696\ttest-mlogloss:0.300822\n",
      "[105]\ttrain-mlogloss:0.298205\ttest-mlogloss:0.300522\n",
      "[106]\ttrain-mlogloss:0.2977\ttest-mlogloss:0.300069\n",
      "[107]\ttrain-mlogloss:0.297203\ttest-mlogloss:0.299629\n",
      "[108]\ttrain-mlogloss:0.296712\ttest-mlogloss:0.299155\n",
      "[109]\ttrain-mlogloss:0.296273\ttest-mlogloss:0.298689\n",
      "[110]\ttrain-mlogloss:0.295826\ttest-mlogloss:0.298275\n",
      "[111]\ttrain-mlogloss:0.295505\ttest-mlogloss:0.297804\n",
      "[112]\ttrain-mlogloss:0.295151\ttest-mlogloss:0.297436\n",
      "[113]\ttrain-mlogloss:0.294847\ttest-mlogloss:0.297134\n",
      "[114]\ttrain-mlogloss:0.294464\ttest-mlogloss:0.296721\n",
      "[115]\ttrain-mlogloss:0.294026\ttest-mlogloss:0.296294\n",
      "[116]\ttrain-mlogloss:0.293718\ttest-mlogloss:0.295997\n",
      "[117]\ttrain-mlogloss:0.293348\ttest-mlogloss:0.295569\n",
      "[118]\ttrain-mlogloss:0.292979\ttest-mlogloss:0.295199\n",
      "[119]\ttrain-mlogloss:0.292677\ttest-mlogloss:0.294761\n",
      "[120]\ttrain-mlogloss:0.292308\ttest-mlogloss:0.294433\n",
      "[121]\ttrain-mlogloss:0.291928\ttest-mlogloss:0.294074\n",
      "[122]\ttrain-mlogloss:0.29142\ttest-mlogloss:0.293786\n",
      "[123]\ttrain-mlogloss:0.291006\ttest-mlogloss:0.293499\n",
      "[124]\ttrain-mlogloss:0.290676\ttest-mlogloss:0.293186\n",
      "[125]\ttrain-mlogloss:0.290294\ttest-mlogloss:0.292856\n",
      "[126]\ttrain-mlogloss:0.289798\ttest-mlogloss:0.292401\n",
      "[127]\ttrain-mlogloss:0.289454\ttest-mlogloss:0.292164\n",
      "[128]\ttrain-mlogloss:0.289098\ttest-mlogloss:0.291799\n",
      "[129]\ttrain-mlogloss:0.288628\ttest-mlogloss:0.291522\n",
      "[130]\ttrain-mlogloss:0.288344\ttest-mlogloss:0.291293\n",
      "[131]\ttrain-mlogloss:0.288145\ttest-mlogloss:0.291025\n",
      "[132]\ttrain-mlogloss:0.287758\ttest-mlogloss:0.290768\n",
      "[133]\ttrain-mlogloss:0.287442\ttest-mlogloss:0.290415\n",
      "[134]\ttrain-mlogloss:0.286979\ttest-mlogloss:0.29012\n",
      "[135]\ttrain-mlogloss:0.286613\ttest-mlogloss:0.289832\n",
      "[136]\ttrain-mlogloss:0.286278\ttest-mlogloss:0.289528\n",
      "[137]\ttrain-mlogloss:0.286013\ttest-mlogloss:0.289211\n",
      "[138]\ttrain-mlogloss:0.285662\ttest-mlogloss:0.288864\n",
      "[139]\ttrain-mlogloss:0.285398\ttest-mlogloss:0.288574\n",
      "[140]\ttrain-mlogloss:0.284926\ttest-mlogloss:0.288271\n",
      "[141]\ttrain-mlogloss:0.284568\ttest-mlogloss:0.287912\n",
      "[142]\ttrain-mlogloss:0.284232\ttest-mlogloss:0.2876\n",
      "[143]\ttrain-mlogloss:0.283976\ttest-mlogloss:0.287311\n",
      "[144]\ttrain-mlogloss:0.283672\ttest-mlogloss:0.287003\n",
      "[145]\ttrain-mlogloss:0.283319\ttest-mlogloss:0.286766\n",
      "[146]\ttrain-mlogloss:0.282985\ttest-mlogloss:0.286459\n",
      "[147]\ttrain-mlogloss:0.282751\ttest-mlogloss:0.286216\n",
      "[148]\ttrain-mlogloss:0.282512\ttest-mlogloss:0.285879\n",
      "[149]\ttrain-mlogloss:0.282166\ttest-mlogloss:0.285602\n",
      "[150]\ttrain-mlogloss:0.281832\ttest-mlogloss:0.285317\n",
      "[151]\ttrain-mlogloss:0.281506\ttest-mlogloss:0.284959\n",
      "[152]\ttrain-mlogloss:0.281124\ttest-mlogloss:0.284548\n",
      "[153]\ttrain-mlogloss:0.280812\ttest-mlogloss:0.284213\n",
      "[154]\ttrain-mlogloss:0.280542\ttest-mlogloss:0.283832\n",
      "[155]\ttrain-mlogloss:0.280287\ttest-mlogloss:0.283443\n",
      "[156]\ttrain-mlogloss:0.279878\ttest-mlogloss:0.28305\n",
      "[157]\ttrain-mlogloss:0.279582\ttest-mlogloss:0.282649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158]\ttrain-mlogloss:0.279266\ttest-mlogloss:0.282387\n",
      "[159]\ttrain-mlogloss:0.278998\ttest-mlogloss:0.282045\n",
      "[160]\ttrain-mlogloss:0.2786\ttest-mlogloss:0.281901\n",
      "[161]\ttrain-mlogloss:0.2782\ttest-mlogloss:0.281586\n",
      "[162]\ttrain-mlogloss:0.277894\ttest-mlogloss:0.281383\n",
      "[163]\ttrain-mlogloss:0.277572\ttest-mlogloss:0.281054\n",
      "[164]\ttrain-mlogloss:0.277269\ttest-mlogloss:0.280807\n",
      "[165]\ttrain-mlogloss:0.27697\ttest-mlogloss:0.28057\n",
      "[166]\ttrain-mlogloss:0.27671\ttest-mlogloss:0.280239\n",
      "[167]\ttrain-mlogloss:0.276429\ttest-mlogloss:0.279917\n",
      "[168]\ttrain-mlogloss:0.276223\ttest-mlogloss:0.279695\n",
      "[169]\ttrain-mlogloss:0.275988\ttest-mlogloss:0.279425\n",
      "[170]\ttrain-mlogloss:0.275705\ttest-mlogloss:0.279188\n",
      "[171]\ttrain-mlogloss:0.275471\ttest-mlogloss:0.278901\n",
      "[172]\ttrain-mlogloss:0.275226\ttest-mlogloss:0.278651\n",
      "[173]\ttrain-mlogloss:0.274851\ttest-mlogloss:0.278315\n",
      "[174]\ttrain-mlogloss:0.274599\ttest-mlogloss:0.278183\n",
      "[175]\ttrain-mlogloss:0.274324\ttest-mlogloss:0.277826\n",
      "[176]\ttrain-mlogloss:0.274049\ttest-mlogloss:0.277601\n",
      "[177]\ttrain-mlogloss:0.273874\ttest-mlogloss:0.277299\n",
      "[178]\ttrain-mlogloss:0.273641\ttest-mlogloss:0.277148\n",
      "[179]\ttrain-mlogloss:0.273316\ttest-mlogloss:0.276863\n",
      "[180]\ttrain-mlogloss:0.273035\ttest-mlogloss:0.276679\n",
      "[181]\ttrain-mlogloss:0.272708\ttest-mlogloss:0.276438\n",
      "[182]\ttrain-mlogloss:0.272427\ttest-mlogloss:0.276254\n",
      "[183]\ttrain-mlogloss:0.272229\ttest-mlogloss:0.275909\n",
      "[184]\ttrain-mlogloss:0.271986\ttest-mlogloss:0.275638\n",
      "[185]\ttrain-mlogloss:0.271684\ttest-mlogloss:0.27548\n",
      "[186]\ttrain-mlogloss:0.271423\ttest-mlogloss:0.275278\n",
      "[187]\ttrain-mlogloss:0.271167\ttest-mlogloss:0.274964\n",
      "[188]\ttrain-mlogloss:0.270843\ttest-mlogloss:0.27464\n",
      "[189]\ttrain-mlogloss:0.270529\ttest-mlogloss:0.274342\n",
      "[190]\ttrain-mlogloss:0.270303\ttest-mlogloss:0.274179\n",
      "[191]\ttrain-mlogloss:0.270063\ttest-mlogloss:0.273934\n",
      "[192]\ttrain-mlogloss:0.269926\ttest-mlogloss:0.273727\n",
      "[193]\ttrain-mlogloss:0.269742\ttest-mlogloss:0.273523\n",
      "[194]\ttrain-mlogloss:0.269498\ttest-mlogloss:0.273316\n",
      "[195]\ttrain-mlogloss:0.269218\ttest-mlogloss:0.273073\n",
      "[196]\ttrain-mlogloss:0.268853\ttest-mlogloss:0.272781\n",
      "[197]\ttrain-mlogloss:0.268591\ttest-mlogloss:0.272604\n",
      "[198]\ttrain-mlogloss:0.268296\ttest-mlogloss:0.272256\n",
      "[199]\ttrain-mlogloss:0.268018\ttest-mlogloss:0.271975\n",
      "[200]\ttrain-mlogloss:0.267761\ttest-mlogloss:0.271695\n",
      "[201]\ttrain-mlogloss:0.267493\ttest-mlogloss:0.271518\n",
      "[202]\ttrain-mlogloss:0.26724\ttest-mlogloss:0.27135\n",
      "[203]\ttrain-mlogloss:0.266955\ttest-mlogloss:0.271144\n",
      "[204]\ttrain-mlogloss:0.266617\ttest-mlogloss:0.270937\n",
      "[205]\ttrain-mlogloss:0.266415\ttest-mlogloss:0.270653\n",
      "[206]\ttrain-mlogloss:0.266164\ttest-mlogloss:0.270356\n",
      "[207]\ttrain-mlogloss:0.265877\ttest-mlogloss:0.270142\n",
      "[208]\ttrain-mlogloss:0.26559\ttest-mlogloss:0.269795\n",
      "[209]\ttrain-mlogloss:0.265366\ttest-mlogloss:0.26958\n",
      "[210]\ttrain-mlogloss:0.265144\ttest-mlogloss:0.269433\n",
      "[211]\ttrain-mlogloss:0.264924\ttest-mlogloss:0.269161\n",
      "[212]\ttrain-mlogloss:0.264781\ttest-mlogloss:0.26892\n",
      "[213]\ttrain-mlogloss:0.264589\ttest-mlogloss:0.26878\n",
      "[214]\ttrain-mlogloss:0.26438\ttest-mlogloss:0.268443\n",
      "[215]\ttrain-mlogloss:0.264144\ttest-mlogloss:0.268138\n",
      "[216]\ttrain-mlogloss:0.26385\ttest-mlogloss:0.267965\n",
      "[217]\ttrain-mlogloss:0.263633\ttest-mlogloss:0.267714\n",
      "[218]\ttrain-mlogloss:0.263358\ttest-mlogloss:0.267496\n",
      "[219]\ttrain-mlogloss:0.26318\ttest-mlogloss:0.267379\n",
      "[220]\ttrain-mlogloss:0.262998\ttest-mlogloss:0.267214\n",
      "[221]\ttrain-mlogloss:0.262768\ttest-mlogloss:0.266977\n",
      "[222]\ttrain-mlogloss:0.262533\ttest-mlogloss:0.266778\n",
      "[223]\ttrain-mlogloss:0.262334\ttest-mlogloss:0.266602\n",
      "[224]\ttrain-mlogloss:0.262145\ttest-mlogloss:0.266367\n",
      "[225]\ttrain-mlogloss:0.261922\ttest-mlogloss:0.266214\n",
      "[226]\ttrain-mlogloss:0.261745\ttest-mlogloss:0.265909\n",
      "[227]\ttrain-mlogloss:0.261502\ttest-mlogloss:0.265726\n",
      "[228]\ttrain-mlogloss:0.261201\ttest-mlogloss:0.265537\n",
      "[229]\ttrain-mlogloss:0.260954\ttest-mlogloss:0.265407\n",
      "[230]\ttrain-mlogloss:0.260719\ttest-mlogloss:0.265125\n",
      "[231]\ttrain-mlogloss:0.260441\ttest-mlogloss:0.265007\n",
      "[232]\ttrain-mlogloss:0.260182\ttest-mlogloss:0.264759\n",
      "[233]\ttrain-mlogloss:0.260001\ttest-mlogloss:0.26452\n",
      "[234]\ttrain-mlogloss:0.259817\ttest-mlogloss:0.264328\n",
      "[235]\ttrain-mlogloss:0.259539\ttest-mlogloss:0.264045\n",
      "[236]\ttrain-mlogloss:0.259304\ttest-mlogloss:0.263828\n",
      "[237]\ttrain-mlogloss:0.259115\ttest-mlogloss:0.263634\n",
      "[238]\ttrain-mlogloss:0.258806\ttest-mlogloss:0.26332\n",
      "[239]\ttrain-mlogloss:0.258475\ttest-mlogloss:0.263093\n",
      "Test error using softprob = 0.09944328106644874\n"
     ]
    }
   ],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_t, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_t).reshape(Y.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != Y) / Y.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_43</th>\n",
       "      <th>svd_word_44</th>\n",
       "      <th>svd_word_45</th>\n",
       "      <th>svd_word_46</th>\n",
       "      <th>svd_word_47</th>\n",
       "      <th>svd_word_48</th>\n",
       "      <th>svd_word_49</th>\n",
       "      <th>nb_cvec_eap</th>\n",
       "      <th>nb_cvec_hpl</th>\n",
       "      <th>nb_cvec_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006320</td>\n",
       "      <td>0.019999</td>\n",
       "      <td>-0.006687</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>-0.046846</td>\n",
       "      <td>-0.042702</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.978387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>-0.000610</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>-0.001725</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>-0.006745</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007646</td>\n",
       "      <td>-0.004253</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>-0.012464</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>0.217325</td>\n",
       "      <td>0.782527</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7     ...       svd_word_43  svd_word_44  svd_word_45  \\\n",
       "0    0.003240     ...         -0.006320     0.019999    -0.006687   \n",
       "1   -0.000215     ...          0.003998    -0.000610     0.001042   \n",
       "2   -0.004598     ...          0.007646    -0.004253     0.031135   \n",
       "\n",
       "   svd_word_46  svd_word_47  svd_word_48  svd_word_49  nb_cvec_eap  \\\n",
       "0     0.008188    -0.046846    -0.042702     0.000003     0.021018   \n",
       "1    -0.000802    -0.001725     0.004586    -0.006745     0.999985   \n",
       "2     0.000088    -0.001402    -0.012464     0.007291     0.217325   \n",
       "\n",
       "   nb_cvec_hpl  nb_cvec_mws  \n",
       "0     0.000595     0.978387  \n",
       "1     0.000009     0.000006  \n",
       "2     0.782527     0.000148  \n",
       "\n",
       "[3 rows x 55 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_45</th>\n",
       "      <th>svd_word_46</th>\n",
       "      <th>svd_word_47</th>\n",
       "      <th>svd_word_48</th>\n",
       "      <th>svd_word_49</th>\n",
       "      <th>nb_cvec_eap</th>\n",
       "      <th>nb_cvec_hpl</th>\n",
       "      <th>nb_cvec_mws</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006687</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>-0.046846</td>\n",
       "      <td>-0.042702</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.978387</td>\n",
       "      <td>[still, i, urg, leav, ireland, inquietud, impa...</td>\n",
       "      <td>still i urg leav ireland inquietud impati fath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>-0.001725</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>-0.006745</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>[if, fire, want, fan, could, readili, fan, new...</td>\n",
       "      <td>if fire want fan could readili fan newspap gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>-0.012464</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>0.217325</td>\n",
       "      <td>0.782527</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>[and, broken, frail, door, found, two, clean, ...</td>\n",
       "      <td>and broken frail door found two clean pick hum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7                        ...                          svd_word_45  \\\n",
       "0    0.003240                        ...                            -0.006687   \n",
       "1   -0.000215                        ...                             0.001042   \n",
       "2   -0.004598                        ...                             0.031135   \n",
       "\n",
       "   svd_word_46  svd_word_47  svd_word_48  svd_word_49  nb_cvec_eap  \\\n",
       "0     0.008188    -0.046846    -0.042702     0.000003     0.021018   \n",
       "1    -0.000802    -0.001725     0.004586    -0.006745     0.999985   \n",
       "2     0.000088    -0.001402    -0.012464     0.007291     0.217325   \n",
       "\n",
       "   nb_cvec_hpl  nb_cvec_mws  \\\n",
       "0     0.000595     0.978387   \n",
       "1     0.000009     0.000006   \n",
       "2     0.782527     0.000148   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [still, i, urg, leav, ireland, inquietud, impa...   \n",
       "1  [if, fire, want, fan, could, readili, fan, new...   \n",
       "2  [and, broken, frail, door, found, two, clean, ...   \n",
       "\n",
       "                                 cleaned_text_string  \n",
       "0  still i urg leav ireland inquietud impati fath...  \n",
       "1  if fire want fan could readili fan newspap gov...  \n",
       "2  and broken frail door found two clean pick hum...  \n",
       "\n",
       "[3 rows x 57 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['cleaned_text'] = df_test.text.apply(tokenize_stem)\n",
    "df_test['cleaned_text_string'] = df_test.cleaned_text.apply(' '.join)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.759282</td>\n",
       "      <td>-0.576076</td>\n",
       "      <td>-0.398607</td>\n",
       "      <td>-3.074733</td>\n",
       "      <td>-2.463161</td>\n",
       "      <td>-2.642718</td>\n",
       "      <td>-4.210669</td>\n",
       "      <td>-1.410656</td>\n",
       "      <td>1.367103</td>\n",
       "      <td>4.731812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.596158</td>\n",
       "      <td>-1.332668</td>\n",
       "      <td>-1.035677</td>\n",
       "      <td>-7.674954</td>\n",
       "      <td>-4.623433</td>\n",
       "      <td>-7.287557</td>\n",
       "      <td>-9.342011</td>\n",
       "      <td>-4.080431</td>\n",
       "      <td>3.039795</td>\n",
       "      <td>10.461497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008800</td>\n",
       "      <td>-0.943616</td>\n",
       "      <td>-0.712211</td>\n",
       "      <td>-5.862047</td>\n",
       "      <td>-1.784481</td>\n",
       "      <td>-6.222224</td>\n",
       "      <td>-5.740798</td>\n",
       "      <td>-3.634120</td>\n",
       "      <td>1.837138</td>\n",
       "      <td>6.516752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7       ...        w2v_feature_90  w2v_feature_91  w2v_feature_92  \\\n",
       "0    0.003240       ...             -1.759282       -0.576076       -0.398607   \n",
       "1   -0.000215       ...             -2.596158       -1.332668       -1.035677   \n",
       "2   -0.004598       ...             -0.008800       -0.943616       -0.712211   \n",
       "\n",
       "   w2v_feature_93  w2v_feature_94  w2v_feature_95  w2v_feature_96  \\\n",
       "0       -3.074733       -2.463161       -2.642718       -4.210669   \n",
       "1       -7.674954       -4.623433       -7.287557       -9.342011   \n",
       "2       -5.862047       -1.784481       -6.222224       -5.740798   \n",
       "\n",
       "   w2v_feature_97  w2v_feature_98  w2v_feature_99  \n",
       "0       -1.410656        1.367103        4.731812  \n",
       "1       -4.080431        3.039795       10.461497  \n",
       "2       -3.634120        1.837138        6.516752  \n",
       "\n",
       "[3 rows x 247 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['length']=df_test['cleaned_text_string'].apply(len)\n",
    "df_test[\"num_words\"] = df_test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_test[\"num_unique_words\"] = df_test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_test[\"num_punctuations\"] =df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test[\"num_words_upper\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_test[\"num_words_title\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_test[\"mean_word_len\"] = df_test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test[\"num_stopwords\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_test['lexical_diversity'] = df_test.text.apply(lexical_diversity)\n",
    "df_test['w2v_array'] = df_test.cleaned_text.apply(sum_up_word2vec_array)\n",
    "count_topwords(df_test)\n",
    "create_w2v_columns(df_test) \n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.074733</td>\n",
       "      <td>-2.463161</td>\n",
       "      <td>-2.642718</td>\n",
       "      <td>-4.210669</td>\n",
       "      <td>-1.410656</td>\n",
       "      <td>1.367103</td>\n",
       "      <td>4.731812</td>\n",
       "      <td>0.071473</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>0.026541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.674954</td>\n",
       "      <td>-4.623433</td>\n",
       "      <td>-7.287557</td>\n",
       "      <td>-9.342011</td>\n",
       "      <td>-4.080431</td>\n",
       "      <td>3.039795</td>\n",
       "      <td>10.461497</td>\n",
       "      <td>0.035932</td>\n",
       "      <td>0.062884</td>\n",
       "      <td>0.039306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.862047</td>\n",
       "      <td>-1.784481</td>\n",
       "      <td>-6.222224</td>\n",
       "      <td>-5.740798</td>\n",
       "      <td>-3.634120</td>\n",
       "      <td>1.837138</td>\n",
       "      <td>6.516752</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.057723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7    ...      w2v_feature_93  w2v_feature_94  w2v_feature_95  \\\n",
       "0    0.003240    ...           -3.074733       -2.463161       -2.642718   \n",
       "1   -0.000215    ...           -7.674954       -4.623433       -7.287557   \n",
       "2   -0.004598    ...           -5.862047       -1.784481       -6.222224   \n",
       "\n",
       "   w2v_feature_96  w2v_feature_97  w2v_feature_98  w2v_feature_99  mws_index  \\\n",
       "0       -4.210669       -1.410656        1.367103        4.731812   0.071473   \n",
       "1       -9.342011       -4.080431        3.039795       10.461497   0.035932   \n",
       "2       -5.740798       -3.634120        1.837138        6.516752   0.027486   \n",
       "\n",
       "   eap_index  hpl_index  \n",
       "0   0.036315   0.026541  \n",
       "1   0.062884   0.039306  \n",
       "2   0.055142   0.057723  \n",
       "\n",
       "[3 rows x 250 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['mws_index']=df_test['cleaned_text'].apply(ind_val_mws)/df_test['length']\n",
    "df_test['eap_index']=df_test['cleaned_text'].apply(ind_val_eap)/df_test['length']\n",
    "df_test['hpl_index']=df_test['cleaned_text'].apply(ind_val_hpl)/df_test['length']\n",
    "df_test.head(n=3)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_test['cleaned_text']\n",
    "del df_test['cleaned_text_string']\n",
    "del df_test['w2v_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author2', 'author', 'cleaned_text', 'cleaned_text_string']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=(df_train.columns.tolist())[6:]\n",
    "[item for item in df_train.columns.tolist() if item not in df_test.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.074733</td>\n",
       "      <td>-2.463161</td>\n",
       "      <td>-2.642718</td>\n",
       "      <td>-4.210669</td>\n",
       "      <td>-1.410656</td>\n",
       "      <td>1.367103</td>\n",
       "      <td>4.731812</td>\n",
       "      <td>0.071473</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>0.026541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.674954</td>\n",
       "      <td>-4.623433</td>\n",
       "      <td>-7.287557</td>\n",
       "      <td>-9.342011</td>\n",
       "      <td>-4.080431</td>\n",
       "      <td>3.039795</td>\n",
       "      <td>10.461497</td>\n",
       "      <td>0.035932</td>\n",
       "      <td>0.062884</td>\n",
       "      <td>0.039306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.862047</td>\n",
       "      <td>-1.784481</td>\n",
       "      <td>-6.222224</td>\n",
       "      <td>-5.740798</td>\n",
       "      <td>-3.634120</td>\n",
       "      <td>1.837138</td>\n",
       "      <td>6.516752</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.057723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>-0.007515</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.004020</td>\n",
       "      <td>-0.004521</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>-0.002882</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.262517</td>\n",
       "      <td>-3.616161</td>\n",
       "      <td>-7.041581</td>\n",
       "      <td>-7.981725</td>\n",
       "      <td>-3.806991</td>\n",
       "      <td>2.530908</td>\n",
       "      <td>9.024186</td>\n",
       "      <td>0.024369</td>\n",
       "      <td>0.065057</td>\n",
       "      <td>0.055736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>-0.003185</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.001152</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>-0.006110</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>-0.002149</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.902167</td>\n",
       "      <td>-1.957995</td>\n",
       "      <td>-1.297328</td>\n",
       "      <td>-2.738015</td>\n",
       "      <td>-0.694716</td>\n",
       "      <td>0.861141</td>\n",
       "      <td>3.303241</td>\n",
       "      <td>0.025615</td>\n",
       "      <td>0.070948</td>\n",
       "      <td>0.028437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "3  id27757  While I was thinking how I should possibly man...    0.013408   \n",
       "4  id04081  I am not sure to what limit his knowledge may ...    0.012565   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "3   -0.007515   -0.000154   -0.004020   -0.004521    0.002712   -0.004220   \n",
       "4   -0.003185   -0.000719   -0.001152    0.000930   -0.006110    0.001690   \n",
       "\n",
       "   svd_word_7    ...      w2v_feature_93  w2v_feature_94  w2v_feature_95  \\\n",
       "0    0.003240    ...           -3.074733       -2.463161       -2.642718   \n",
       "1   -0.000215    ...           -7.674954       -4.623433       -7.287557   \n",
       "2   -0.004598    ...           -5.862047       -1.784481       -6.222224   \n",
       "3   -0.002882    ...           -7.262517       -3.616161       -7.041581   \n",
       "4   -0.002149    ...           -1.902167       -1.957995       -1.297328   \n",
       "\n",
       "   w2v_feature_96  w2v_feature_97  w2v_feature_98  w2v_feature_99  mws_index  \\\n",
       "0       -4.210669       -1.410656        1.367103        4.731812   0.071473   \n",
       "1       -9.342011       -4.080431        3.039795       10.461497   0.035932   \n",
       "2       -5.740798       -3.634120        1.837138        6.516752   0.027486   \n",
       "3       -7.981725       -3.806991        2.530908        9.024186   0.024369   \n",
       "4       -2.738015       -0.694716        0.861141        3.303241   0.025615   \n",
       "\n",
       "   eap_index  hpl_index  \n",
       "0   0.036315   0.026541  \n",
       "1   0.062884   0.039306  \n",
       "2   0.055142   0.057723  \n",
       "3   0.065057   0.055736  \n",
       "4   0.070948   0.028437  \n",
       "\n",
       "[5 rows x 247 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'text', 'svd_word_0', 'svd_word_1', 'svd_word_2', 'svd_word_3',\n",
       "       'svd_word_4', 'svd_word_5', 'svd_word_6', 'svd_word_7',\n",
       "       ...\n",
       "       'w2v_feature_93', 'w2v_feature_94', 'w2v_feature_95', 'w2v_feature_96',\n",
       "       'w2v_feature_97', 'w2v_feature_98', 'w2v_feature_99', 'mws_index',\n",
       "       'eap_index', 'hpl_index'],\n",
       "      dtype='object', length=247)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_test=df_test[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t=ds_test[:, :]\n",
    "y_t=df_test['id'].values\n",
    "xg_t=xgb.DMatrix(x_t)\n",
    "pred_prob = bstp.predict(xg_t).reshape(y_t.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.38422842e-02   4.59687738e-03   9.61560845e-01]\n",
      " [  9.96320367e-01   2.27206061e-03   1.40760711e-03]\n",
      " [  6.60936609e-02   9.32789862e-01   1.11650804e-03]\n",
      " ..., \n",
      " [  9.05344009e-01   5.72326817e-02   3.74233350e-02]\n",
      " [  2.18897983e-02   1.98120577e-03   9.76129055e-01]\n",
      " [  1.70164816e-02   9.82668400e-01   3.15122714e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.033842</td>\n",
       "      <td>0.004597</td>\n",
       "      <td>0.961561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.996320</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.001408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.066094</td>\n",
       "      <td>0.932790</td>\n",
       "      <td>0.001117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.780370</td>\n",
       "      <td>0.216980</td>\n",
       "      <td>0.002650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.982041</td>\n",
       "      <td>0.013768</td>\n",
       "      <td>0.004191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.033842  0.004597  0.961561\n",
       "1  id24541  0.996320  0.002272  0.001408\n",
       "2  id00134  0.066094  0.932790  0.001117\n",
       "3  id27757  0.780370  0.216980  0.002650\n",
       "4  id04081  0.982041  0.013768  0.004191"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export=pd.DataFrame(pred_prob)\n",
    "export.insert(loc=0, column='id', value=y_t)\n",
    "export.columns=['id','EAP', 'HPL', 'MWS']\n",
    "export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8392"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>id23301</td>\n",
       "      <td>0.337495</td>\n",
       "      <td>0.166845</td>\n",
       "      <td>0.49566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       EAP       HPL      MWS\n",
       "6106  id23301  0.337495  0.166845  0.49566"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export[export['id']=='id23301']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "export.to_csv(path_or_buf=\"../data/export.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
