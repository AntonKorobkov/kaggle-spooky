{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tnsuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tnsuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "from string import digits\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec(df_train['cleaned_text'], size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_array_len = list(w2v.items())[0][1].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# нулей не должно быть!\n",
    "\n",
    "\n",
    "def sum_up_word2vec_array(clnd_text):\n",
    "    \n",
    "    total = None\n",
    "    \n",
    "    for word in clnd_text:\n",
    "        if word in w2v:\n",
    "            if total is None:\n",
    "                total = w2v[word]\n",
    "            else:\n",
    "                total = np.add(total, w2v[word])\n",
    "                \n",
    "    if total is None:\n",
    "        return np.zeros(w2v_array_len)\n",
    "    else:\n",
    "        return total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "# 3 столбца - id, text, author\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "def tokenize_stem(file_text):\n",
    "    #firstly let's apply nltk tokenization\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #deleting stop_words\n",
    "    stop_words = set(stopwords.words('english')).union(set(get_stop_words('english')))\n",
    "    tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "    \n",
    "    # additional stop words removal\n",
    "    \n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words('english')).union(set(get_stop_words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(file_text):\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "    except:\n",
    "        nltk.download('punkt')\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "        \n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return len(set(tokens))/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[matrix([[ 0.00054413,  0.00054413,  0.00108826, ...,  0.00054413,\n",
      "          0.        ,  0.        ]]), matrix([[ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.0013942,\n",
      "          0.0006971]]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "# вытаскиваем \"значимые\" слова\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "raw_documents_authors = ['', '', '']\n",
    "\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    if row['author'] == 'EAP':\n",
    "        raw_documents_authors[0] += row['cleaned_text_string'] + ' '\n",
    "    elif row['author'] == 'HPL':\n",
    "        raw_documents_authors[1] += row['cleaned_text_string'] + ' '\n",
    "    else:\n",
    "        raw_documents_authors[2] += row['cleaned_text_string'] + ' '\n",
    "        \n",
    "\n",
    "# удалим уникальные слова, не встречающиеся у других писателей\n",
    "\n",
    "eap_only = set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "hpl_only = set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "msh_only = set(raw_documents_authors[2].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' '))\n",
    "\n",
    "unique_words = eap_only.union(hpl_only).union(msh_only)\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word')\n",
    "idf_matrix =  tf.fit_transform(raw_documents_authors)\n",
    "feature_names = tf.get_feature_names()\n",
    "# dictionary_word = dict(zip(feature_names, idf_matrix))\n",
    "\n",
    "dense_idf = [i.todense() for i in idf_matrix]\n",
    "print(dense_idf)\n",
    "\n",
    "max_weighted_term = []\n",
    "\n",
    "eap_dense_list = dense_idf[0].tolist()[0]\n",
    "hpl_dense_list = dense_idf[1].tolist()[0]\n",
    "mws_dense_list = dense_idf[2].tolist()[0]\n",
    "\n",
    "for inum, i in enumerate(eap_dense_list):\n",
    "    max_weighted_term.append(max(hpl_dense_list[inum], mws_dense_list[inum], \n",
    "                             i))\n",
    "\n",
    "max_tf_dict = dict(zip(feature_names, max_weighted_term))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_top_words(tfidfdict, numwrd):\n",
    "\n",
    "    top_word_dict, min_value, min_key = {}, 99, ''\n",
    "    \n",
    "\n",
    "    for k, v in max_tf_dict.items():\n",
    "        # print(top_word_dict.values())\n",
    "        # print(v)\n",
    "        if k not in unique_words and k not in eng_stopwords:\n",
    "        \n",
    "            if len(top_word_dict) < numwrd:\n",
    "                top_word_dict[k] = v\n",
    "                if v <= min_value:\n",
    "                    min_key = k\n",
    "            else:\n",
    "                # print(v, min(list(top_word_dict.values())))\n",
    "                if v > min(list(top_word_dict.values())) and k not in eng_stopwords:\n",
    "\n",
    "                    min_value = min(top_word_dict.values())\n",
    "\n",
    "                    for ky, va in top_word_dict.items():\n",
    "                        if va == min_value:\n",
    "                            min_key = ky\n",
    "\n",
    "                    top_word_dict.pop(min_key)\n",
    "                    top_word_dict[k] = v\n",
    "                \n",
    "    return top_word_dict\n",
    "\n",
    "\n",
    "\n",
    "def count_topwords(target_df):\n",
    "\n",
    "    for word in high_tf_idf_words_columns:\n",
    "        \n",
    "        # TODO: костыль, нужен, когда у нас уже есть такие столбцы\n",
    "        # в датасете\n",
    "#         try:\n",
    "#             target_df = target_df.drop(word, 1)\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "        \n",
    "\n",
    "        def count_numwords(collist):\n",
    "            value = 0\n",
    "\n",
    "            for wd in collist:\n",
    "                if wd == word:\n",
    "                    value += 1\n",
    "            return value\n",
    "\n",
    "\n",
    "        target_df[word] = target_df.cleaned_text.apply(count_numwords)\n",
    "        \n",
    "\n",
    "another_top_words_dict = extract_top_words(max_tf_dict, 80)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_tf_idf_words_columns = list(another_top_words_dict.keys())\n",
    "\n",
    "# count_topwords(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  \n",
       "0  this process howev afford mean ascertain dimen...  \n",
       "1             it never occur fumbl might mere mistak  \n",
       "2  in left hand gold snuff box caper hill cut man...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['cleaned_text'] = df_train.text.apply(tokenize_stem)\n",
    "df_train['cleaned_text_string'] = df_train.cleaned_text.apply(' '.join)\n",
    "df_train.head(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test['cleaned_text'] = df_test.text.apply(tokenize_stem)\n",
    "\n",
    "\n",
    "df_train['w2v_array'] = df_train.cleaned_text.apply(sum_up_word2vec_array)\n",
    "df_test['w2v_array'] = df_test.cleaned_text.apply(sum_up_word2vec_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_copy = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19579"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_w2v_columns(target_df):\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    for index, row in target_df.iterrows():\n",
    "        counter += 1\n",
    "        \n",
    "#         if counter % 100 == 0:\n",
    "#             print('another hundred processed')\n",
    "        \n",
    "        w2v_array = row['w2v_array']\n",
    "        \n",
    "        for elnum, el in enumerate(w2v_array):\n",
    "            target_df['w2v_feature_' + str(elnum)] = el\n",
    "        \n",
    "\n",
    "# create_w2v_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_w2v_columns(df_test)\n",
    "create_w2v_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_w2v_columns(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'text', 'author', 'cleaned_text', 'cleaned_text_string'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'text', 'author', 'cleaned_text', 'cleaned_text_string'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'text', 'author', 'cleaned_text', 'cleaned_text_string',\n",
       "       'length', 'num_words', 'num_unique_words', 'num_punctuations',\n",
       "       'num_words_upper', 'num_words_title', 'mean_word_len',\n",
       "       'num_stopwords', 'mws_index', 'eap_index', 'hpl_index',\n",
       "       'lexical_diversity', 'w2v_array'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>face</th>\n",
       "      <th>night</th>\n",
       "      <th>must</th>\n",
       "      <th>said</th>\n",
       "      <th>much</th>\n",
       "      <th>may</th>\n",
       "      <th>word</th>\n",
       "      <th>saw</th>\n",
       "      <th>year</th>\n",
       "      <th>feel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper  ...   face  night  \\\n",
       "0                35                 7                2  ...      0      0   \n",
       "1                14                 1                0  ...      0      0   \n",
       "2                32                 5                0  ...      0      0   \n",
       "\n",
       "   must  said  much  may  word saw  year  feel  \n",
       "0     0     0     0    0     0   0     0     0  \n",
       "1     0     0     0    0     0   0     0     0  \n",
       "2     0     0     0    0     0   0     0     0  \n",
       "\n",
       "[3 rows x 98 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train['length']=df_train['cleaned_text_string'].apply(len)\n",
    "# df_train[\"num_words\"] = df_train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "# df_train[\"num_unique_words\"] = df_train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "# df_train[\"num_punctuations\"] =df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "# df_train[\"num_words_upper\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "# df_train[\"num_words_title\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "# df_train[\"mean_word_len\"] = df_train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "# df_train[\"num_stopwords\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "# df_train['mws_index']=df_train['cleaned_text'].apply(ind_val_mws)/df_train['length']\n",
    "# df_train['eap_index']=df_train['cleaned_text'].apply(ind_val_eap)/df_train['length']\n",
    "# df_train['hpl_index']=df_train['cleaned_text'].apply(ind_val_hpl)/df_train['length']\n",
    "# df_train['lexical_diversity'] = df_train.text.apply(lexical_diversity)\n",
    "# df_train['w2v_array'] = df_train.cleaned_text.apply(sum_up_word2vec_array)\n",
    "count_topwords(df_train)\n",
    "\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>great</th>\n",
       "      <th>year</th>\n",
       "      <th>littl</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "      <td>5635.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>92.474357</td>\n",
       "      <td>27.799645</td>\n",
       "      <td>24.437977</td>\n",
       "      <td>3.206921</td>\n",
       "      <td>0.500266</td>\n",
       "      <td>2.334694</td>\n",
       "      <td>4.625193</td>\n",
       "      <td>13.150665</td>\n",
       "      <td>0.889980</td>\n",
       "      <td>0.041526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036025</td>\n",
       "      <td>0.028749</td>\n",
       "      <td>0.024135</td>\n",
       "      <td>0.030169</td>\n",
       "      <td>0.038332</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.028394</td>\n",
       "      <td>0.027329</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.033895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>50.839074</td>\n",
       "      <td>14.123252</td>\n",
       "      <td>11.053739</td>\n",
       "      <td>2.108637</td>\n",
       "      <td>0.852313</td>\n",
       "      <td>2.041579</td>\n",
       "      <td>0.554917</td>\n",
       "      <td>6.916422</td>\n",
       "      <td>0.076026</td>\n",
       "      <td>0.200409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208825</td>\n",
       "      <td>0.171311</td>\n",
       "      <td>0.160270</td>\n",
       "      <td>0.183094</td>\n",
       "      <td>0.192013</td>\n",
       "      <td>0.159049</td>\n",
       "      <td>0.171370</td>\n",
       "      <td>0.176640</td>\n",
       "      <td>0.170821</td>\n",
       "      <td>0.183894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>57.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.258065</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>117.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.961538</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>561.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>7.833333</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            length    num_words  num_unique_words  num_punctuations  \\\n",
       "count  5635.000000  5635.000000       5635.000000       5635.000000   \n",
       "mean     92.474357    27.799645         24.437977          3.206921   \n",
       "std      50.839074    14.123252         11.053739          2.108637   \n",
       "min       7.000000     4.000000          3.000000          1.000000   \n",
       "25%      57.000000    18.000000         17.000000          2.000000   \n",
       "50%      84.000000    26.000000         23.000000          3.000000   \n",
       "75%     117.000000    35.000000         30.000000          4.000000   \n",
       "max     561.000000   147.000000        102.000000         28.000000   \n",
       "\n",
       "       num_words_upper  num_words_title  mean_word_len  num_stopwords  \\\n",
       "count      5635.000000      5635.000000    5635.000000    5635.000000   \n",
       "mean          0.500266         2.334694       4.625193      13.150665   \n",
       "std           0.852313         2.041579       0.554917       6.916422   \n",
       "min           0.000000         0.000000       2.222222       0.000000   \n",
       "25%           0.000000         1.000000       4.258065       8.000000   \n",
       "50%           0.000000         2.000000       4.600000      12.000000   \n",
       "75%           1.000000         3.000000       4.961538      17.000000   \n",
       "max          10.000000        39.000000       7.833333      63.000000   \n",
       "\n",
       "       lexical_diversity         even     ...             great         year  \\\n",
       "count        5635.000000  5635.000000     ...       5635.000000  5635.000000   \n",
       "mean            0.889980     0.041526     ...          0.036025     0.028749   \n",
       "std             0.076026     0.200409     ...          0.208825     0.171311   \n",
       "min             0.250000     0.000000     ...          0.000000     0.000000   \n",
       "25%             0.840000     0.000000     ...          0.000000     0.000000   \n",
       "50%             0.891892     0.000000     ...          0.000000     0.000000   \n",
       "75%             0.947368     0.000000     ...          0.000000     0.000000   \n",
       "max             1.000000     2.000000     ...          4.000000     2.000000   \n",
       "\n",
       "             littl        whose         came         said          see  \\\n",
       "count  5635.000000  5635.000000  5635.000000  5635.000000  5635.000000   \n",
       "mean      0.024135     0.030169     0.038332     0.024845     0.028394   \n",
       "std       0.160270     0.183094     0.192013     0.159049     0.171370   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       2.000000     2.000000     1.000000     2.000000     3.000000   \n",
       "\n",
       "              door      certain        found  \n",
       "count  5635.000000  5635.000000  5635.000000  \n",
       "mean      0.027329     0.028571     0.033895  \n",
       "std       0.176640     0.170821     0.183894  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000  \n",
       "max       4.000000     2.000000     2.000000  \n",
       "\n",
       "[8 rows x 89 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hpl=df_train[df_train['author']=='HPL']\n",
    "df_hpl.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>great</th>\n",
       "      <th>year</th>\n",
       "      <th>littl</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>80.893038</td>\n",
       "      <td>25.442405</td>\n",
       "      <td>21.894937</td>\n",
       "      <td>4.096329</td>\n",
       "      <td>0.553291</td>\n",
       "      <td>2.102405</td>\n",
       "      <td>4.644952</td>\n",
       "      <td>12.747595</td>\n",
       "      <td>0.886060</td>\n",
       "      <td>0.037468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032278</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.035063</td>\n",
       "      <td>0.011646</td>\n",
       "      <td>0.016203</td>\n",
       "      <td>0.045063</td>\n",
       "      <td>0.018481</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>0.013038</td>\n",
       "      <td>0.030253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>59.749772</td>\n",
       "      <td>18.567706</td>\n",
       "      <td>13.727397</td>\n",
       "      <td>3.573788</td>\n",
       "      <td>0.892966</td>\n",
       "      <td>2.052241</td>\n",
       "      <td>0.631340</td>\n",
       "      <td>9.619779</td>\n",
       "      <td>0.097354</td>\n",
       "      <td>0.195826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190537</td>\n",
       "      <td>0.134984</td>\n",
       "      <td>0.195948</td>\n",
       "      <td>0.119568</td>\n",
       "      <td>0.129235</td>\n",
       "      <td>0.223892</td>\n",
       "      <td>0.143784</td>\n",
       "      <td>0.153253</td>\n",
       "      <td>0.119953</td>\n",
       "      <td>0.174950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>925.000000</td>\n",
       "      <td>267.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            length    num_words  num_unique_words  num_punctuations  \\\n",
       "count  7900.000000  7900.000000       7900.000000       7900.000000   \n",
       "mean     80.893038    25.442405         21.894937          4.096329   \n",
       "std      59.749772    18.567706         13.727397          3.573788   \n",
       "min       5.000000     2.000000          2.000000          1.000000   \n",
       "25%      40.000000    12.000000         12.000000          2.000000   \n",
       "50%      65.000000    21.000000         19.000000          3.000000   \n",
       "75%     106.000000    33.000000         29.000000          5.000000   \n",
       "max     925.000000   267.000000        155.000000         71.000000   \n",
       "\n",
       "       num_words_upper  num_words_title  mean_word_len  num_stopwords  \\\n",
       "count      7900.000000      7900.000000    7900.000000    7900.000000   \n",
       "mean          0.553291         2.102405       4.644952      12.747595   \n",
       "std           0.892966         2.052241       0.631340       9.619779   \n",
       "min           0.000000         0.000000       2.000000       0.000000   \n",
       "25%           0.000000         1.000000       4.250000       6.000000   \n",
       "50%           0.000000         1.000000       4.600000      10.000000   \n",
       "75%           1.000000         2.000000       5.000000      17.000000   \n",
       "max          15.000000        43.000000      11.000000     135.000000   \n",
       "\n",
       "       lexical_diversity         even     ...             great         year  \\\n",
       "count        7900.000000  7900.000000     ...       7900.000000  7900.000000   \n",
       "mean            0.886060     0.037468     ...          0.032278     0.016203   \n",
       "std             0.097354     0.195826     ...          0.190537     0.134984   \n",
       "min             0.333333     0.000000     ...          0.000000     0.000000   \n",
       "25%             0.821429     0.000000     ...          0.000000     0.000000   \n",
       "50%             0.894737     0.000000     ...          0.000000     0.000000   \n",
       "75%             1.000000     0.000000     ...          0.000000     0.000000   \n",
       "max             1.000000     2.000000     ...          4.000000     2.000000   \n",
       "\n",
       "             littl        whose         came         said          see  \\\n",
       "count  7900.000000  7900.000000  7900.000000  7900.000000  7900.000000   \n",
       "mean      0.035063     0.011646     0.016203     0.045063     0.018481   \n",
       "std       0.195948     0.119568     0.129235     0.223892     0.143784   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       3.000000     3.000000     2.000000     6.000000     4.000000   \n",
       "\n",
       "              door      certain        found  \n",
       "count  7900.000000  7900.000000  7900.000000  \n",
       "mean      0.017722     0.013038     0.030253  \n",
       "std       0.153253     0.119953     0.174950  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000  \n",
       "max       4.000000     3.000000     2.000000  \n",
       "\n",
       "[8 rows x 89 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eap=df_train[df_train['author']=='EAP']\n",
    "df_eap.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>even</th>\n",
       "      <th>...</th>\n",
       "      <th>great</th>\n",
       "      <th>year</th>\n",
       "      <th>littl</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>85.267869</td>\n",
       "      <td>27.417273</td>\n",
       "      <td>23.544672</td>\n",
       "      <td>3.833719</td>\n",
       "      <td>0.751489</td>\n",
       "      <td>2.124255</td>\n",
       "      <td>4.598182</td>\n",
       "      <td>13.896923</td>\n",
       "      <td>0.883407</td>\n",
       "      <td>0.049305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017704</td>\n",
       "      <td>0.025645</td>\n",
       "      <td>0.019854</td>\n",
       "      <td>0.022998</td>\n",
       "      <td>0.019358</td>\n",
       "      <td>0.034414</td>\n",
       "      <td>0.024653</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.024156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>71.372940</td>\n",
       "      <td>23.134440</td>\n",
       "      <td>14.925835</td>\n",
       "      <td>2.840625</td>\n",
       "      <td>1.203636</td>\n",
       "      <td>1.759572</td>\n",
       "      <td>0.561558</td>\n",
       "      <td>12.196599</td>\n",
       "      <td>0.086804</td>\n",
       "      <td>0.225507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141565</td>\n",
       "      <td>0.166251</td>\n",
       "      <td>0.144178</td>\n",
       "      <td>0.168612</td>\n",
       "      <td>0.137791</td>\n",
       "      <td>0.184112</td>\n",
       "      <td>0.168378</td>\n",
       "      <td>0.091508</td>\n",
       "      <td>0.066693</td>\n",
       "      <td>0.158844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>73.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.560791</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>107.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.907156</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2709.000000</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            length    num_words  num_unique_words  num_punctuations  \\\n",
       "count  6044.000000  6044.000000       6044.000000       6044.000000   \n",
       "mean     85.267869    27.417273         23.544672          3.833719   \n",
       "std      71.372940    23.134440         14.925835          2.840625   \n",
       "min       3.000000     2.000000          2.000000          1.000000   \n",
       "25%      48.000000    15.000000         14.000000          2.000000   \n",
       "50%      73.000000    23.000000         21.000000          3.000000   \n",
       "75%     107.000000    34.000000         30.000000          5.000000   \n",
       "max    2709.000000   861.000000        429.000000         59.000000   \n",
       "\n",
       "       num_words_upper  num_words_title  mean_word_len  num_stopwords  \\\n",
       "count      6044.000000      6044.000000    6044.000000    6044.000000   \n",
       "mean          0.751489         2.124255       4.598182      13.896923   \n",
       "std           1.203636         1.759572       0.561558      12.196599   \n",
       "min           0.000000         0.000000       2.666667       0.000000   \n",
       "25%           0.000000         1.000000       4.250000       7.000000   \n",
       "50%           0.000000         2.000000       4.560791      12.000000   \n",
       "75%           1.000000         3.000000       4.907156      18.000000   \n",
       "max          27.000000        46.000000      10.500000     437.000000   \n",
       "\n",
       "       lexical_diversity         even     ...             great         year  \\\n",
       "count        6044.000000  6044.000000     ...       6044.000000  6044.000000   \n",
       "mean            0.883407     0.049305     ...          0.017704     0.025645   \n",
       "std             0.086804     0.225507     ...          0.141565     0.166251   \n",
       "min             0.398990     0.000000     ...          0.000000     0.000000   \n",
       "25%             0.823529     0.000000     ...          0.000000     0.000000   \n",
       "50%             0.885714     0.000000     ...          0.000000     0.000000   \n",
       "75%             0.950000     0.000000     ...          0.000000     0.000000   \n",
       "max             1.000000     3.000000     ...          4.000000     3.000000   \n",
       "\n",
       "             littl        whose         came         said          see  \\\n",
       "count  6044.000000  6044.000000  6044.000000  6044.000000  6044.000000   \n",
       "mean      0.019854     0.022998     0.019358     0.034414     0.024653   \n",
       "std       0.144178     0.168612     0.137791     0.184112     0.168378   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       2.000000     3.000000     1.000000     2.000000     4.000000   \n",
       "\n",
       "              door      certain        found  \n",
       "count  6044.000000  6044.000000  6044.000000  \n",
       "mean      0.008107     0.004467     0.024156  \n",
       "std       0.091508     0.066693     0.158844  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000  \n",
       "max       2.000000     1.000000     2.000000  \n",
       "\n",
       "[8 rows x 89 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mws=df_train[df_train['author']=='MWS']\n",
    "df_mws.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordset=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#делаю сет со всеми словами\n",
    "for i in df_train.index:\n",
    "    wordset |= set(df_train['cleaned_text'][i])\n",
    "wordlist=list(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>mws</th>\n",
       "      <th>eap</th>\n",
       "      <th>hpl</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>equit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unobtrus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ottoman</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oaken</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>harmoni</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  mws  eap  hpl  all\n",
       "0     equit    0    0    0    0\n",
       "1  unobtrus    0    0    0    0\n",
       "2   ottoman    0    0    0    0\n",
       "3     oaken    0    0    0    0\n",
       "4   harmoni    0    0    0    0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#делаю фрейм со словами\n",
    "df_word=pd.DataFrame(columns=[\"word\", \"mws\", \"eap\", \"hpl\", \"all\"])\n",
    "df_word[\"word\"]=wordlist\n",
    "df_word[\"mws\"]=0\n",
    "df_word[\"eap\"]=0\n",
    "df_word[\"hpl\"]=0\n",
    "df_word[\"all\"]=0\n",
    "df_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# как мы будем эту штуку правильнее делать (возможно это жуткий костыль), я хз\n",
    "# сначала создаем словарь где ключ - уникальное слово, а значение - его порядковый номер\n",
    "# затем создаем разреженную матрицу, которую заполняем в зависимости от порядковых номеров \n",
    "word_dict = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#делаю сет со всеми словами\n",
    "# и сразу заготовку под шапку(потом увидишь зачем)\n",
    "counter = 0\n",
    "head = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    for word in wordlist:\n",
    "        if word not in word_dict:\n",
    "            head.append(word)\n",
    "            word_dict[word] = counter\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# видоизменять колонки в pandas руками по одному значению в строке или столбце - очень плохая идея\n",
    "# колонка это numpy.ndarray, а значит при каждой итерации она будет пересоздаваться\n",
    "# что угробит производительность\n",
    "# делаем значит так. считаем где сколько и где встречались отдельные слова, затем создаем строку за строкой для \n",
    "# каждого предложения\n",
    "\n",
    "list_of_lists = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    row = [0 for i in range(len(word_dict))]\n",
    "    for word in wordlist:\n",
    "        row[word_dict[word]] += 1\n",
    "    list_of_lists.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... и для того чтобы посмотреть встречаемость того или иного слова по авторам добавим такую колонку\n",
    "\n",
    "count_frame = pd.DataFrame(list_of_lists)\n",
    "count_frame['author'] = df_train['author']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... и теперь нормальную шапку делаем\n",
    "\n",
    "count_frame.columns = head + ['author']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   this  process  howev  afford  mean  ascertain  dimens  dungeon  i  might  \\\n",
      "0     1        1      1       1     1          1       1        1  2      1   \n",
      "1     0        0      0       0     0          0       0        0  0      1   \n",
      "2     0        0      0       0     0          0       0        0  0      0   \n",
      "3     0        0      0       0     0          0       0        0  0      0   \n",
      "4     0        0      0       0     0          0       0        0  0      0   \n",
      "\n",
      "    ...    aegidus  burr  bentley  waltzer  binder  brusqueri  adriat  ancona  \\\n",
      "0   ...          0     0        0        0       0          0       0       0   \n",
      "1   ...          0     0        0        0       0          0       0       0   \n",
      "2   ...          0     0        0        0       0          0       0       0   \n",
      "3   ...          0     0        0        0       0          0       0       0   \n",
      "4   ...          0     0        0        0       0          0       0       0   \n",
      "\n",
      "   agir  author  \n",
      "0     0     EAP  \n",
      "1     0     HPL  \n",
      "2     0     EAP  \n",
      "3     0     MWS  \n",
      "4     0     HPL  \n",
      "\n",
      "[5 rows x 15230 columns]\n"
     ]
    }
   ],
   "source": [
    "print(count_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Пока объединим все, потом может быть будем использовать\n",
    "col=list(count_frame.columns)\n",
    "col[-1]='author_name'\n",
    "count_frame.columns=col\n",
    "pivot_col=pd.pivot_table(count_frame, aggfunc=np.sum, values=col, index=['author_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 14350 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  \\\n",
       "author_name                                                                  \n",
       "EAP             1      2      0       22       0     2      1     2      3   \n",
       "HPL             0      0      0       17      24     0      1     3      0   \n",
       "MWS             0      0      1        9       0     0      0     1      2   \n",
       "\n",
       "             abbrevi  ...    æmilianus  æneid  ærial  æronaut  ærostat  \\\n",
       "author_name           ...                                                \n",
       "EAP                2  ...            0      0      1        3        1   \n",
       "HPL                0  ...            2      1      0        0        0   \n",
       "MWS                0  ...            0      0      0        0        0   \n",
       "\n",
       "             æschylus  élite  émeut  οἶδα  υπνος  \n",
       "author_name                                       \n",
       "EAP                 1      1      1     0      0  \n",
       "HPL                 0      0      0     2      1  \n",
       "MWS                 0      0      0     0      0  \n",
       "\n",
       "[3 rows x 14350 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Убираем лишние слова, которые не учли раньше\n",
    "col=list(pivot_col.columns)\n",
    "col2=[string for string in col if (string[0]!='\"' and string[0]!=\"'\"\n",
    "                                  and string[0]!='.' and string[0]!='`'\n",
    "                                   and len(string)>3 and '.' not in string)]\n",
    "col=[]\n",
    "pivot_col=pivot_col[col2]\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUMA</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 14350 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  abbrevi  \\\n",
       "EAP      1      2      0       22       0     2      1     2      3        2   \n",
       "HPL      0      0      0       17      24     0      1     3      0        0   \n",
       "MWS      0      0      1        9       0     0      0     1      2        0   \n",
       "SUMA     1      2      1       48      24     2      2     6      5        2   \n",
       "\n",
       "      ...    æmilianus  æneid  ærial  æronaut  ærostat  æschylus  élite  \\\n",
       "EAP   ...            0      0      1        3        1         1      1   \n",
       "HPL   ...            2      1      0        0        0         0      0   \n",
       "MWS   ...            0      0      0        0        0         0      0   \n",
       "SUMA  ...            2      1      1        3        1         1      1   \n",
       "\n",
       "      émeut  οἶδα  υπνος  \n",
       "EAP       1     0      0  \n",
       "HPL       0     2      1  \n",
       "MWS       0     0      0  \n",
       "SUMA      1     2      1  \n",
       "\n",
       "[4 rows x 14350 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create pivot\n",
    "pivot_col=pivot_col.append(pivot_col.sum(), ignore_index=True)\n",
    "pivot_col.index=['EAP', 'HPL', 'MWS', 'SUMA']\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "      <th>summa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>73404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUMA</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>232610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 14351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  abbrevi  \\\n",
       "EAP      1      2      0       22       0     2      1     2      3        2   \n",
       "HPL      0      0      0       17      24     0      1     3      0        0   \n",
       "MWS      0      0      1        9       0     0      0     1      2        0   \n",
       "SUMA     1      2      1       48      24     2      2     6      5        2   \n",
       "\n",
       "       ...    æneid  ærial  æronaut  ærostat  æschylus  élite  émeut  οἶδα  \\\n",
       "EAP    ...        0      1        3        1         1      1      1     0   \n",
       "HPL    ...        1      0        0        0         0      0      0     2   \n",
       "MWS    ...        0      0        0        0         0      0      0     0   \n",
       "SUMA   ...        1      1        3        1         1      1      1     2   \n",
       "\n",
       "      υπνος   summa  \n",
       "EAP       0   86909  \n",
       "HPL       1   73404  \n",
       "MWS       0   72297  \n",
       "SUMA      1  232610  \n",
       "\n",
       "[4 rows x 14351 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa=[pivot_col.loc['EAP'].sum(), pivot_col.loc['HPL'].sum(), \n",
    "       pivot_col.loc['MWS'].sum(), pivot_col.loc['SUMA'].sum()]\n",
    "pivot_col['summa']=summa\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdic</th>\n",
       "      <th>aberr</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abhorr</th>\n",
       "      <th>abil</th>\n",
       "      <th>abject</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>your</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zenith</th>\n",
       "      <th>zest</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zone</th>\n",
       "      <th>summa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.373625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.315567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.310808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 6618 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandon  abash      abat  abbey     abdic     aberr     abhor    abhorr  \\\n",
       "EAP  0.458333    0.5  0.333333    0.6  0.142857  0.166667  0.058824  0.111111   \n",
       "HPL  0.354167    0.5  0.500000    0.0  0.000000  0.666667  0.235294  0.555556   \n",
       "MWS  0.187500    0.0  0.166667    0.4  0.857143  0.166667  0.705882  0.333333   \n",
       "\n",
       "         abil    abject    ...      younger  youngest      your     youth  \\\n",
       "EAP  0.789474  0.333333    ...     0.272727       0.2  0.534884  0.101562   \n",
       "HPL  0.052632  0.000000    ...     0.000000       0.4  0.069767  0.429688   \n",
       "MWS  0.157895  0.666667    ...     0.727273       0.4  0.395349  0.468750   \n",
       "\n",
       "         zeal  zenith  zest  zigzag      zone     summa  \n",
       "EAP  0.117647     0.4   0.2     0.4  0.666667  0.373625  \n",
       "HPL  0.470588     0.6   0.2     0.6  0.333333  0.315567  \n",
       "MWS  0.411765     0.0   0.6     0.0  0.000000  0.310808  \n",
       "\n",
       "[3 rows x 6618 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create probability of author text knowing that a word was used\n",
    "pivot_part=pivot_col\n",
    "pivot_part.loc['EAP']=pivot_col.loc['EAP']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['HPL']=pivot_col.loc['HPL']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['MWS']=pivot_col.loc['MWS']/pivot_col.loc['SUMA']\n",
    "pivot_part=pivot_part.loc[['EAP', 'HPL', 'MWS']]\n",
    "# Delete unique words\n",
    "pivot_part=pivot_part.loc[:, (pivot_part!=1).all(axis=0)]\n",
    "pivot_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44859813084112149"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will be easier to work this way\n",
    "eap_dict=pivot_part.loc['EAP'].to_dict()\n",
    "hpl_dict=pivot_part.loc['HPL'].to_dict()\n",
    "mws_dict=pivot_part.loc['MWS'].to_dict()\n",
    "eap_dict['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create author score \n",
    "def ind_val_eap(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=eap_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_hpl(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=hpl_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_mws(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=mws_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.074388</td>\n",
       "      <td>0.034504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.034739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper    ...      whose  \\\n",
       "0                35                 7                2    ...          0   \n",
       "1                14                 1                0    ...          0   \n",
       "2                32                 5                0    ...          0   \n",
       "\n",
       "   came  said  see  door  certain  found  mws_index  eap_index  hpl_index  \n",
       "0     0     0    0     0        0      0   0.035935   0.074388   0.034504  \n",
       "1     0     0    0     0        0      0   0.035860   0.060980   0.034739  \n",
       "2     0     0    0     0        0      0   0.026900   0.047832   0.037337  \n",
       "\n",
       "[3 rows x 97 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add index of author\n",
    "df_train['mws_index']=df_train['cleaned_text'].apply(ind_val_mws)/df_train['length']\n",
    "df_train['eap_index']=df_train['cleaned_text'].apply(ind_val_eap)/df_train['length']\n",
    "df_train['hpl_index']=df_train['cleaned_text'].apply(ind_val_hpl)/df_train['length']\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author2</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>...</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.074388</td>\n",
       "      <td>0.034504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.034739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, love, spring, as, look, windsor, terrac,...</td>\n",
       "      <td>how love spring as look windsor terrac sixteen...</td>\n",
       "      <td>144</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.033438</td>\n",
       "      <td>0.033601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[find, noth, els, even, gold, superintend, aba...</td>\n",
       "      <td>find noth els even gold superintend abandon at...</td>\n",
       "      <td>102</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036859</td>\n",
       "      <td>0.056661</td>\n",
       "      <td>0.043735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author2       id                                               text author  \\\n",
       "0        0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1        1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2        0  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3        2  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4        1  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "3  [how, love, spring, as, look, windsor, terrac,...   \n",
       "4  [find, noth, els, even, gold, superintend, aba...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "3  how love spring as look windsor terrac sixteen...     144         34   \n",
       "4  find noth els even gold superintend abandon at...     102         27   \n",
       "\n",
       "   num_unique_words  num_punctuations    ...      whose  came  said  see  \\\n",
       "0                35                 7    ...          0     0     0    0   \n",
       "1                14                 1    ...          0     0     0    0   \n",
       "2                32                 5    ...          0     0     0    0   \n",
       "3                32                 4    ...          0     0     0    0   \n",
       "4                25                 4    ...          0     0     0    0   \n",
       "\n",
       "   door  certain  found  mws_index  eap_index  hpl_index  \n",
       "0     0        0      0   0.035935   0.074388   0.034504  \n",
       "1     0        0      0   0.035860   0.060980   0.034739  \n",
       "2     0        0      0   0.026900   0.047832   0.037337  \n",
       "3     0        0      0   0.071850   0.033438   0.033601  \n",
       "4     0        0      0   0.036859   0.056661   0.043735  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform authors' names to numeric\n",
    "df_train['author']=df_train['author'].astype('category')\n",
    "df_train['author2']=df_train['author'].cat.codes\n",
    "# Create different features \n",
    "df_train.head(n=3)\n",
    "mid = df_train['author2']\n",
    "df_train.drop(labels=['author2'], axis=1,inplace = True)\n",
    "df_train.insert(0, 'author2', mid)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = df_train['author'].map(author_mapping_dict)\n",
    "train_id = df_train['id'].values\n",
    "test_id = df_test['id'].values\n",
    "cols_to_drop = ['id', 'text']\n",
    "train_X = df_train.drop(cols_to_drop+['author'], axis=1)\n",
    "test_X = df_test.drop(cols_to_drop, axis=1)\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf = tfidf_vec.fit_transform(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.842216198361\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "df_test = pd.concat([df_test, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the count vectorizer ###\n",
    "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "tfidf_vec.fit(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.450918416166\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# add the predictions as new features #\n",
    "df_train[\"nb_cvec_eap\"] = pred_train[:,0]\n",
    "df_train[\"nb_cvec_hpl\"] = pred_train[:,1]\n",
    "df_train[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "df_test[\"nb_cvec_eap\"] = pred_full_test[:,0]\n",
    "df_test[\"nb_cvec_hpl\"] = pred_full_test[:,1]\n",
    "df_test[\"nb_cvec_mws\"] = pred_full_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-b9d21a5fd81a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(x_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create data set\n",
    "ds_train=df_train.values\n",
    "X=ds_train[:, 6:]\n",
    "Y=ds_train[:, 0]\n",
    "seed=7\n",
    "test_size=0.3\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X,Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[float(j) for j in i] for i in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a float is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-f15857ea6db6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxg_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'can not initialize DMatrix from {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mset_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0minformation\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mset\u001b[0m \u001b[0minto\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_float_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mset_float_info\u001b[0;34m(self, field, data)\u001b[0m\n\u001b[1;32m    379\u001b[0m         _check_call(_LIB.XGDMatrixSetFloatInfo(self.handle,\n\u001b[1;32m    380\u001b[0m                                                \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                                                \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                                                len(data)))\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mc_array\u001b[0;34m(ctype, values)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m\"\"\"Convert a python string to c array.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a float is required"
     ]
    }
   ],
   "source": [
    "xg_train=xgb.DMatrix(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a float is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-673314d359ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxg_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mxg_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxg_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'can not initialize DMatrix from {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mset_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0minformation\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mset\u001b[0m \u001b[0minto\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_float_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mset_float_info\u001b[0;34m(self, field, data)\u001b[0m\n\u001b[1;32m    379\u001b[0m         _check_call(_LIB.XGDMatrixSetFloatInfo(self.handle,\n\u001b[1;32m    380\u001b[0m                                                \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                                                \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                                                len(data)))\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mc_array\u001b[0;34m(ctype, values)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m\"\"\"Convert a python string to c array.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a float is required"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "xg_train=xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test=xgb.DMatrix(X_test, label=y_test)\n",
    "xg_t=xgb.DMatrix(X, label=Y)\n",
    "param={}\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['eta'] = 0.3\n",
    "param['max_depth'] = 3\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 3\n",
    "param['eval_metric']= \"mlogloss\"\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 70\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# get prediction\n",
    "pred = bst.predict(xg_test)\n",
    "error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.835585\ttest-mlogloss:0.837725\n",
      "[1]\ttrain-mlogloss:0.68415\ttest-mlogloss:0.687706\n",
      "[2]\ttrain-mlogloss:0.587061\ttest-mlogloss:0.592043\n",
      "[3]\ttrain-mlogloss:0.521673\ttest-mlogloss:0.527715\n",
      "[4]\ttrain-mlogloss:0.476203\ttest-mlogloss:0.483356\n",
      "[5]\ttrain-mlogloss:0.443035\ttest-mlogloss:0.451809\n",
      "[6]\ttrain-mlogloss:0.419353\ttest-mlogloss:0.429201\n",
      "[7]\ttrain-mlogloss:0.400974\ttest-mlogloss:0.412195\n",
      "[8]\ttrain-mlogloss:0.387214\ttest-mlogloss:0.40011\n",
      "[9]\ttrain-mlogloss:0.375655\ttest-mlogloss:0.39004\n",
      "[10]\ttrain-mlogloss:0.366574\ttest-mlogloss:0.382388\n",
      "[11]\ttrain-mlogloss:0.358829\ttest-mlogloss:0.376405\n",
      "[12]\ttrain-mlogloss:0.352795\ttest-mlogloss:0.371874\n",
      "[13]\ttrain-mlogloss:0.347295\ttest-mlogloss:0.367584\n",
      "[14]\ttrain-mlogloss:0.342647\ttest-mlogloss:0.36442\n",
      "[15]\ttrain-mlogloss:0.338423\ttest-mlogloss:0.361727\n",
      "[16]\ttrain-mlogloss:0.334811\ttest-mlogloss:0.359318\n",
      "[17]\ttrain-mlogloss:0.331139\ttest-mlogloss:0.35714\n",
      "[18]\ttrain-mlogloss:0.328364\ttest-mlogloss:0.355447\n",
      "[19]\ttrain-mlogloss:0.325305\ttest-mlogloss:0.352994\n",
      "[20]\ttrain-mlogloss:0.322119\ttest-mlogloss:0.350542\n",
      "[21]\ttrain-mlogloss:0.319794\ttest-mlogloss:0.349249\n",
      "[22]\ttrain-mlogloss:0.316907\ttest-mlogloss:0.347616\n",
      "[23]\ttrain-mlogloss:0.314426\ttest-mlogloss:0.346598\n",
      "[24]\ttrain-mlogloss:0.311809\ttest-mlogloss:0.345387\n",
      "[25]\ttrain-mlogloss:0.309887\ttest-mlogloss:0.344942\n",
      "[26]\ttrain-mlogloss:0.308003\ttest-mlogloss:0.343978\n",
      "[27]\ttrain-mlogloss:0.305548\ttest-mlogloss:0.34336\n",
      "[28]\ttrain-mlogloss:0.303932\ttest-mlogloss:0.34238\n",
      "[29]\ttrain-mlogloss:0.302133\ttest-mlogloss:0.342035\n",
      "[30]\ttrain-mlogloss:0.300035\ttest-mlogloss:0.341176\n",
      "[31]\ttrain-mlogloss:0.298006\ttest-mlogloss:0.340698\n",
      "[32]\ttrain-mlogloss:0.295826\ttest-mlogloss:0.33986\n",
      "[33]\ttrain-mlogloss:0.294698\ttest-mlogloss:0.339788\n",
      "[34]\ttrain-mlogloss:0.292917\ttest-mlogloss:0.33952\n",
      "[35]\ttrain-mlogloss:0.290839\ttest-mlogloss:0.338726\n",
      "[36]\ttrain-mlogloss:0.288822\ttest-mlogloss:0.338372\n",
      "[37]\ttrain-mlogloss:0.287194\ttest-mlogloss:0.338032\n",
      "[38]\ttrain-mlogloss:0.285623\ttest-mlogloss:0.337184\n",
      "[39]\ttrain-mlogloss:0.284408\ttest-mlogloss:0.336862\n",
      "[40]\ttrain-mlogloss:0.28334\ttest-mlogloss:0.336597\n",
      "[41]\ttrain-mlogloss:0.282424\ttest-mlogloss:0.336453\n",
      "[42]\ttrain-mlogloss:0.281122\ttest-mlogloss:0.336237\n",
      "[43]\ttrain-mlogloss:0.280143\ttest-mlogloss:0.335983\n",
      "[44]\ttrain-mlogloss:0.278748\ttest-mlogloss:0.335791\n",
      "[45]\ttrain-mlogloss:0.27758\ttest-mlogloss:0.33524\n",
      "[46]\ttrain-mlogloss:0.276401\ttest-mlogloss:0.335061\n",
      "[47]\ttrain-mlogloss:0.275483\ttest-mlogloss:0.334842\n",
      "[48]\ttrain-mlogloss:0.273968\ttest-mlogloss:0.334702\n",
      "[49]\ttrain-mlogloss:0.272973\ttest-mlogloss:0.334426\n",
      "[50]\ttrain-mlogloss:0.271775\ttest-mlogloss:0.334399\n",
      "[51]\ttrain-mlogloss:0.270336\ttest-mlogloss:0.334156\n",
      "[52]\ttrain-mlogloss:0.269494\ttest-mlogloss:0.333969\n",
      "[53]\ttrain-mlogloss:0.268118\ttest-mlogloss:0.333971\n",
      "[54]\ttrain-mlogloss:0.267113\ttest-mlogloss:0.333528\n",
      "[55]\ttrain-mlogloss:0.266108\ttest-mlogloss:0.33315\n",
      "[56]\ttrain-mlogloss:0.265003\ttest-mlogloss:0.33318\n",
      "[57]\ttrain-mlogloss:0.263666\ttest-mlogloss:0.33324\n",
      "[58]\ttrain-mlogloss:0.262536\ttest-mlogloss:0.333168\n",
      "[59]\ttrain-mlogloss:0.261605\ttest-mlogloss:0.333251\n",
      "[60]\ttrain-mlogloss:0.260706\ttest-mlogloss:0.333276\n",
      "[61]\ttrain-mlogloss:0.259881\ttest-mlogloss:0.333145\n",
      "[62]\ttrain-mlogloss:0.259069\ttest-mlogloss:0.33326\n",
      "[63]\ttrain-mlogloss:0.258449\ttest-mlogloss:0.33294\n",
      "[64]\ttrain-mlogloss:0.257779\ttest-mlogloss:0.332938\n",
      "[65]\ttrain-mlogloss:0.256978\ttest-mlogloss:0.332935\n",
      "[66]\ttrain-mlogloss:0.255902\ttest-mlogloss:0.333106\n",
      "[67]\ttrain-mlogloss:0.254978\ttest-mlogloss:0.333134\n",
      "[68]\ttrain-mlogloss:0.254198\ttest-mlogloss:0.33307\n",
      "[69]\ttrain-mlogloss:0.253427\ttest-mlogloss:0.332898\n",
      "Test error using softprob = 0.13057541709227102\n"
     ]
    }
   ],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_test).reshape(y_test.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != y_test) / y_test.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.836322\ttest-mlogloss:0.835926\n",
      "[1]\ttrain-mlogloss:0.685431\ttest-mlogloss:0.684692\n",
      "[2]\ttrain-mlogloss:0.588655\ttest-mlogloss:0.587662\n",
      "[3]\ttrain-mlogloss:0.523748\ttest-mlogloss:0.522269\n",
      "[4]\ttrain-mlogloss:0.478684\ttest-mlogloss:0.476855\n",
      "[5]\ttrain-mlogloss:0.44625\ttest-mlogloss:0.444233\n",
      "[6]\ttrain-mlogloss:0.423007\ttest-mlogloss:0.42076\n",
      "[7]\ttrain-mlogloss:0.404697\ttest-mlogloss:0.402665\n",
      "[8]\ttrain-mlogloss:0.391392\ttest-mlogloss:0.389223\n",
      "[9]\ttrain-mlogloss:0.380203\ttest-mlogloss:0.378347\n",
      "[10]\ttrain-mlogloss:0.371416\ttest-mlogloss:0.369665\n",
      "[11]\ttrain-mlogloss:0.36354\ttest-mlogloss:0.362414\n",
      "[12]\ttrain-mlogloss:0.357645\ttest-mlogloss:0.356544\n",
      "[13]\ttrain-mlogloss:0.352108\ttest-mlogloss:0.351681\n",
      "[14]\ttrain-mlogloss:0.347992\ttest-mlogloss:0.347653\n",
      "[15]\ttrain-mlogloss:0.343259\ttest-mlogloss:0.343469\n",
      "[16]\ttrain-mlogloss:0.33965\ttest-mlogloss:0.340181\n",
      "[17]\ttrain-mlogloss:0.336416\ttest-mlogloss:0.33691\n",
      "[18]\ttrain-mlogloss:0.333316\ttest-mlogloss:0.334061\n",
      "[19]\ttrain-mlogloss:0.33014\ttest-mlogloss:0.331316\n",
      "[20]\ttrain-mlogloss:0.327775\ttest-mlogloss:0.328821\n",
      "[21]\ttrain-mlogloss:0.325236\ttest-mlogloss:0.326021\n",
      "[22]\ttrain-mlogloss:0.323189\ttest-mlogloss:0.324142\n",
      "[23]\ttrain-mlogloss:0.321147\ttest-mlogloss:0.322247\n",
      "[24]\ttrain-mlogloss:0.31887\ttest-mlogloss:0.319957\n",
      "[25]\ttrain-mlogloss:0.31675\ttest-mlogloss:0.317883\n",
      "[26]\ttrain-mlogloss:0.314733\ttest-mlogloss:0.315604\n",
      "[27]\ttrain-mlogloss:0.312974\ttest-mlogloss:0.313677\n",
      "[28]\ttrain-mlogloss:0.311423\ttest-mlogloss:0.311989\n",
      "[29]\ttrain-mlogloss:0.309807\ttest-mlogloss:0.310367\n",
      "[30]\ttrain-mlogloss:0.307996\ttest-mlogloss:0.308879\n",
      "[31]\ttrain-mlogloss:0.306678\ttest-mlogloss:0.307646\n",
      "[32]\ttrain-mlogloss:0.304883\ttest-mlogloss:0.306069\n",
      "[33]\ttrain-mlogloss:0.303483\ttest-mlogloss:0.305056\n",
      "[34]\ttrain-mlogloss:0.301849\ttest-mlogloss:0.303614\n",
      "[35]\ttrain-mlogloss:0.300377\ttest-mlogloss:0.301915\n",
      "[36]\ttrain-mlogloss:0.298779\ttest-mlogloss:0.300377\n",
      "[37]\ttrain-mlogloss:0.297769\ttest-mlogloss:0.299196\n",
      "[38]\ttrain-mlogloss:0.296748\ttest-mlogloss:0.298154\n",
      "[39]\ttrain-mlogloss:0.295657\ttest-mlogloss:0.297221\n",
      "[40]\ttrain-mlogloss:0.294278\ttest-mlogloss:0.29582\n",
      "[41]\ttrain-mlogloss:0.293161\ttest-mlogloss:0.294745\n",
      "[42]\ttrain-mlogloss:0.292126\ttest-mlogloss:0.293478\n",
      "[43]\ttrain-mlogloss:0.290786\ttest-mlogloss:0.292494\n",
      "[44]\ttrain-mlogloss:0.289876\ttest-mlogloss:0.291239\n",
      "[45]\ttrain-mlogloss:0.288766\ttest-mlogloss:0.290069\n",
      "[46]\ttrain-mlogloss:0.287682\ttest-mlogloss:0.289103\n",
      "[47]\ttrain-mlogloss:0.286593\ttest-mlogloss:0.287893\n",
      "[48]\ttrain-mlogloss:0.285678\ttest-mlogloss:0.287256\n",
      "[49]\ttrain-mlogloss:0.284703\ttest-mlogloss:0.286383\n",
      "[50]\ttrain-mlogloss:0.283662\ttest-mlogloss:0.285632\n",
      "[51]\ttrain-mlogloss:0.282745\ttest-mlogloss:0.285045\n",
      "[52]\ttrain-mlogloss:0.281887\ttest-mlogloss:0.284079\n",
      "[53]\ttrain-mlogloss:0.28074\ttest-mlogloss:0.283182\n",
      "[54]\ttrain-mlogloss:0.27993\ttest-mlogloss:0.282469\n",
      "[55]\ttrain-mlogloss:0.279018\ttest-mlogloss:0.28159\n",
      "[56]\ttrain-mlogloss:0.27808\ttest-mlogloss:0.280759\n",
      "[57]\ttrain-mlogloss:0.277311\ttest-mlogloss:0.280035\n",
      "[58]\ttrain-mlogloss:0.276198\ttest-mlogloss:0.279236\n",
      "[59]\ttrain-mlogloss:0.275192\ttest-mlogloss:0.278451\n",
      "[60]\ttrain-mlogloss:0.2743\ttest-mlogloss:0.277694\n",
      "[61]\ttrain-mlogloss:0.27378\ttest-mlogloss:0.277153\n",
      "[62]\ttrain-mlogloss:0.273109\ttest-mlogloss:0.276637\n",
      "[63]\ttrain-mlogloss:0.27235\ttest-mlogloss:0.27601\n",
      "[64]\ttrain-mlogloss:0.271627\ttest-mlogloss:0.275211\n",
      "[65]\ttrain-mlogloss:0.270704\ttest-mlogloss:0.274405\n",
      "[66]\ttrain-mlogloss:0.269972\ttest-mlogloss:0.273497\n",
      "[67]\ttrain-mlogloss:0.269261\ttest-mlogloss:0.272852\n",
      "[68]\ttrain-mlogloss:0.268483\ttest-mlogloss:0.271889\n",
      "[69]\ttrain-mlogloss:0.267586\ttest-mlogloss:0.27097\n",
      "Test error using softprob = 0.10741100158332907\n"
     ]
    }
   ],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_t, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_t).reshape(Y.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != Y) / Y.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.DMatrix at 0x7fbecbbe7eb8>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_10</th>\n",
       "      <th>svd_word_11</th>\n",
       "      <th>svd_word_12</th>\n",
       "      <th>svd_word_13</th>\n",
       "      <th>svd_word_14</th>\n",
       "      <th>svd_word_15</th>\n",
       "      <th>svd_word_16</th>\n",
       "      <th>svd_word_17</th>\n",
       "      <th>svd_word_18</th>\n",
       "      <th>svd_word_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>67</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.842105</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007403</td>\n",
       "      <td>-0.000670</td>\n",
       "      <td>-0.008355</td>\n",
       "      <td>-0.011837</td>\n",
       "      <td>0.036064</td>\n",
       "      <td>-0.016591</td>\n",
       "      <td>-0.025580</td>\n",
       "      <td>-0.018785</td>\n",
       "      <td>0.031289</td>\n",
       "      <td>-0.047220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>175</td>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.338710</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045866</td>\n",
       "      <td>-0.017172</td>\n",
       "      <td>0.019988</td>\n",
       "      <td>-0.004397</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.008583</td>\n",
       "      <td>0.006335</td>\n",
       "      <td>-0.004216</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.001767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>114</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.757576</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002360</td>\n",
       "      <td>0.002929</td>\n",
       "      <td>-0.017479</td>\n",
       "      <td>0.006063</td>\n",
       "      <td>-0.003324</td>\n",
       "      <td>-0.009452</td>\n",
       "      <td>0.013239</td>\n",
       "      <td>0.004852</td>\n",
       "      <td>-0.007478</td>\n",
       "      <td>0.002786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  length  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...      67   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...     175   \n",
       "2  id00134  And when they had broken down the frail door t...     114   \n",
       "\n",
       "   num_words  num_unique_words  num_punctuations  num_words_upper  \\\n",
       "0         19                19                 3                1   \n",
       "1         62                49                 7                1   \n",
       "2         33                30                 3                0   \n",
       "\n",
       "   num_words_title  mean_word_len  num_stopwords     ...       svd_word_10  \\\n",
       "0                3       4.842105              9     ...          0.007403   \n",
       "1                3       4.338710             34     ...         -0.045866   \n",
       "2                1       4.757576             15     ...         -0.002360   \n",
       "\n",
       "   svd_word_11  svd_word_12  svd_word_13  svd_word_14  svd_word_15  \\\n",
       "0    -0.000670    -0.008355    -0.011837     0.036064    -0.016591   \n",
       "1    -0.017172     0.019988    -0.004397    -0.000020    -0.008583   \n",
       "2     0.002929    -0.017479     0.006063    -0.003324    -0.009452   \n",
       "\n",
       "   svd_word_16  svd_word_17  svd_word_18  svd_word_19  \n",
       "0    -0.025580    -0.018785     0.031289    -0.047220  \n",
       "1     0.006335    -0.004216     0.001810     0.001767  \n",
       "2     0.013239     0.004852    -0.007478     0.002786  \n",
       "\n",
       "[3 rows x 237 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>[still, i, urg, leav, ireland, inquietud, impa...</td>\n",
       "      <td>still i urg leav ireland inquietud impati fath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>[if, fire, want, fan, readili, fan, newspap, g...</td>\n",
       "      <td>if fire want fan readili fan newspap govern gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>[and, broken, frail, door, found, two, clean, ...</td>\n",
       "      <td>and broken frail door found two clean pick hum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [still, i, urg, leav, ireland, inquietud, impa...   \n",
       "1  [if, fire, want, fan, readili, fan, newspap, g...   \n",
       "2  [and, broken, frail, door, found, two, clean, ...   \n",
       "\n",
       "                                 cleaned_text_string  \n",
       "0  still i urg leav ireland inquietud impati fath...  \n",
       "1  if fire want fan readili fan newspap govern gr...  \n",
       "2  and broken frail door found two clean pick hum...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['cleaned_text'] = df_test.text.apply(tokenize_stem)\n",
    "df_test['cleaned_text_string'] = df_test.cleaned_text.apply(' '.join)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'text', 'cleaned_text', 'cleaned_text_string', 'length',\n",
       "       'num_words', 'num_unique_words', 'num_punctuations',\n",
       "       'num_words_upper', 'num_words_title', 'mean_word_len',\n",
       "       'num_stopwords', 'mws_index', 'eap_index', 'hpl_index',\n",
       "       'lexical_diversity', 'w2v_array', 'heard', 'raymond', 'fear',\n",
       "       'long', 'chang', 'death', 'love', 'certain', 'like', 'man', 'come',\n",
       "       'even', 'still', 'place', 'hand', 'door', 'strang', 'room', 'men',\n",
       "       'friend', 'appear', 'father', 'eye', 'window', 'street', 'look',\n",
       "       'us', 'made', 'well', 'die', 'ever', 'found', 'two', 'know',\n",
       "       'never', 'say', 'thought', 'light', 'old', 'see', 'whose', 'thus',\n",
       "       'happi', 'everi', 'dream', 'mani', 'time', 'seem', 'hope', 'upon',\n",
       "       'natur', 'might', 'life', 'yet', 'though', 'mind', 'first', 'hous',\n",
       "       'dark', 'heart', 'howev', 'day', 'one', 'return', 'pass', 'shall',\n",
       "       'thing', 'came', 'great', 'littl', 'face', 'night', 'must', 'said',\n",
       "       'much', 'may', 'word', 'saw', 'year', 'feel', 'w2v_feature_0',\n",
       "       'w2v_feature_1', 'w2v_feature_2', 'w2v_feature_3', 'w2v_feature_4',\n",
       "       'w2v_feature_5', 'w2v_feature_6', 'w2v_feature_7', 'w2v_feature_8',\n",
       "       'w2v_feature_9', 'w2v_feature_10', 'w2v_feature_11',\n",
       "       'w2v_feature_12', 'w2v_feature_13', 'w2v_feature_14',\n",
       "       'w2v_feature_15', 'w2v_feature_16', 'w2v_feature_17',\n",
       "       'w2v_feature_18', 'w2v_feature_19', 'w2v_feature_20',\n",
       "       'w2v_feature_21', 'w2v_feature_22', 'w2v_feature_23',\n",
       "       'w2v_feature_24', 'w2v_feature_25', 'w2v_feature_26',\n",
       "       'w2v_feature_27', 'w2v_feature_28', 'w2v_feature_29',\n",
       "       'w2v_feature_30', 'w2v_feature_31', 'w2v_feature_32',\n",
       "       'w2v_feature_33', 'w2v_feature_34', 'w2v_feature_35',\n",
       "       'w2v_feature_36', 'w2v_feature_37', 'w2v_feature_38',\n",
       "       'w2v_feature_39', 'w2v_feature_40', 'w2v_feature_41',\n",
       "       'w2v_feature_42', 'w2v_feature_43', 'w2v_feature_44',\n",
       "       'w2v_feature_45', 'w2v_feature_46', 'w2v_feature_47',\n",
       "       'w2v_feature_48', 'w2v_feature_49', 'w2v_feature_50',\n",
       "       'w2v_feature_51', 'w2v_feature_52', 'w2v_feature_53',\n",
       "       'w2v_feature_54', 'w2v_feature_55', 'w2v_feature_56',\n",
       "       'w2v_feature_57', 'w2v_feature_58', 'w2v_feature_59',\n",
       "       'w2v_feature_60', 'w2v_feature_61', 'w2v_feature_62',\n",
       "       'w2v_feature_63', 'w2v_feature_64', 'w2v_feature_65',\n",
       "       'w2v_feature_66', 'w2v_feature_67', 'w2v_feature_68',\n",
       "       'w2v_feature_69', 'w2v_feature_70', 'w2v_feature_71',\n",
       "       'w2v_feature_72', 'w2v_feature_73', 'w2v_feature_74',\n",
       "       'w2v_feature_75', 'w2v_feature_76', 'w2v_feature_77',\n",
       "       'w2v_feature_78', 'w2v_feature_79', 'w2v_feature_80',\n",
       "       'w2v_feature_81', 'w2v_feature_82', 'w2v_feature_83',\n",
       "       'w2v_feature_84', 'w2v_feature_85', 'w2v_feature_86',\n",
       "       'w2v_feature_87', 'w2v_feature_88', 'w2v_feature_89',\n",
       "       'w2v_feature_90', 'w2v_feature_91', 'w2v_feature_92',\n",
       "       'w2v_feature_93', 'w2v_feature_94', 'w2v_feature_95',\n",
       "       'w2v_feature_96', 'w2v_feature_97', 'w2v_feature_98',\n",
       "       'w2v_feature_99'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'text', 'author', 'cleaned_text', 'cleaned_text_string',\n",
       "       'length', 'num_words', 'num_unique_words', 'num_punctuations',\n",
       "       'num_words_upper', 'num_words_title', 'mean_word_len',\n",
       "       'num_stopwords', 'mws_index', 'eap_index', 'hpl_index',\n",
       "       'lexical_diversity', 'w2v_array', 'heard', 'raymond', 'fear',\n",
       "       'long', 'chang', 'death', 'love', 'certain', 'like', 'man', 'come',\n",
       "       'even', 'still', 'place', 'hand', 'door', 'strang', 'room', 'men',\n",
       "       'friend', 'appear', 'father', 'eye', 'window', 'street', 'look',\n",
       "       'us', 'made', 'well', 'die', 'ever', 'found', 'two', 'know',\n",
       "       'never', 'say', 'thought', 'light', 'old', 'see', 'whose', 'thus',\n",
       "       'happi', 'everi', 'dream', 'mani', 'time', 'seem', 'hope', 'upon',\n",
       "       'natur', 'might', 'life', 'yet', 'though', 'mind', 'first', 'hous',\n",
       "       'dark', 'heart', 'howev', 'day', 'one', 'return', 'pass', 'shall',\n",
       "       'thing', 'came', 'great', 'littl', 'face', 'night', 'must', 'said',\n",
       "       'much', 'may', 'word', 'saw', 'year', 'feel', 'w2v_feature_0',\n",
       "       'w2v_feature_1', 'w2v_feature_2', 'w2v_feature_3', 'w2v_feature_4',\n",
       "       'w2v_feature_5', 'w2v_feature_6', 'w2v_feature_7', 'w2v_feature_8',\n",
       "       'w2v_feature_9', 'w2v_feature_10', 'w2v_feature_11',\n",
       "       'w2v_feature_12', 'w2v_feature_13', 'w2v_feature_14',\n",
       "       'w2v_feature_15', 'w2v_feature_16', 'w2v_feature_17',\n",
       "       'w2v_feature_18', 'w2v_feature_19', 'w2v_feature_20',\n",
       "       'w2v_feature_21', 'w2v_feature_22', 'w2v_feature_23',\n",
       "       'w2v_feature_24', 'w2v_feature_25', 'w2v_feature_26',\n",
       "       'w2v_feature_27', 'w2v_feature_28', 'w2v_feature_29',\n",
       "       'w2v_feature_30', 'w2v_feature_31', 'w2v_feature_32',\n",
       "       'w2v_feature_33', 'w2v_feature_34', 'w2v_feature_35',\n",
       "       'w2v_feature_36', 'w2v_feature_37', 'w2v_feature_38',\n",
       "       'w2v_feature_39', 'w2v_feature_40', 'w2v_feature_41',\n",
       "       'w2v_feature_42', 'w2v_feature_43', 'w2v_feature_44',\n",
       "       'w2v_feature_45', 'w2v_feature_46', 'w2v_feature_47',\n",
       "       'w2v_feature_48', 'w2v_feature_49', 'w2v_feature_50',\n",
       "       'w2v_feature_51', 'w2v_feature_52', 'w2v_feature_53',\n",
       "       'w2v_feature_54', 'w2v_feature_55', 'w2v_feature_56',\n",
       "       'w2v_feature_57', 'w2v_feature_58', 'w2v_feature_59',\n",
       "       'w2v_feature_60', 'w2v_feature_61', 'w2v_feature_62',\n",
       "       'w2v_feature_63', 'w2v_feature_64', 'w2v_feature_65',\n",
       "       'w2v_feature_66', 'w2v_feature_67', 'w2v_feature_68',\n",
       "       'w2v_feature_69', 'w2v_feature_70', 'w2v_feature_71',\n",
       "       'w2v_feature_72', 'w2v_feature_73', 'w2v_feature_74',\n",
       "       'w2v_feature_75', 'w2v_feature_76', 'w2v_feature_77',\n",
       "       'w2v_feature_78', 'w2v_feature_79', 'w2v_feature_80',\n",
       "       'w2v_feature_81', 'w2v_feature_82', 'w2v_feature_83',\n",
       "       'w2v_feature_84', 'w2v_feature_85', 'w2v_feature_86',\n",
       "       'w2v_feature_87', 'w2v_feature_88', 'w2v_feature_89',\n",
       "       'w2v_feature_90', 'w2v_feature_91', 'w2v_feature_92',\n",
       "       'w2v_feature_93', 'w2v_feature_94', 'w2v_feature_95',\n",
       "       'w2v_feature_96', 'w2v_feature_97', 'w2v_feature_98',\n",
       "       'w2v_feature_99'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>...</th>\n",
       "      <th>face</th>\n",
       "      <th>night</th>\n",
       "      <th>must</th>\n",
       "      <th>said</th>\n",
       "      <th>much</th>\n",
       "      <th>may</th>\n",
       "      <th>word</th>\n",
       "      <th>saw</th>\n",
       "      <th>year</th>\n",
       "      <th>feel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>[still, i, urg, leav, ireland, inquietud, impa...</td>\n",
       "      <td>still i urg leav ireland inquietud impati fath...</td>\n",
       "      <td>67</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>[if, fire, want, fan, readili, fan, newspap, g...</td>\n",
       "      <td>if fire want fan readili fan newspap govern gr...</td>\n",
       "      <td>175</td>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>[and, broken, frail, door, found, two, clean, ...</td>\n",
       "      <td>and broken frail door found two clean pick hum...</td>\n",
       "      <td>114</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [still, i, urg, leav, ireland, inquietud, impa...   \n",
       "1  [if, fire, want, fan, readili, fan, newspap, g...   \n",
       "2  [and, broken, frail, door, found, two, clean, ...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  still i urg leav ireland inquietud impati fath...      67         19   \n",
       "1  if fire want fan readili fan newspap govern gr...     175         62   \n",
       "2  and broken frail door found two clean pick hum...     114         33   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper  num_words_title  ...   \\\n",
       "0                19                 3                1                3  ...    \n",
       "1                49                 7                1                3  ...    \n",
       "2                30                 3                0                1  ...    \n",
       "\n",
       "   face  night  must  said  much  may word  saw  year  feel  \n",
       "0     0      0     0     0     0    0    0    0     0     0  \n",
       "1     0      0     0     0     0    0    0    0     0     0  \n",
       "2     0      0     0     0     0    0    0    0     0     0  \n",
       "\n",
       "[3 rows x 97 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_test['length']=df_test['cleaned_text_string'].apply(len)\n",
    "df_test[\"num_words\"] = df_test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_test[\"num_unique_words\"] = df_test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_test[\"num_punctuations\"] =df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test[\"num_words_upper\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_test[\"num_words_title\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_test[\"mean_word_len\"] = df_test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test[\"num_stopwords\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_test['mws_index']=df_test['cleaned_text'].apply(ind_val_mws)/df_test['length']\n",
    "df_test['eap_index']=df_test['cleaned_text'].apply(ind_val_eap)/df_test['length']\n",
    "df_test['hpl_index']=df_test['cleaned_text'].apply(ind_val_hpl)/df_test['length']\n",
    "df_test['lexical_diversity'] = df_test.text.apply(lexical_diversity)\n",
    "df_test['w2v_array'] = df_test.cleaned_text.apply(sum_up_word2vec_array)\n",
    "count_topwords(df_test)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>...</th>\n",
       "      <th>whose</th>\n",
       "      <th>came</th>\n",
       "      <th>said</th>\n",
       "      <th>see</th>\n",
       "      <th>door</th>\n",
       "      <th>certain</th>\n",
       "      <th>found</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>[still, i, urg, leav, ireland, inquietud, impa...</td>\n",
       "      <td>still i urg leav ireland inquietud impati fath...</td>\n",
       "      <td>67</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071473</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>0.026541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>[if, fire, want, fan, readili, fan, newspap, g...</td>\n",
       "      <td>if fire want fan readili fan newspap govern gr...</td>\n",
       "      <td>175</td>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035520</td>\n",
       "      <td>0.063073</td>\n",
       "      <td>0.038550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>[and, broken, frail, door, found, two, clean, ...</td>\n",
       "      <td>and broken frail door found two clean pick hum...</td>\n",
       "      <td>114</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.057723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [still, i, urg, leav, ireland, inquietud, impa...   \n",
       "1  [if, fire, want, fan, readili, fan, newspap, g...   \n",
       "2  [and, broken, frail, door, found, two, clean, ...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  still i urg leav ireland inquietud impati fath...      67         19   \n",
       "1  if fire want fan readili fan newspap govern gr...     175         62   \n",
       "2  and broken frail door found two clean pick hum...     114         33   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper  num_words_title  \\\n",
       "0                19                 3                1                3   \n",
       "1                49                 7                1                3   \n",
       "2                30                 3                0                1   \n",
       "\n",
       "     ...      whose  came  said  see  door  certain  found  mws_index  \\\n",
       "0    ...          0     0     0    0     0        0      0   0.071473   \n",
       "1    ...          0     0     0    0     0        0      0   0.035520   \n",
       "2    ...          0     0     0    0     1        0      1   0.027486   \n",
       "\n",
       "   eap_index  hpl_index  \n",
       "0   0.036315   0.026541  \n",
       "1   0.063073   0.038550  \n",
       "2   0.055142   0.057723  \n",
       "\n",
       "[3 rows x 96 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['mws_index']=df_test['cleaned_text'].apply(ind_val_mws)/df_test['length']\n",
    "df_test['eap_index']=df_test['cleaned_text'].apply(ind_val_eap)/df_test['length']\n",
    "df_test['hpl_index']=df_test['cleaned_text'].apply(ind_val_hpl)/df_test['length']\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del df_test['cleaned_text']\n",
    "# del df_test['cleaned_text_string']\n",
    "del df_test['w2v_array']\n",
    "del df_train['w2v_array']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author2', 'author', 'cleaned_text', 'cleaned_text_string']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=(df_train.columns.tolist())[6:]\n",
    "[item for item in df_train.columns.tolist() if item not in df_test.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['length',\n",
       " 'num_words',\n",
       " 'num_unique_words',\n",
       " 'num_punctuations',\n",
       " 'num_words_upper',\n",
       " 'num_words_title',\n",
       " 'mean_word_len',\n",
       " 'num_stopwords',\n",
       " 'lexical_diversity',\n",
       " 'even',\n",
       " 'seem',\n",
       " 'friend',\n",
       " 'thought',\n",
       " 'hand',\n",
       " 'hope',\n",
       " 'fear',\n",
       " 'mind',\n",
       " 'ever',\n",
       " 'place',\n",
       " 'feel',\n",
       " 'room',\n",
       " 'natur',\n",
       " 'everi',\n",
       " 'face',\n",
       " 'happi',\n",
       " 'love',\n",
       " 'death',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'raymond',\n",
       " 'chang',\n",
       " 'life',\n",
       " 'like',\n",
       " 'still',\n",
       " 'strang',\n",
       " 'much',\n",
       " 'night',\n",
       " 'two',\n",
       " 'window',\n",
       " 'saw',\n",
       " 'made',\n",
       " 'men',\n",
       " 'mani',\n",
       " 'must',\n",
       " 'appear',\n",
       " 'never',\n",
       " 'thus',\n",
       " 'dark',\n",
       " 'know',\n",
       " 'eye',\n",
       " 'pass',\n",
       " 'howev',\n",
       " 'dream',\n",
       " 'shall',\n",
       " 'man',\n",
       " 'us',\n",
       " 'long',\n",
       " 'might',\n",
       " 'yet',\n",
       " 'look',\n",
       " 'light',\n",
       " 'die',\n",
       " 'old',\n",
       " 'may',\n",
       " 'first',\n",
       " 'street',\n",
       " 'say',\n",
       " 'well',\n",
       " 'upon',\n",
       " 'one',\n",
       " 'father',\n",
       " 'come',\n",
       " 'thing',\n",
       " 'time',\n",
       " 'word',\n",
       " 'though',\n",
       " 'day',\n",
       " 'hous',\n",
       " 'return',\n",
       " 'great',\n",
       " 'year',\n",
       " 'littl',\n",
       " 'whose',\n",
       " 'came',\n",
       " 'said',\n",
       " 'see',\n",
       " 'door',\n",
       " 'certain',\n",
       " 'found',\n",
       " 'mws_index',\n",
       " 'eap_index',\n",
       " 'hpl_index',\n",
       " 'svd_word_0',\n",
       " 'svd_word_1',\n",
       " 'svd_word_2',\n",
       " 'svd_word_3',\n",
       " 'svd_word_4',\n",
       " 'svd_word_5',\n",
       " 'svd_word_6',\n",
       " 'svd_word_7',\n",
       " 'svd_word_8',\n",
       " 'svd_word_9',\n",
       " 'svd_word_10',\n",
       " 'svd_word_11',\n",
       " 'svd_word_12',\n",
       " 'svd_word_13',\n",
       " 'svd_word_14',\n",
       " 'svd_word_15',\n",
       " 'svd_word_16',\n",
       " 'svd_word_17',\n",
       " 'svd_word_18',\n",
       " 'svd_word_19',\n",
       " 'nb_cvec_eap',\n",
       " 'nb_cvec_hpl',\n",
       " 'nb_cvec_mws',\n",
       " 'w2v_feature_0',\n",
       " 'w2v_feature_1',\n",
       " 'w2v_feature_2',\n",
       " 'w2v_feature_3',\n",
       " 'w2v_feature_4',\n",
       " 'w2v_feature_5',\n",
       " 'w2v_feature_6',\n",
       " 'w2v_feature_7',\n",
       " 'w2v_feature_8',\n",
       " 'w2v_feature_9',\n",
       " 'w2v_feature_10',\n",
       " 'w2v_feature_11',\n",
       " 'w2v_feature_12',\n",
       " 'w2v_feature_13',\n",
       " 'w2v_feature_14',\n",
       " 'w2v_feature_15',\n",
       " 'w2v_feature_16',\n",
       " 'w2v_feature_17',\n",
       " 'w2v_feature_18',\n",
       " 'w2v_feature_19',\n",
       " 'w2v_feature_20',\n",
       " 'w2v_feature_21',\n",
       " 'w2v_feature_22',\n",
       " 'w2v_feature_23',\n",
       " 'w2v_feature_24',\n",
       " 'w2v_feature_25',\n",
       " 'w2v_feature_26',\n",
       " 'w2v_feature_27',\n",
       " 'w2v_feature_28',\n",
       " 'w2v_feature_29',\n",
       " 'w2v_feature_30',\n",
       " 'w2v_feature_31',\n",
       " 'w2v_feature_32',\n",
       " 'w2v_feature_33',\n",
       " 'w2v_feature_34',\n",
       " 'w2v_feature_35',\n",
       " 'w2v_feature_36',\n",
       " 'w2v_feature_37',\n",
       " 'w2v_feature_38',\n",
       " 'w2v_feature_39',\n",
       " 'w2v_feature_40',\n",
       " 'w2v_feature_41',\n",
       " 'w2v_feature_42',\n",
       " 'w2v_feature_43',\n",
       " 'w2v_feature_44',\n",
       " 'w2v_feature_45',\n",
       " 'w2v_feature_46',\n",
       " 'w2v_feature_47',\n",
       " 'w2v_feature_48',\n",
       " 'w2v_feature_49',\n",
       " 'w2v_feature_50',\n",
       " 'w2v_feature_51',\n",
       " 'w2v_feature_52',\n",
       " 'w2v_feature_53',\n",
       " 'w2v_feature_54',\n",
       " 'w2v_feature_55',\n",
       " 'w2v_feature_56',\n",
       " 'w2v_feature_57',\n",
       " 'w2v_feature_58',\n",
       " 'w2v_feature_59',\n",
       " 'w2v_feature_60',\n",
       " 'w2v_feature_61',\n",
       " 'w2v_feature_62',\n",
       " 'w2v_feature_63',\n",
       " 'w2v_feature_64',\n",
       " 'w2v_feature_65',\n",
       " 'w2v_feature_66',\n",
       " 'w2v_feature_67',\n",
       " 'w2v_feature_68',\n",
       " 'w2v_feature_69',\n",
       " 'w2v_feature_70',\n",
       " 'w2v_feature_71',\n",
       " 'w2v_feature_72',\n",
       " 'w2v_feature_73',\n",
       " 'w2v_feature_74',\n",
       " 'w2v_feature_75',\n",
       " 'w2v_feature_76',\n",
       " 'w2v_feature_77',\n",
       " 'w2v_feature_78',\n",
       " 'w2v_feature_79',\n",
       " 'w2v_feature_80',\n",
       " 'w2v_feature_81',\n",
       " 'w2v_feature_82',\n",
       " 'w2v_feature_83',\n",
       " 'w2v_feature_84',\n",
       " 'w2v_feature_85',\n",
       " 'w2v_feature_86',\n",
       " 'w2v_feature_87',\n",
       " 'w2v_feature_88',\n",
       " 'w2v_feature_89',\n",
       " 'w2v_feature_90',\n",
       " 'w2v_feature_91',\n",
       " 'w2v_feature_92',\n",
       " 'w2v_feature_93',\n",
       " 'w2v_feature_94',\n",
       " 'w2v_feature_95',\n",
       " 'w2v_feature_96',\n",
       " 'w2v_feature_97',\n",
       " 'w2v_feature_98',\n",
       " 'w2v_feature_99',\n",
       " 'svd_word_0',\n",
       " 'svd_word_1',\n",
       " 'svd_word_2',\n",
       " 'svd_word_3',\n",
       " 'svd_word_4',\n",
       " 'svd_word_5',\n",
       " 'svd_word_6',\n",
       " 'svd_word_7',\n",
       " 'svd_word_8',\n",
       " 'svd_word_9',\n",
       " 'svd_word_10',\n",
       " 'svd_word_11',\n",
       " 'svd_word_12',\n",
       " 'svd_word_13',\n",
       " 'svd_word_14',\n",
       " 'svd_word_15',\n",
       " 'svd_word_16',\n",
       " 'svd_word_17',\n",
       " 'svd_word_18',\n",
       " 'svd_word_19']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols[0:235]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['id24265',\n",
       "        'That which is not matter, is not at all unless qualities are things.',\n",
       "        32, ..., 0.011050695953026251, 0.05808397604522373,\n",
       "        -0.015135726659374661],\n",
       "       ['id25917',\n",
       "        'I sought for repose although I did not hope for forgetfulness; I knew I should be pursued by dreams, but did not dread the frightful one that I really had.',\n",
       "        84, ..., -0.009014895176013588, -0.010479364908868475,\n",
       "        0.027816286396748116],\n",
       "       ['id04951',\n",
       "        'Upon the fourth day of the assassination, a party of the police came, very unexpectedly, into the house, and proceeded again to make rigorous investigation of the premises.',\n",
       "        90, ..., -0.034283603615444705, -0.04589236208720345,\n",
       "        -0.03883216131000478],\n",
       "       ..., \n",
       "       ['id27002', 'Say something about objectivity and subjectivity.', 25,\n",
       "        ..., 0.00596432580043175, -0.005367215224570375,\n",
       "        -0.0020850441738909514],\n",
       "       ['id21418',\n",
       "        'In each of the five smaller holes, I deposited a canister containing fifty pounds, and in the larger one a keg holding one hundred and fifty pounds, of cannon powder.',\n",
       "        113, ..., -0.00010823016028945662, -0.0011091773952438472,\n",
       "        -0.0005360678583965142],\n",
       "       ['id23018', 'At other times I was quickly and impetuously smitten.',\n",
       "        30, ..., 0.0010818168854191273, -0.0006070172316537977,\n",
       "        -0.0024135321330780836]], dtype=object)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_test=df_test[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(236, 237)\n"
     ]
    }
   ],
   "source": [
    "print(ds_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'At other times I was quickly and impetuously smitten.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-37848931b58e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mxg_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbstp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_npy2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_init_from_npy2d\u001b[0;34m(self, mat, missing)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input numpy.ndarray must be 2 dimensional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'At other times I was quickly and impetuously smitten.'"
     ]
    }
   ],
   "source": [
    "x_t=ds_test[:, :]\n",
    "y_t=df_test['id'].values\n",
    "xg_t=xgb.DMatrix(x_t)\n",
    "pred_prob = bstp.predict(xg_t).reshape(y_t.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.07466558e-02   4.53274138e-03   9.74720597e-01]\n",
      " [  9.95783210e-01   2.83912686e-03   1.37771375e-03]\n",
      " [  1.08631335e-01   8.89534473e-01   1.83415657e-03]\n",
      " ..., \n",
      " [  9.35469210e-01   2.85144355e-02   3.60163823e-02]\n",
      " [  1.29575823e-02   2.84086540e-03   9.84201610e-01]\n",
      " [  8.50096811e-03   9.90899801e-01   5.99243445e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>0.974721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.995783</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.001378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.108631</td>\n",
       "      <td>0.889534</td>\n",
       "      <td>0.001834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.842110</td>\n",
       "      <td>0.156198</td>\n",
       "      <td>0.001692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.972890</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>0.008357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.020747  0.004533  0.974721\n",
       "1  id24541  0.995783  0.002839  0.001378\n",
       "2  id00134  0.108631  0.889534  0.001834\n",
       "3  id27757  0.842110  0.156198  0.001692\n",
       "4  id04081  0.972890  0.018753  0.008357"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export=pd.DataFrame(pred_prob)\n",
    "export.insert(loc=0, column='id', value=y_t)\n",
    "export.columns=['id','EAP', 'HPL', 'MWS']\n",
    "export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8392"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>id23301</td>\n",
       "      <td>0.175089</td>\n",
       "      <td>0.22535</td>\n",
       "      <td>0.59956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       EAP      HPL      MWS\n",
       "6106  id23301  0.175089  0.22535  0.59956"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export[export['id']=='id23301']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export.to_csv(path_or_buf=\"../data/export.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2vec attempt\n",
    "\n",
    "import gensim\n",
    "# let X be a list of tokenized texts (i.e. list of lists of tokens)\n",
    "model = gensim.models.Word2Vec(filtered_words, size=100)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
