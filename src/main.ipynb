{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from string import digits\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes, pipeline\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from stop_words import get_stop_words\n",
    "import gensim\n",
    "import re\n",
    "from gensim.models import ldamodel as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "# 3 columns id, text, author\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no zeroes!\n",
    "\n",
    "def sum_up_word2vec_array(clnd_text):\n",
    "    \n",
    "    total = None\n",
    "    \n",
    "    for word in clnd_text:\n",
    "        if word in w2v:\n",
    "            if total is None:\n",
    "                total = w2v[word]\n",
    "            else:\n",
    "                total = np.add(total, w2v[word])\n",
    "                \n",
    "    if total is None:\n",
    "        return np.zeros(w2v_array_len)\n",
    "    else:\n",
    "        return total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words('english')).union(set(get_stop_words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "\n",
    "\n",
    "def tokenize_stem(file_text):\n",
    "    #firstly let's apply nltk tokenization\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #deleting stop_words\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens = [i for i in tokens if ( i not in eng_stopwords )]\n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  \n",
       "0  this process howev afford mean ascertain dimen...  \n",
       "1             it never occur fumbl might mere mistak  \n",
       "2  in left hand gold snuff box caper hill cut man...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['cleaned_text'] = df_train.text.apply(tokenize_stem)\n",
    "df_train['cleaned_text_string'] = df_train.cleaned_text.apply(' '.join)\n",
    "df_train.head(n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(file_text):\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "    except:\n",
    "        nltk.download('punkt')\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "        \n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return len(set(tokens))/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[matrix([[ 0.00054413,  0.00054413,  0.00108826, ...,  0.00054413,\n",
      "          0.        ,  0.        ]]), matrix([[ 0.       ,  0.       ,  0.       , ...,  0.       ,  0.0013942,\n",
      "          0.0006971]]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "# extract \"meaningful\" words\n",
    "\n",
    "raw_documents_authors = ['', '', '']\n",
    "\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    if row['author'] == 'EAP':\n",
    "        raw_documents_authors[0] += row['cleaned_text_string'] + ' '\n",
    "    elif row['author'] == 'HPL':\n",
    "        raw_documents_authors[1] += row['cleaned_text_string'] + ' '\n",
    "    else:\n",
    "        raw_documents_authors[2] += row['cleaned_text_string'] + ' '\n",
    "        \n",
    "\n",
    "# delete unique words\n",
    "\n",
    "eap_only = set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "hpl_only = set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "msh_only = set(raw_documents_authors[2].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' '))\n",
    "\n",
    "unique_words = eap_only.union(hpl_only).union(msh_only)\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word')\n",
    "idf_matrix =  tf.fit_transform(raw_documents_authors)\n",
    "feature_names = tf.get_feature_names()\n",
    "# dictionary_word = dict(zip(feature_names, idf_matrix))\n",
    "\n",
    "dense_idf = [i.todense() for i in idf_matrix]\n",
    "print(dense_idf)\n",
    "\n",
    "max_weighted_term = []\n",
    "\n",
    "eap_dense_list = dense_idf[0].tolist()[0]\n",
    "hpl_dense_list = dense_idf[1].tolist()[0]\n",
    "mws_dense_list = dense_idf[2].tolist()[0]\n",
    "\n",
    "for inum, i in enumerate(eap_dense_list):\n",
    "    max_weighted_term.append(max(hpl_dense_list[inum], mws_dense_list[inum], \n",
    "                             i))\n",
    "\n",
    "max_tf_dict = dict(zip(feature_names, max_weighted_term))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(df_train['cleaned_text'], size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_array_len = list(w2v.items())[0][1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_top_words(tfidfdict, numwrd):\n",
    "\n",
    "    top_word_dict, min_value, min_key = {}, 99, ''\n",
    "    \n",
    "\n",
    "    for k, v in max_tf_dict.items():\n",
    "        # print(top_word_dict.values())\n",
    "        # print(v)\n",
    "        if k not in unique_words and k not in eng_stopwords:\n",
    "        \n",
    "            if len(top_word_dict) < numwrd:\n",
    "                top_word_dict[k] = v\n",
    "                if v <= min_value:\n",
    "                    min_key = k\n",
    "            else:\n",
    "                # print(v, min(list(top_word_dict.values())))\n",
    "                if v > min(list(top_word_dict.values())) and k not in eng_stopwords:\n",
    "\n",
    "                    min_value = min(top_word_dict.values())\n",
    "\n",
    "                    for ky, va in top_word_dict.items():\n",
    "                        if va == min_value:\n",
    "                            min_key = ky\n",
    "\n",
    "                    top_word_dict.pop(min_key)\n",
    "                    top_word_dict[k] = v\n",
    "                \n",
    "    return top_word_dict\n",
    "another_top_words_dict = extract_top_words(max_tf_dict, 80)\n",
    "high_tf_idf_words_columns = list(another_top_words_dict.keys())\n",
    "\n",
    "\n",
    "def count_topwords(target_df):\n",
    "\n",
    "    for word in high_tf_idf_words_columns:\n",
    "        \n",
    "        # TODO: костыль, нужен, когда у нас уже есть такие столбцы\n",
    "        # в датасете\n",
    "#         try:\n",
    "#             target_df = target_df.drop(word, 1)\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "        \n",
    "\n",
    "        def count_numwords(collist):\n",
    "            value = 0\n",
    "\n",
    "            for wd in collist:\n",
    "                if wd == word:\n",
    "                    value += 1\n",
    "            return value\n",
    "\n",
    "\n",
    "        target_df[word] = target_df.cleaned_text.apply(count_numwords)\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>heard</th>\n",
       "      <th>made</th>\n",
       "      <th>day</th>\n",
       "      <th>death</th>\n",
       "      <th>eye</th>\n",
       "      <th>one</th>\n",
       "      <th>certain</th>\n",
       "      <th>look</th>\n",
       "      <th>though</th>\n",
       "      <th>see</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper ...   heard  made  day  \\\n",
       "0                35                 7                2 ...       0     0    0   \n",
       "1                14                 1                0 ...       0     0    0   \n",
       "2                32                 5                0 ...       0     0    0   \n",
       "\n",
       "   death eye  one  certain  look  though  see  \n",
       "0      0   0    0        0     0       0    0  \n",
       "1      0   0    0        0     0       0    0  \n",
       "2      0   0    0        0     0       0    0  \n",
       "\n",
       "[3 rows x 95 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['length']=df_train['cleaned_text_string'].apply(len)\n",
    "df_train[\"num_words\"] = df_train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_train[\"num_unique_words\"] = df_train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_train[\"num_punctuations\"] =df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_train[\"num_words_upper\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_train[\"num_words_title\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_train[\"mean_word_len\"] = df_train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_train[\"num_stopwords\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_train['lexical_diversity'] = df_train.text.apply(lexical_diversity)\n",
    "df_train['w2v_array'] = df_train.cleaned_text.apply(sum_up_word2vec_array)\n",
    "count_topwords(df_train)\n",
    "\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_w2v_columns(target_df):\n",
    "    \n",
    "    # сначала вытаскиваем колонку как список списков\n",
    "    \n",
    "    w2v_array = target_df['w2v_array'].tolist()\n",
    "    \n",
    "    for i in range(100):\n",
    "        \n",
    "        target_df['w2v_feature_' + str(i)] = [j[i] for j in w2v_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_w2v_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.282747</td>\n",
       "      <td>9.626265</td>\n",
       "      <td>2.237465</td>\n",
       "      <td>6.716990</td>\n",
       "      <td>-13.649048</td>\n",
       "      <td>4.225520</td>\n",
       "      <td>1.332154</td>\n",
       "      <td>-4.678023</td>\n",
       "      <td>0.837265</td>\n",
       "      <td>1.132221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.068661</td>\n",
       "      <td>2.744264</td>\n",
       "      <td>0.738569</td>\n",
       "      <td>2.069827</td>\n",
       "      <td>-3.973595</td>\n",
       "      <td>1.240327</td>\n",
       "      <td>0.297657</td>\n",
       "      <td>-1.333887</td>\n",
       "      <td>0.329666</td>\n",
       "      <td>0.331183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper       ...        \\\n",
       "0                35                 7                2       ...         \n",
       "1                14                 1                0       ...         \n",
       "\n",
       "   w2v_feature_90  w2v_feature_91  w2v_feature_92  w2v_feature_93  \\\n",
       "0       -7.282747        9.626265        2.237465        6.716990   \n",
       "1       -2.068661        2.744264        0.738569        2.069827   \n",
       "\n",
       "  w2v_feature_94  w2v_feature_95  w2v_feature_96  w2v_feature_97  \\\n",
       "0     -13.649048        4.225520        1.332154       -4.678023   \n",
       "1      -3.973595        1.240327        0.297657       -1.333887   \n",
       "\n",
       "   w2v_feature_98  w2v_feature_99  \n",
       "0        0.837265        1.132221  \n",
       "1        0.329666        0.331183  \n",
       "\n",
       "[2 rows x 195 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>yet</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>80.893038</td>\n",
       "      <td>25.442405</td>\n",
       "      <td>21.894937</td>\n",
       "      <td>4.096329</td>\n",
       "      <td>0.553291</td>\n",
       "      <td>2.102405</td>\n",
       "      <td>4.644952</td>\n",
       "      <td>12.747595</td>\n",
       "      <td>0.886060</td>\n",
       "      <td>0.029367</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.121095</td>\n",
       "      <td>5.174927</td>\n",
       "      <td>1.259972</td>\n",
       "      <td>3.763914</td>\n",
       "      <td>-7.366056</td>\n",
       "      <td>2.379205</td>\n",
       "      <td>0.613379</td>\n",
       "      <td>-2.561031</td>\n",
       "      <td>0.648804</td>\n",
       "      <td>0.580359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>59.749772</td>\n",
       "      <td>18.567706</td>\n",
       "      <td>13.727397</td>\n",
       "      <td>3.573788</td>\n",
       "      <td>0.892966</td>\n",
       "      <td>2.052241</td>\n",
       "      <td>0.631340</td>\n",
       "      <td>9.619779</td>\n",
       "      <td>0.097354</td>\n",
       "      <td>0.174013</td>\n",
       "      <td>...</td>\n",
       "      <td>2.901784</td>\n",
       "      <td>3.640101</td>\n",
       "      <td>1.320235</td>\n",
       "      <td>3.268300</td>\n",
       "      <td>5.187300</td>\n",
       "      <td>1.852029</td>\n",
       "      <td>0.740328</td>\n",
       "      <td>1.783153</td>\n",
       "      <td>1.065674</td>\n",
       "      <td>0.418002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-55.469582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.364278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-99.334419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.296139</td>\n",
       "      <td>-25.904354</td>\n",
       "      <td>-0.995410</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.257265</td>\n",
       "      <td>2.726165</td>\n",
       "      <td>0.484218</td>\n",
       "      <td>1.743163</td>\n",
       "      <td>-9.374925</td>\n",
       "      <td>1.175117</td>\n",
       "      <td>0.228258</td>\n",
       "      <td>-3.289303</td>\n",
       "      <td>0.126715</td>\n",
       "      <td>0.299102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.416716</td>\n",
       "      <td>4.290519</td>\n",
       "      <td>0.881104</td>\n",
       "      <td>2.855869</td>\n",
       "      <td>-6.099398</td>\n",
       "      <td>1.907407</td>\n",
       "      <td>0.533437</td>\n",
       "      <td>-2.120817</td>\n",
       "      <td>0.318356</td>\n",
       "      <td>0.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.164654</td>\n",
       "      <td>6.585654</td>\n",
       "      <td>1.576490</td>\n",
       "      <td>4.724890</td>\n",
       "      <td>-3.866128</td>\n",
       "      <td>3.005250</td>\n",
       "      <td>0.964207</td>\n",
       "      <td>-1.328563</td>\n",
       "      <td>0.720160</td>\n",
       "      <td>0.746170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>925.000000</td>\n",
       "      <td>267.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.969254</td>\n",
       "      <td>29.584175</td>\n",
       "      <td>74.442268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.654839</td>\n",
       "      <td>6.608479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.606798</td>\n",
       "      <td>7.662760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            length    num_words  num_unique_words  num_punctuations  \\\n",
       "count  7900.000000  7900.000000       7900.000000       7900.000000   \n",
       "mean     80.893038    25.442405         21.894937          4.096329   \n",
       "std      59.749772    18.567706         13.727397          3.573788   \n",
       "min       5.000000     2.000000          2.000000          1.000000   \n",
       "25%      40.000000    12.000000         12.000000          2.000000   \n",
       "50%      65.000000    21.000000         19.000000          3.000000   \n",
       "75%     106.000000    33.000000         29.000000          5.000000   \n",
       "max     925.000000   267.000000        155.000000         71.000000   \n",
       "\n",
       "       num_words_upper  num_words_title  mean_word_len  num_stopwords  \\\n",
       "count      7900.000000      7900.000000    7900.000000    7900.000000   \n",
       "mean          0.553291         2.102405       4.644952      12.747595   \n",
       "std           0.892966         2.052241       0.631340       9.619779   \n",
       "min           0.000000         0.000000       2.000000       0.000000   \n",
       "25%           0.000000         1.000000       4.250000       6.000000   \n",
       "50%           0.000000         1.000000       4.600000      10.000000   \n",
       "75%           1.000000         2.000000       5.000000      17.000000   \n",
       "max          15.000000        43.000000      11.000000     135.000000   \n",
       "\n",
       "       lexical_diversity          yet       ...        w2v_feature_90  \\\n",
       "count        7900.000000  7900.000000       ...           7900.000000   \n",
       "mean            0.886060     0.029367       ...             -4.121095   \n",
       "std             0.097354     0.174013       ...              2.901784   \n",
       "min             0.333333     0.000000       ...            -55.469582   \n",
       "25%             0.821429     0.000000       ...             -5.257265   \n",
       "50%             0.894737     0.000000       ...             -3.416716   \n",
       "75%             1.000000     0.000000       ...             -2.164654   \n",
       "max             1.000000     2.000000       ...              0.000000   \n",
       "\n",
       "       w2v_feature_91  w2v_feature_92  w2v_feature_93  w2v_feature_94  \\\n",
       "count     7900.000000     7900.000000     7900.000000     7900.000000   \n",
       "mean         5.174927        1.259972        3.763914       -7.366056   \n",
       "std          3.640101        1.320235        3.268300        5.187300   \n",
       "min          0.000000       -1.364278        0.000000      -99.334419   \n",
       "25%          2.726165        0.484218        1.743163       -9.374925   \n",
       "50%          4.290519        0.881104        2.855869       -6.099398   \n",
       "75%          6.585654        1.576490        4.724890       -3.866128   \n",
       "max         69.969254       29.584175       74.442268        0.000000   \n",
       "\n",
       "       w2v_feature_95  w2v_feature_96  w2v_feature_97  w2v_feature_98  \\\n",
       "count     7900.000000     7900.000000     7900.000000     7900.000000   \n",
       "mean         2.379205        0.613379       -2.561031        0.648804   \n",
       "std          1.852029        0.740328        1.783153        1.065674   \n",
       "min          0.000000       -5.296139      -25.904354       -0.995410   \n",
       "25%          1.175117        0.228258       -3.289303        0.126715   \n",
       "50%          1.907407        0.533437       -2.120817        0.318356   \n",
       "75%          3.005250        0.964207       -1.328563        0.720160   \n",
       "max         40.654839        6.608479        0.000000       24.606798   \n",
       "\n",
       "       w2v_feature_99  \n",
       "count     7900.000000  \n",
       "mean         0.580359  \n",
       "std          0.418002  \n",
       "min          0.000000  \n",
       "25%          0.299102  \n",
       "50%          0.477100  \n",
       "75%          0.746170  \n",
       "max          7.662760  \n",
       "\n",
       "[8 rows x 189 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eap=df_train[df_train['author']=='EAP']\n",
    "df_eap.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>yet</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "      <td>6044.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>85.267869</td>\n",
       "      <td>27.417273</td>\n",
       "      <td>23.544672</td>\n",
       "      <td>3.833719</td>\n",
       "      <td>0.751489</td>\n",
       "      <td>2.124255</td>\n",
       "      <td>4.598182</td>\n",
       "      <td>13.896923</td>\n",
       "      <td>0.883407</td>\n",
       "      <td>0.052614</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.448266</td>\n",
       "      <td>5.683639</td>\n",
       "      <td>1.393814</td>\n",
       "      <td>4.109165</td>\n",
       "      <td>-8.117860</td>\n",
       "      <td>2.570948</td>\n",
       "      <td>0.714982</td>\n",
       "      <td>-2.832133</td>\n",
       "      <td>0.597980</td>\n",
       "      <td>0.675044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>71.372940</td>\n",
       "      <td>23.134440</td>\n",
       "      <td>14.925835</td>\n",
       "      <td>2.840625</td>\n",
       "      <td>1.203636</td>\n",
       "      <td>1.759572</td>\n",
       "      <td>0.561558</td>\n",
       "      <td>12.196599</td>\n",
       "      <td>0.086804</td>\n",
       "      <td>0.234839</td>\n",
       "      <td>...</td>\n",
       "      <td>3.698518</td>\n",
       "      <td>4.743915</td>\n",
       "      <td>1.375281</td>\n",
       "      <td>3.693571</td>\n",
       "      <td>6.772512</td>\n",
       "      <td>2.194987</td>\n",
       "      <td>0.814026</td>\n",
       "      <td>2.396792</td>\n",
       "      <td>0.858745</td>\n",
       "      <td>0.573873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-143.409912</td>\n",
       "      <td>0.078739</td>\n",
       "      <td>-0.414662</td>\n",
       "      <td>0.070037</td>\n",
       "      <td>-259.238342</td>\n",
       "      <td>0.036848</td>\n",
       "      <td>-3.067170</td>\n",
       "      <td>-95.582367</td>\n",
       "      <td>-0.641901</td>\n",
       "      <td>0.009522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.511232</td>\n",
       "      <td>3.250366</td>\n",
       "      <td>0.631881</td>\n",
       "      <td>2.155786</td>\n",
       "      <td>-9.991849</td>\n",
       "      <td>1.414498</td>\n",
       "      <td>0.323831</td>\n",
       "      <td>-3.531134</td>\n",
       "      <td>0.163795</td>\n",
       "      <td>0.376359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>73.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.560791</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.786991</td>\n",
       "      <td>4.843415</td>\n",
       "      <td>1.043284</td>\n",
       "      <td>3.289917</td>\n",
       "      <td>-6.919820</td>\n",
       "      <td>2.121407</td>\n",
       "      <td>0.629973</td>\n",
       "      <td>-2.425043</td>\n",
       "      <td>0.344649</td>\n",
       "      <td>0.569625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>107.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.907156</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.549342</td>\n",
       "      <td>6.997622</td>\n",
       "      <td>1.700543</td>\n",
       "      <td>4.944173</td>\n",
       "      <td>-4.632903</td>\n",
       "      <td>3.121715</td>\n",
       "      <td>1.013919</td>\n",
       "      <td>-1.605256</td>\n",
       "      <td>0.670134</td>\n",
       "      <td>0.835616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2709.000000</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059117</td>\n",
       "      <td>181.707382</td>\n",
       "      <td>36.505371</td>\n",
       "      <td>119.148956</td>\n",
       "      <td>-0.113325</td>\n",
       "      <td>76.459557</td>\n",
       "      <td>29.182920</td>\n",
       "      <td>-0.038485</td>\n",
       "      <td>12.197112</td>\n",
       "      <td>21.186209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            length    num_words  num_unique_words  num_punctuations  \\\n",
       "count  6044.000000  6044.000000       6044.000000       6044.000000   \n",
       "mean     85.267869    27.417273         23.544672          3.833719   \n",
       "std      71.372940    23.134440         14.925835          2.840625   \n",
       "min       3.000000     2.000000          2.000000          1.000000   \n",
       "25%      48.000000    15.000000         14.000000          2.000000   \n",
       "50%      73.000000    23.000000         21.000000          3.000000   \n",
       "75%     107.000000    34.000000         30.000000          5.000000   \n",
       "max    2709.000000   861.000000        429.000000         59.000000   \n",
       "\n",
       "       num_words_upper  num_words_title  mean_word_len  num_stopwords  \\\n",
       "count      6044.000000      6044.000000    6044.000000    6044.000000   \n",
       "mean          0.751489         2.124255       4.598182      13.896923   \n",
       "std           1.203636         1.759572       0.561558      12.196599   \n",
       "min           0.000000         0.000000       2.666667       0.000000   \n",
       "25%           0.000000         1.000000       4.250000       7.000000   \n",
       "50%           0.000000         2.000000       4.560791      12.000000   \n",
       "75%           1.000000         3.000000       4.907156      18.000000   \n",
       "max          27.000000        46.000000      10.500000     437.000000   \n",
       "\n",
       "       lexical_diversity          yet       ...        w2v_feature_90  \\\n",
       "count        6044.000000  6044.000000       ...           6044.000000   \n",
       "mean            0.883407     0.052614       ...             -4.448266   \n",
       "std             0.086804     0.234839       ...              3.698518   \n",
       "min             0.398990     0.000000       ...           -143.409912   \n",
       "25%             0.823529     0.000000       ...             -5.511232   \n",
       "50%             0.885714     0.000000       ...             -3.786991   \n",
       "75%             0.950000     0.000000       ...             -2.549342   \n",
       "max             1.000000     3.000000       ...             -0.059117   \n",
       "\n",
       "       w2v_feature_91  w2v_feature_92  w2v_feature_93  w2v_feature_94  \\\n",
       "count     6044.000000     6044.000000     6044.000000     6044.000000   \n",
       "mean         5.683639        1.393814        4.109165       -8.117860   \n",
       "std          4.743915        1.375281        3.693571        6.772512   \n",
       "min          0.078739       -0.414662        0.070037     -259.238342   \n",
       "25%          3.250366        0.631881        2.155786       -9.991849   \n",
       "50%          4.843415        1.043284        3.289917       -6.919820   \n",
       "75%          6.997622        1.700543        4.944173       -4.632903   \n",
       "max        181.707382       36.505371      119.148956       -0.113325   \n",
       "\n",
       "       w2v_feature_95  w2v_feature_96  w2v_feature_97  w2v_feature_98  \\\n",
       "count     6044.000000     6044.000000     6044.000000     6044.000000   \n",
       "mean         2.570948        0.714982       -2.832133        0.597980   \n",
       "std          2.194987        0.814026        2.396792        0.858745   \n",
       "min          0.036848       -3.067170      -95.582367       -0.641901   \n",
       "25%          1.414498        0.323831       -3.531134        0.163795   \n",
       "50%          2.121407        0.629973       -2.425043        0.344649   \n",
       "75%          3.121715        1.013919       -1.605256        0.670134   \n",
       "max         76.459557       29.182920       -0.038485       12.197112   \n",
       "\n",
       "       w2v_feature_99  \n",
       "count     6044.000000  \n",
       "mean         0.675044  \n",
       "std          0.573873  \n",
       "min          0.009522  \n",
       "25%          0.376359  \n",
       "50%          0.569625  \n",
       "75%          0.835616  \n",
       "max         21.186209  \n",
       "\n",
       "[8 rows x 189 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mws=df_train[df_train['author']=='MWS']\n",
    "df_mws.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all words set\n",
    "\n",
    "wordset=set()\n",
    "\n",
    "for i in df_train.index:\n",
    "    wordset |= set(df_train['cleaned_text'][i])\n",
    "wordlist=list(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>mws</th>\n",
       "      <th>eap</th>\n",
       "      <th>hpl</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inessenti</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doco</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kirwin</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>propens</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drinen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  mws  eap  hpl  all\n",
       "0  inessenti    0    0    0    0\n",
       "1       doco    0    0    0    0\n",
       "2     kirwin    0    0    0    0\n",
       "3    propens    0    0    0    0\n",
       "4     drinen    0    0    0    0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#делаю фрейм со словами\n",
    "df_word=pd.DataFrame(columns=[\"word\", \"mws\", \"eap\", \"hpl\", \"all\"])\n",
    "df_word[\"word\"]=wordlist\n",
    "df_word[\"mws\"]=0\n",
    "df_word[\"eap\"]=0\n",
    "df_word[\"hpl\"]=0\n",
    "df_word[\"all\"]=0\n",
    "df_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "word_dict = {}\n",
    "counter = 0\n",
    "head = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    for word in wordlist:\n",
    "        if word not in word_dict:\n",
    "            head.append(word)\n",
    "            word_dict[word] = counter\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "list_of_lists = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    row = [0 for i in range(len(word_dict))]\n",
    "    for word in wordlist:\n",
    "        row[word_dict[word]] += 1\n",
    "    list_of_lists.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "count_frame = pd.DataFrame(list_of_lists)\n",
    "count_frame['author'] = df_train['author']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "count_frame.columns = head + ['author']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col=list(count_frame.columns)\n",
    "col[-1]='author_name'\n",
    "count_frame.columns=col\n",
    "pivot_col=pd.pivot_table(count_frame, aggfunc=np.sum, values=col, index=['author_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 14351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  \\\n",
       "author_name                                                                  \n",
       "EAP             1      2      0       22       0     2      1     2      3   \n",
       "HPL             0      0      0       17      24     0      1     3      0   \n",
       "MWS             0      0      1        9       0     0      0     1      2   \n",
       "\n",
       "             abbrevi  ...    æmilianus  æneid  ærial  æronaut  ærostat  \\\n",
       "author_name           ...                                                \n",
       "EAP                2  ...            0      0      1        3        1   \n",
       "HPL                0  ...            2      1      0        0        0   \n",
       "MWS                0  ...            0      0      0        0        0   \n",
       "\n",
       "             æschylus  élite  émeut  οἶδα  υπνος  \n",
       "author_name                                       \n",
       "EAP                 1      1      1     0      0  \n",
       "HPL                 0      0      0     2      1  \n",
       "MWS                 0      0      0     0      0  \n",
       "\n",
       "[3 rows x 14351 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete excessive words\n",
    "col=list(pivot_col.columns)\n",
    "col2=[string for string in col if (string[0]!='\"' and string[0]!=\"'\"\n",
    "                                  and string[0]!='.' and string[0]!='`'\n",
    "                                   and len(string)>3 and '.' not in string)]\n",
    "col=[]\n",
    "pivot_col=pivot_col[col2]\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUMA</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 14351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  abbrevi  \\\n",
       "EAP      1      2      0       22       0     2      1     2      3        2   \n",
       "HPL      0      0      0       17      24     0      1     3      0        0   \n",
       "MWS      0      0      1        9       0     0      0     1      2        0   \n",
       "SUMA     1      2      1       48      24     2      2     6      5        2   \n",
       "\n",
       "      ...    æmilianus  æneid  ærial  æronaut  ærostat  æschylus  élite  \\\n",
       "EAP   ...            0      0      1        3        1         1      1   \n",
       "HPL   ...            2      1      0        0        0         0      0   \n",
       "MWS   ...            0      0      0        0        0         0      0   \n",
       "SUMA  ...            2      1      1        3        1         1      1   \n",
       "\n",
       "      émeut  οἶδα  υπνος  \n",
       "EAP       1     0      0  \n",
       "HPL       0     2      1  \n",
       "MWS       0     0      0  \n",
       "SUMA      1     2      1  \n",
       "\n",
       "[4 rows x 14351 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create pivot\n",
    "pivot_col=pivot_col.append(pivot_col.sum(), ignore_index=True)\n",
    "pivot_col.index=['EAP', 'HPL', 'MWS', 'SUMA']\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "      <th>summa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>74269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUMA</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>235194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 14352 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  abbrevi  \\\n",
       "EAP      1      2      0       22       0     2      1     2      3        2   \n",
       "HPL      0      0      0       17      24     0      1     3      0        0   \n",
       "MWS      0      0      1        9       0     0      0     1      2        0   \n",
       "SUMA     1      2      1       48      24     2      2     6      5        2   \n",
       "\n",
       "       ...    æneid  ærial  æronaut  ærostat  æschylus  élite  émeut  οἶδα  \\\n",
       "EAP    ...        0      1        3        1         1      1      1     0   \n",
       "HPL    ...        1      0        0        0         0      0      0     2   \n",
       "MWS    ...        0      0        0        0         0      0      0     0   \n",
       "SUMA   ...        1      1        3        1         1      1      1     2   \n",
       "\n",
       "      υπνος   summa  \n",
       "EAP       0   87765  \n",
       "HPL       1   74269  \n",
       "MWS       0   73160  \n",
       "SUMA      1  235194  \n",
       "\n",
       "[4 rows x 14352 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa=[pivot_col.loc['EAP'].sum(), pivot_col.loc['HPL'].sum(), \n",
    "       pivot_col.loc['MWS'].sum(), pivot_col.loc['SUMA'].sum()]\n",
    "pivot_col['summa']=summa\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdic</th>\n",
       "      <th>aberr</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abhorr</th>\n",
       "      <th>abil</th>\n",
       "      <th>abject</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>your</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zenith</th>\n",
       "      <th>zest</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zone</th>\n",
       "      <th>summa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.373160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.315778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 6619 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandon  abash      abat  abbey     abdic     aberr     abhor    abhorr  \\\n",
       "EAP  0.458333    0.5  0.333333    0.6  0.142857  0.166667  0.058824  0.111111   \n",
       "HPL  0.354167    0.5  0.500000    0.0  0.000000  0.666667  0.235294  0.555556   \n",
       "MWS  0.187500    0.0  0.166667    0.4  0.857143  0.166667  0.705882  0.333333   \n",
       "\n",
       "         abil    abject    ...      younger  youngest      your     youth  \\\n",
       "EAP  0.789474  0.333333    ...     0.272727       0.2  0.534884  0.101562   \n",
       "HPL  0.052632  0.000000    ...     0.000000       0.4  0.069767  0.429688   \n",
       "MWS  0.157895  0.666667    ...     0.727273       0.4  0.395349  0.468750   \n",
       "\n",
       "         zeal  zenith  zest  zigzag      zone     summa  \n",
       "EAP  0.117647     0.4   0.2     0.4  0.666667  0.373160  \n",
       "HPL  0.470588     0.6   0.2     0.6  0.333333  0.315778  \n",
       "MWS  0.411765     0.0   0.6     0.0  0.000000  0.311062  \n",
       "\n",
       "[3 rows x 6619 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create probability of author text knowing that a word was used\n",
    "pivot_part=pivot_col\n",
    "pivot_part.loc['EAP']=pivot_col.loc['EAP']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['HPL']=pivot_col.loc['HPL']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['MWS']=pivot_col.loc['MWS']/pivot_col.loc['SUMA']\n",
    "pivot_part=pivot_part.loc[['EAP', 'HPL', 'MWS']]\n",
    "# Delete unique words\n",
    "pivot_part=pivot_part.loc[:, (pivot_part!=1).all(axis=0)]\n",
    "pivot_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44859813084112149"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will be easier to work this way\n",
    "eap_dict=pivot_part.loc['EAP'].to_dict()\n",
    "hpl_dict=pivot_part.loc['HPL'].to_dict()\n",
    "mws_dict=pivot_part.loc['MWS'].to_dict()\n",
    "eap_dict['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create author score \n",
    "def ind_val_eap(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=eap_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_hpl(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=hpl_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_mws(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=mws_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984288</td>\n",
       "      <td>1.852068</td>\n",
       "      <td>1.080477</td>\n",
       "      <td>-0.010269</td>\n",
       "      <td>-7.757530</td>\n",
       "      <td>-3.017276</td>\n",
       "      <td>2.205754</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.074388</td>\n",
       "      <td>0.034504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255773</td>\n",
       "      <td>0.446480</td>\n",
       "      <td>0.274428</td>\n",
       "      <td>0.282043</td>\n",
       "      <td>-2.035049</td>\n",
       "      <td>-0.809668</td>\n",
       "      <td>0.596570</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.034739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>2.103114</td>\n",
       "      <td>1.331694</td>\n",
       "      <td>-1.781096</td>\n",
       "      <td>-7.162513</td>\n",
       "      <td>-2.376012</td>\n",
       "      <td>0.950710</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper    ...      \\\n",
       "0                35                 7                2    ...       \n",
       "1                14                 1                0    ...       \n",
       "2                32                 5                0    ...       \n",
       "\n",
       "   w2v_feature_93  w2v_feature_94  w2v_feature_95  w2v_feature_96  \\\n",
       "0        0.984288        1.852068        1.080477       -0.010269   \n",
       "1        0.255773        0.446480        0.274428        0.282043   \n",
       "2        0.230420        2.103114        1.331694       -1.781096   \n",
       "\n",
       "  w2v_feature_97  w2v_feature_98  w2v_feature_99  mws_index  eap_index  \\\n",
       "0      -7.757530       -3.017276        2.205754   0.035935   0.074388   \n",
       "1      -2.035049       -0.809668        0.596570   0.035860   0.060980   \n",
       "2      -7.162513       -2.376012        0.950710   0.026900   0.047832   \n",
       "\n",
       "   hpl_index  \n",
       "0   0.034504  \n",
       "1   0.034739  \n",
       "2   0.037337  \n",
       "\n",
       "[3 rows x 198 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add index of author\n",
    "df_train['mws_index']=df_train['cleaned_text'].apply(ind_val_mws)/df_train['length']\n",
    "df_train['eap_index']=df_train['cleaned_text'].apply(ind_val_eap)/df_train['length']\n",
    "df_train['hpl_index']=df_train['cleaned_text'].apply(ind_val_hpl)/df_train['length']\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author2</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984288</td>\n",
       "      <td>1.852068</td>\n",
       "      <td>1.080477</td>\n",
       "      <td>-0.010269</td>\n",
       "      <td>-7.757530</td>\n",
       "      <td>-3.017276</td>\n",
       "      <td>2.205754</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.074388</td>\n",
       "      <td>0.034504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255773</td>\n",
       "      <td>0.446480</td>\n",
       "      <td>0.274428</td>\n",
       "      <td>0.282043</td>\n",
       "      <td>-2.035049</td>\n",
       "      <td>-0.809668</td>\n",
       "      <td>0.596570</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.034739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>2.103114</td>\n",
       "      <td>1.331694</td>\n",
       "      <td>-1.781096</td>\n",
       "      <td>-7.162513</td>\n",
       "      <td>-2.376012</td>\n",
       "      <td>0.950710</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, love, spring, as, look, windsor, terrac,...</td>\n",
       "      <td>how love spring as look windsor terrac sixteen...</td>\n",
       "      <td>144</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434389</td>\n",
       "      <td>2.018498</td>\n",
       "      <td>1.203822</td>\n",
       "      <td>-1.145513</td>\n",
       "      <td>-7.376847</td>\n",
       "      <td>-2.525478</td>\n",
       "      <td>1.201493</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.033438</td>\n",
       "      <td>0.033601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[find, noth, els, even, gold, superintend, aba...</td>\n",
       "      <td>find noth els even gold superintend abandon at...</td>\n",
       "      <td>102</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414073</td>\n",
       "      <td>1.278332</td>\n",
       "      <td>0.731568</td>\n",
       "      <td>-0.165426</td>\n",
       "      <td>-4.971582</td>\n",
       "      <td>-1.783214</td>\n",
       "      <td>1.067079</td>\n",
       "      <td>0.036859</td>\n",
       "      <td>0.056661</td>\n",
       "      <td>0.043735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author2       id                                               text author  \\\n",
       "0        0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1        1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2        0  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3        2  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4        1  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "3  [how, love, spring, as, look, windsor, terrac,...   \n",
       "4  [find, noth, els, even, gold, superintend, aba...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "3  how love spring as look windsor terrac sixteen...     144         34   \n",
       "4  find noth els even gold superintend abandon at...     102         27   \n",
       "\n",
       "   num_unique_words  num_punctuations    ...      w2v_feature_93  \\\n",
       "0                35                 7    ...            0.984288   \n",
       "1                14                 1    ...            0.255773   \n",
       "2                32                 5    ...            0.230420   \n",
       "3                32                 4    ...            0.434389   \n",
       "4                25                 4    ...            0.414073   \n",
       "\n",
       "   w2v_feature_94  w2v_feature_95  w2v_feature_96  w2v_feature_97  \\\n",
       "0        1.852068        1.080477       -0.010269       -7.757530   \n",
       "1        0.446480        0.274428        0.282043       -2.035049   \n",
       "2        2.103114        1.331694       -1.781096       -7.162513   \n",
       "3        2.018498        1.203822       -1.145513       -7.376847   \n",
       "4        1.278332        0.731568       -0.165426       -4.971582   \n",
       "\n",
       "   w2v_feature_98  w2v_feature_99  mws_index  eap_index  hpl_index  \n",
       "0       -3.017276        2.205754   0.035935   0.074388   0.034504  \n",
       "1       -0.809668        0.596570   0.035860   0.060980   0.034739  \n",
       "2       -2.376012        0.950710   0.026900   0.047832   0.037337  \n",
       "3       -2.525478        1.201493   0.071850   0.033438   0.033601  \n",
       "4       -1.783214        1.067079   0.036859   0.056661   0.043735  \n",
       "\n",
       "[5 rows x 198 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform authors' names to numeric\n",
    "df_train['author']=df_train['author'].astype('category')\n",
    "df_train['author2']=df_train['author'].cat.codes\n",
    "# Create different features \n",
    "df_train.head(n=3)\n",
    "mid = df_train['author2']\n",
    "df_train.drop(labels=['author2'], axis=1,inplace = True)\n",
    "df_train.insert(0, 'author2', mid)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = df_train['author'].map(author_mapping_dict)\n",
    "train_id = df_train['id'].values\n",
    "test_id = df_test['id'].values\n",
    "cols_to_drop = ['id', 'text']\n",
    "train_X = df_train.drop(cols_to_drop+['author'], axis=1)\n",
    "test_X = df_test.drop(cols_to_drop, axis=1)\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf = tfidf_vec.fit_transform(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.862729297781\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=3, shuffle=True, random_state=194)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_comp = 50\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "df_test = pd.concat([df_test, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the count vectorizer ###\n",
    "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "tfidf_vec.fit(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.450918416166\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# add the predictions as new features #\n",
    "df_train[\"nb_cvec_eap\"] = pred_train[:,0]\n",
    "df_train[\"nb_cvec_hpl\"] = pred_train[:,1]\n",
    "df_train[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "df_test[\"nb_cvec_eap\"] = pred_full_test[:,0]\n",
    "df_test[\"nb_cvec_hpl\"] = pred_full_test[:,1]\n",
    "df_test[\"nb_cvec_mws\"] = pred_full_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author2</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_43</th>\n",
       "      <th>svd_word_44</th>\n",
       "      <th>svd_word_45</th>\n",
       "      <th>svd_word_46</th>\n",
       "      <th>svd_word_47</th>\n",
       "      <th>svd_word_48</th>\n",
       "      <th>svd_word_49</th>\n",
       "      <th>nb_cvec_eap</th>\n",
       "      <th>nb_cvec_hpl</th>\n",
       "      <th>nb_cvec_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060822</td>\n",
       "      <td>-0.019470</td>\n",
       "      <td>0.022374</td>\n",
       "      <td>0.019055</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.037074</td>\n",
       "      <td>0.002873</td>\n",
       "      <td>9.999933e-01</td>\n",
       "      <td>2.752790e-06</td>\n",
       "      <td>3.990111e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>-0.001032</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>-0.004602</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>-0.002629</td>\n",
       "      <td>8.226820e-01</td>\n",
       "      <td>1.492107e-01</td>\n",
       "      <td>2.810727e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031303</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>-0.014951</td>\n",
       "      <td>-0.028148</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.024380</td>\n",
       "      <td>-0.027564</td>\n",
       "      <td>9.999918e-01</td>\n",
       "      <td>8.206128e-06</td>\n",
       "      <td>1.064720e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, love, spring, as, look, windsor, terrac,...</td>\n",
       "      <td>how love spring as look windsor terrac sixteen...</td>\n",
       "      <td>144</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>-0.009910</td>\n",
       "      <td>0.015214</td>\n",
       "      <td>0.013138</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.017966</td>\n",
       "      <td>-0.011112</td>\n",
       "      <td>1.436890e-09</td>\n",
       "      <td>7.472578e-10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[find, noth, els, even, gold, superintend, aba...</td>\n",
       "      <td>find noth els even gold superintend abandon at...</td>\n",
       "      <td>102</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>-0.001091</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.009183</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>8.960309e-01</td>\n",
       "      <td>1.016456e-01</td>\n",
       "      <td>2.323469e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author2       id                                               text author  \\\n",
       "0        0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1        1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2        0  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3        2  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4        1  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "3  [how, love, spring, as, look, windsor, terrac,...   \n",
       "4  [find, noth, els, even, gold, superintend, aba...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "3  how love spring as look windsor terrac sixteen...     144         34   \n",
       "4  find noth els even gold superintend abandon at...     102         27   \n",
       "\n",
       "   num_unique_words  num_punctuations      ...       svd_word_43  svd_word_44  \\\n",
       "0                35                 7      ...          0.060822    -0.019470   \n",
       "1                14                 1      ...          0.001035     0.000631   \n",
       "2                32                 5      ...         -0.031303     0.011510   \n",
       "3                32                 4      ...          0.009119    -0.009910   \n",
       "4                25                 4      ...          0.001328    -0.000561   \n",
       "\n",
       "   svd_word_45  svd_word_46  svd_word_47 svd_word_48  svd_word_49  \\\n",
       "0     0.022374     0.019055     0.001587    0.037074     0.002873   \n",
       "1    -0.001032     0.003187    -0.004602    0.004110    -0.002629   \n",
       "2    -0.014951    -0.028148     0.010121    0.024380    -0.027564   \n",
       "3     0.015214     0.013138     0.002048    0.017966    -0.011112   \n",
       "4     0.003316    -0.001091     0.000047   -0.009183    -0.002713   \n",
       "\n",
       "    nb_cvec_eap   nb_cvec_hpl   nb_cvec_mws  \n",
       "0  9.999933e-01  2.752790e-06  3.990111e-06  \n",
       "1  8.226820e-01  1.492107e-01  2.810727e-02  \n",
       "2  9.999918e-01  8.206128e-06  1.064720e-08  \n",
       "3  1.436890e-09  7.472578e-10  1.000000e+00  \n",
       "4  8.960309e-01  1.016456e-01  2.323469e-03  \n",
       "\n",
       "[5 rows x 252 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_train['nb_cvec_eap']\n",
    "del df_train['nb_cvec_hpl']\n",
    "del df_train['nb_cvec_mws']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n",
    "        print(message)\n",
    "        print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemm = WordNetLemmatizer()\n",
    "class LemmaCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Storing the entire training text in a list\n",
    "text = list(df_train.text.values)\n",
    "# Calling our overwritten Count vectorizer\n",
    "tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n",
    "                                     min_df=2,\n",
    "                                     stop_words='english',\n",
    "                                     decode_error='ignore')\n",
    "tf = tf_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=13, max_iter=5,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=13, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00452489,  0.00452489,  0.00452489, ...,  0.00452499,\n",
       "         0.06415156,  0.00452489],\n",
       "       [ 0.01538462,  0.01538462,  0.01538467, ...,  0.01538462,\n",
       "         0.01538462,  0.01538462],\n",
       "       [ 0.00404858,  0.00404859,  0.89772543, ...,  0.00404861,\n",
       "         0.00404865,  0.00404867],\n",
       "       ..., \n",
       "       [ 0.00961538,  0.00961538,  0.00961556, ...,  0.0096154 ,\n",
       "         0.00961538,  0.00961538],\n",
       "       [ 0.01098901,  0.21741942,  0.01098901, ...,  0.01098901,\n",
       "         0.01098901,  0.01098906],\n",
       "       [ 0.00961538,  0.00961538,  0.13461525, ...,  0.00961563,\n",
       "         0.00961538,  0.00961544]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create data set\n",
    "ds_train=df_train.values\n",
    "X=ds_train[:, 6:]\n",
    "Y=ds_train[:, 0]\n",
    "seed=7\n",
    "test_size=0.3\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X,Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.921491\ttest-mlogloss:0.922556\n",
      "[1]\ttrain-mlogloss:0.798604\ttest-mlogloss:0.800504\n",
      "[2]\ttrain-mlogloss:0.708918\ttest-mlogloss:0.711609\n",
      "[3]\ttrain-mlogloss:0.641138\ttest-mlogloss:0.644234\n",
      "[4]\ttrain-mlogloss:0.589194\ttest-mlogloss:0.592531\n",
      "[5]\ttrain-mlogloss:0.54839\ttest-mlogloss:0.552153\n",
      "[6]\ttrain-mlogloss:0.516373\ttest-mlogloss:0.520772\n",
      "[7]\ttrain-mlogloss:0.490757\ttest-mlogloss:0.495507\n",
      "[8]\ttrain-mlogloss:0.469581\ttest-mlogloss:0.475061\n",
      "[9]\ttrain-mlogloss:0.452602\ttest-mlogloss:0.458706\n",
      "[10]\ttrain-mlogloss:0.438182\ttest-mlogloss:0.44437\n",
      "[11]\ttrain-mlogloss:0.426372\ttest-mlogloss:0.432798\n",
      "[12]\ttrain-mlogloss:0.4165\ttest-mlogloss:0.423322\n",
      "[13]\ttrain-mlogloss:0.408062\ttest-mlogloss:0.415548\n",
      "[14]\ttrain-mlogloss:0.40077\ttest-mlogloss:0.408611\n",
      "[15]\ttrain-mlogloss:0.394704\ttest-mlogloss:0.402764\n",
      "[16]\ttrain-mlogloss:0.389174\ttest-mlogloss:0.397481\n",
      "[17]\ttrain-mlogloss:0.384247\ttest-mlogloss:0.393244\n",
      "[18]\ttrain-mlogloss:0.380209\ttest-mlogloss:0.389446\n",
      "[19]\ttrain-mlogloss:0.376382\ttest-mlogloss:0.386165\n",
      "[20]\ttrain-mlogloss:0.372874\ttest-mlogloss:0.383272\n",
      "[21]\ttrain-mlogloss:0.369807\ttest-mlogloss:0.380355\n",
      "[22]\ttrain-mlogloss:0.367108\ttest-mlogloss:0.378179\n",
      "[23]\ttrain-mlogloss:0.364248\ttest-mlogloss:0.37589\n",
      "[24]\ttrain-mlogloss:0.361871\ttest-mlogloss:0.373916\n",
      "[25]\ttrain-mlogloss:0.35933\ttest-mlogloss:0.371738\n",
      "[26]\ttrain-mlogloss:0.357122\ttest-mlogloss:0.370107\n",
      "[27]\ttrain-mlogloss:0.355187\ttest-mlogloss:0.368833\n",
      "[28]\ttrain-mlogloss:0.353017\ttest-mlogloss:0.367314\n",
      "[29]\ttrain-mlogloss:0.351283\ttest-mlogloss:0.365844\n",
      "[30]\ttrain-mlogloss:0.349667\ttest-mlogloss:0.36465\n",
      "[31]\ttrain-mlogloss:0.347984\ttest-mlogloss:0.363512\n",
      "[32]\ttrain-mlogloss:0.346273\ttest-mlogloss:0.362436\n",
      "[33]\ttrain-mlogloss:0.344756\ttest-mlogloss:0.361309\n",
      "[34]\ttrain-mlogloss:0.343422\ttest-mlogloss:0.360442\n",
      "[35]\ttrain-mlogloss:0.342238\ttest-mlogloss:0.359547\n",
      "[36]\ttrain-mlogloss:0.340702\ttest-mlogloss:0.3584\n",
      "[37]\ttrain-mlogloss:0.339433\ttest-mlogloss:0.357341\n",
      "[38]\ttrain-mlogloss:0.338227\ttest-mlogloss:0.356714\n",
      "[39]\ttrain-mlogloss:0.336746\ttest-mlogloss:0.355786\n",
      "[40]\ttrain-mlogloss:0.335556\ttest-mlogloss:0.354983\n",
      "[41]\ttrain-mlogloss:0.334403\ttest-mlogloss:0.354302\n",
      "[42]\ttrain-mlogloss:0.333291\ttest-mlogloss:0.353613\n",
      "[43]\ttrain-mlogloss:0.332053\ttest-mlogloss:0.352872\n",
      "[44]\ttrain-mlogloss:0.33099\ttest-mlogloss:0.352244\n",
      "[45]\ttrain-mlogloss:0.329789\ttest-mlogloss:0.351363\n",
      "[46]\ttrain-mlogloss:0.328755\ttest-mlogloss:0.350791\n",
      "[47]\ttrain-mlogloss:0.327752\ttest-mlogloss:0.350437\n",
      "[48]\ttrain-mlogloss:0.326766\ttest-mlogloss:0.349765\n",
      "[49]\ttrain-mlogloss:0.325855\ttest-mlogloss:0.349313\n",
      "[50]\ttrain-mlogloss:0.324758\ttest-mlogloss:0.348701\n",
      "[51]\ttrain-mlogloss:0.323943\ttest-mlogloss:0.348295\n",
      "[52]\ttrain-mlogloss:0.322922\ttest-mlogloss:0.347859\n",
      "[53]\ttrain-mlogloss:0.322023\ttest-mlogloss:0.347291\n",
      "[54]\ttrain-mlogloss:0.320991\ttest-mlogloss:0.34656\n",
      "[55]\ttrain-mlogloss:0.319947\ttest-mlogloss:0.346231\n",
      "[56]\ttrain-mlogloss:0.318968\ttest-mlogloss:0.345606\n",
      "[57]\ttrain-mlogloss:0.318241\ttest-mlogloss:0.345216\n",
      "[58]\ttrain-mlogloss:0.317446\ttest-mlogloss:0.344695\n",
      "[59]\ttrain-mlogloss:0.316675\ttest-mlogloss:0.3442\n",
      "[60]\ttrain-mlogloss:0.315812\ttest-mlogloss:0.343815\n",
      "[61]\ttrain-mlogloss:0.314948\ttest-mlogloss:0.343477\n",
      "[62]\ttrain-mlogloss:0.314045\ttest-mlogloss:0.343234\n",
      "[63]\ttrain-mlogloss:0.313184\ttest-mlogloss:0.342837\n",
      "[64]\ttrain-mlogloss:0.312344\ttest-mlogloss:0.34259\n",
      "[65]\ttrain-mlogloss:0.311792\ttest-mlogloss:0.342345\n",
      "[66]\ttrain-mlogloss:0.31109\ttest-mlogloss:0.342063\n",
      "[67]\ttrain-mlogloss:0.31029\ttest-mlogloss:0.341568\n",
      "[68]\ttrain-mlogloss:0.309638\ttest-mlogloss:0.341355\n",
      "[69]\ttrain-mlogloss:0.30885\ttest-mlogloss:0.34109\n",
      "[70]\ttrain-mlogloss:0.30816\ttest-mlogloss:0.340889\n",
      "[71]\ttrain-mlogloss:0.307484\ttest-mlogloss:0.340622\n",
      "[72]\ttrain-mlogloss:0.306774\ttest-mlogloss:0.340146\n",
      "[73]\ttrain-mlogloss:0.305997\ttest-mlogloss:0.339937\n",
      "[74]\ttrain-mlogloss:0.305341\ttest-mlogloss:0.339637\n",
      "[75]\ttrain-mlogloss:0.30464\ttest-mlogloss:0.339418\n",
      "[76]\ttrain-mlogloss:0.303847\ttest-mlogloss:0.339149\n",
      "[77]\ttrain-mlogloss:0.303013\ttest-mlogloss:0.338789\n",
      "[78]\ttrain-mlogloss:0.302311\ttest-mlogloss:0.338775\n",
      "[79]\ttrain-mlogloss:0.301651\ttest-mlogloss:0.338397\n",
      "[80]\ttrain-mlogloss:0.301019\ttest-mlogloss:0.338019\n",
      "[81]\ttrain-mlogloss:0.300483\ttest-mlogloss:0.33771\n",
      "[82]\ttrain-mlogloss:0.299976\ttest-mlogloss:0.337479\n",
      "[83]\ttrain-mlogloss:0.299326\ttest-mlogloss:0.337221\n",
      "[84]\ttrain-mlogloss:0.298647\ttest-mlogloss:0.337124\n",
      "[85]\ttrain-mlogloss:0.298069\ttest-mlogloss:0.337099\n",
      "[86]\ttrain-mlogloss:0.297523\ttest-mlogloss:0.336899\n",
      "[87]\ttrain-mlogloss:0.296953\ttest-mlogloss:0.336737\n",
      "[88]\ttrain-mlogloss:0.296447\ttest-mlogloss:0.33652\n",
      "[89]\ttrain-mlogloss:0.295895\ttest-mlogloss:0.33657\n",
      "[90]\ttrain-mlogloss:0.295467\ttest-mlogloss:0.336401\n",
      "[91]\ttrain-mlogloss:0.294898\ttest-mlogloss:0.336209\n",
      "[92]\ttrain-mlogloss:0.294346\ttest-mlogloss:0.336148\n",
      "[93]\ttrain-mlogloss:0.293747\ttest-mlogloss:0.336058\n",
      "[94]\ttrain-mlogloss:0.293231\ttest-mlogloss:0.335899\n",
      "[95]\ttrain-mlogloss:0.29262\ttest-mlogloss:0.335692\n",
      "[96]\ttrain-mlogloss:0.292116\ttest-mlogloss:0.335511\n",
      "[97]\ttrain-mlogloss:0.291575\ttest-mlogloss:0.335258\n",
      "[98]\ttrain-mlogloss:0.291008\ttest-mlogloss:0.335038\n",
      "[99]\ttrain-mlogloss:0.290621\ttest-mlogloss:0.334904\n",
      "[100]\ttrain-mlogloss:0.290158\ttest-mlogloss:0.334801\n",
      "[101]\ttrain-mlogloss:0.289555\ttest-mlogloss:0.334543\n",
      "[102]\ttrain-mlogloss:0.28906\ttest-mlogloss:0.334467\n",
      "[103]\ttrain-mlogloss:0.288533\ttest-mlogloss:0.334321\n",
      "[104]\ttrain-mlogloss:0.287963\ttest-mlogloss:0.334069\n",
      "[105]\ttrain-mlogloss:0.287434\ttest-mlogloss:0.333859\n",
      "[106]\ttrain-mlogloss:0.287002\ttest-mlogloss:0.333796\n",
      "[107]\ttrain-mlogloss:0.286375\ttest-mlogloss:0.333588\n",
      "[108]\ttrain-mlogloss:0.285917\ttest-mlogloss:0.333486\n",
      "[109]\ttrain-mlogloss:0.285371\ttest-mlogloss:0.333315\n",
      "[110]\ttrain-mlogloss:0.285011\ttest-mlogloss:0.333114\n",
      "[111]\ttrain-mlogloss:0.284494\ttest-mlogloss:0.333023\n",
      "[112]\ttrain-mlogloss:0.283938\ttest-mlogloss:0.332901\n",
      "[113]\ttrain-mlogloss:0.283406\ttest-mlogloss:0.332726\n",
      "[114]\ttrain-mlogloss:0.283001\ttest-mlogloss:0.332596\n",
      "[115]\ttrain-mlogloss:0.282554\ttest-mlogloss:0.332448\n",
      "[116]\ttrain-mlogloss:0.282152\ttest-mlogloss:0.332277\n",
      "[117]\ttrain-mlogloss:0.281537\ttest-mlogloss:0.332124\n",
      "[118]\ttrain-mlogloss:0.281093\ttest-mlogloss:0.331925\n",
      "[119]\ttrain-mlogloss:0.280709\ttest-mlogloss:0.331762\n",
      "[120]\ttrain-mlogloss:0.280217\ttest-mlogloss:0.331616\n",
      "[121]\ttrain-mlogloss:0.27983\ttest-mlogloss:0.33155\n",
      "[122]\ttrain-mlogloss:0.279443\ttest-mlogloss:0.331384\n",
      "[123]\ttrain-mlogloss:0.279089\ttest-mlogloss:0.331406\n",
      "[124]\ttrain-mlogloss:0.278686\ttest-mlogloss:0.331292\n",
      "[125]\ttrain-mlogloss:0.278252\ttest-mlogloss:0.331209\n",
      "[126]\ttrain-mlogloss:0.277738\ttest-mlogloss:0.330999\n",
      "[127]\ttrain-mlogloss:0.277313\ttest-mlogloss:0.330964\n",
      "[128]\ttrain-mlogloss:0.276792\ttest-mlogloss:0.330778\n",
      "[129]\ttrain-mlogloss:0.276405\ttest-mlogloss:0.330739\n",
      "[130]\ttrain-mlogloss:0.276034\ttest-mlogloss:0.330857\n",
      "[131]\ttrain-mlogloss:0.275636\ttest-mlogloss:0.330819\n",
      "[132]\ttrain-mlogloss:0.275207\ttest-mlogloss:0.330757\n",
      "[133]\ttrain-mlogloss:0.274798\ttest-mlogloss:0.330501\n",
      "[134]\ttrain-mlogloss:0.274407\ttest-mlogloss:0.330361\n",
      "[135]\ttrain-mlogloss:0.273933\ttest-mlogloss:0.33038\n",
      "[136]\ttrain-mlogloss:0.273449\ttest-mlogloss:0.330412\n",
      "[137]\ttrain-mlogloss:0.273136\ttest-mlogloss:0.330359\n",
      "[138]\ttrain-mlogloss:0.272723\ttest-mlogloss:0.330404\n",
      "[139]\ttrain-mlogloss:0.272367\ttest-mlogloss:0.330456\n",
      "[140]\ttrain-mlogloss:0.27197\ttest-mlogloss:0.330482\n",
      "[141]\ttrain-mlogloss:0.271603\ttest-mlogloss:0.330529\n",
      "[142]\ttrain-mlogloss:0.271184\ttest-mlogloss:0.330509\n",
      "[143]\ttrain-mlogloss:0.270792\ttest-mlogloss:0.330523\n",
      "[144]\ttrain-mlogloss:0.270361\ttest-mlogloss:0.330293\n",
      "[145]\ttrain-mlogloss:0.269941\ttest-mlogloss:0.33011\n",
      "[146]\ttrain-mlogloss:0.269596\ttest-mlogloss:0.329972\n",
      "[147]\ttrain-mlogloss:0.269206\ttest-mlogloss:0.329806\n",
      "[148]\ttrain-mlogloss:0.268801\ttest-mlogloss:0.329703\n",
      "[149]\ttrain-mlogloss:0.268407\ttest-mlogloss:0.329689\n",
      "[150]\ttrain-mlogloss:0.268025\ttest-mlogloss:0.329696\n",
      "[151]\ttrain-mlogloss:0.267601\ttest-mlogloss:0.329559\n",
      "[152]\ttrain-mlogloss:0.267266\ttest-mlogloss:0.329505\n",
      "[153]\ttrain-mlogloss:0.266843\ttest-mlogloss:0.329441\n",
      "[154]\ttrain-mlogloss:0.266478\ttest-mlogloss:0.329432\n",
      "[155]\ttrain-mlogloss:0.266197\ttest-mlogloss:0.329371\n",
      "[156]\ttrain-mlogloss:0.265811\ttest-mlogloss:0.329354\n",
      "[157]\ttrain-mlogloss:0.265426\ttest-mlogloss:0.329231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158]\ttrain-mlogloss:0.265062\ttest-mlogloss:0.329184\n",
      "[159]\ttrain-mlogloss:0.264783\ttest-mlogloss:0.329138\n",
      "[160]\ttrain-mlogloss:0.264363\ttest-mlogloss:0.329166\n",
      "[161]\ttrain-mlogloss:0.264102\ttest-mlogloss:0.329087\n",
      "[162]\ttrain-mlogloss:0.263768\ttest-mlogloss:0.329008\n",
      "[163]\ttrain-mlogloss:0.263426\ttest-mlogloss:0.328956\n",
      "[164]\ttrain-mlogloss:0.262933\ttest-mlogloss:0.328963\n",
      "[165]\ttrain-mlogloss:0.262586\ttest-mlogloss:0.328765\n",
      "[166]\ttrain-mlogloss:0.262196\ttest-mlogloss:0.328762\n",
      "[167]\ttrain-mlogloss:0.261851\ttest-mlogloss:0.328684\n",
      "[168]\ttrain-mlogloss:0.261571\ttest-mlogloss:0.328725\n",
      "[169]\ttrain-mlogloss:0.26122\ttest-mlogloss:0.328756\n",
      "[170]\ttrain-mlogloss:0.260942\ttest-mlogloss:0.328775\n",
      "[171]\ttrain-mlogloss:0.260638\ttest-mlogloss:0.328852\n",
      "[172]\ttrain-mlogloss:0.260242\ttest-mlogloss:0.328796\n",
      "[173]\ttrain-mlogloss:0.259864\ttest-mlogloss:0.328758\n",
      "[174]\ttrain-mlogloss:0.25949\ttest-mlogloss:0.328719\n",
      "[175]\ttrain-mlogloss:0.259133\ttest-mlogloss:0.328689\n",
      "[176]\ttrain-mlogloss:0.258801\ttest-mlogloss:0.328684\n",
      "[177]\ttrain-mlogloss:0.258406\ttest-mlogloss:0.328656\n",
      "[178]\ttrain-mlogloss:0.258058\ttest-mlogloss:0.328553\n",
      "[179]\ttrain-mlogloss:0.257809\ttest-mlogloss:0.328537\n",
      "[180]\ttrain-mlogloss:0.257542\ttest-mlogloss:0.328509\n",
      "[181]\ttrain-mlogloss:0.257138\ttest-mlogloss:0.328314\n",
      "[182]\ttrain-mlogloss:0.256819\ttest-mlogloss:0.328378\n",
      "[183]\ttrain-mlogloss:0.256489\ttest-mlogloss:0.328309\n",
      "[184]\ttrain-mlogloss:0.256191\ttest-mlogloss:0.328303\n",
      "[185]\ttrain-mlogloss:0.255863\ttest-mlogloss:0.328286\n",
      "[186]\ttrain-mlogloss:0.255521\ttest-mlogloss:0.328164\n",
      "[187]\ttrain-mlogloss:0.255207\ttest-mlogloss:0.328122\n",
      "[188]\ttrain-mlogloss:0.254821\ttest-mlogloss:0.327938\n",
      "[189]\ttrain-mlogloss:0.254453\ttest-mlogloss:0.327873\n",
      "[190]\ttrain-mlogloss:0.254038\ttest-mlogloss:0.327738\n",
      "[191]\ttrain-mlogloss:0.253683\ttest-mlogloss:0.327678\n",
      "[192]\ttrain-mlogloss:0.253394\ttest-mlogloss:0.327645\n",
      "[193]\ttrain-mlogloss:0.25312\ttest-mlogloss:0.327614\n",
      "[194]\ttrain-mlogloss:0.252756\ttest-mlogloss:0.327683\n",
      "[195]\ttrain-mlogloss:0.252433\ttest-mlogloss:0.327634\n",
      "[196]\ttrain-mlogloss:0.252152\ttest-mlogloss:0.327689\n",
      "[197]\ttrain-mlogloss:0.251768\ttest-mlogloss:0.327594\n",
      "[198]\ttrain-mlogloss:0.251428\ttest-mlogloss:0.327529\n",
      "[199]\ttrain-mlogloss:0.251015\ttest-mlogloss:0.327386\n",
      "[200]\ttrain-mlogloss:0.250701\ttest-mlogloss:0.327445\n",
      "[201]\ttrain-mlogloss:0.25041\ttest-mlogloss:0.32743\n",
      "[202]\ttrain-mlogloss:0.250129\ttest-mlogloss:0.32728\n",
      "[203]\ttrain-mlogloss:0.249814\ttest-mlogloss:0.327287\n",
      "[204]\ttrain-mlogloss:0.249528\ttest-mlogloss:0.327227\n",
      "[205]\ttrain-mlogloss:0.249218\ttest-mlogloss:0.327203\n",
      "[206]\ttrain-mlogloss:0.248865\ttest-mlogloss:0.327242\n",
      "[207]\ttrain-mlogloss:0.24862\ttest-mlogloss:0.327242\n",
      "[208]\ttrain-mlogloss:0.248402\ttest-mlogloss:0.327198\n",
      "[209]\ttrain-mlogloss:0.248142\ttest-mlogloss:0.327112\n",
      "[210]\ttrain-mlogloss:0.247833\ttest-mlogloss:0.327158\n",
      "[211]\ttrain-mlogloss:0.247543\ttest-mlogloss:0.327034\n",
      "[212]\ttrain-mlogloss:0.247225\ttest-mlogloss:0.32705\n",
      "[213]\ttrain-mlogloss:0.246954\ttest-mlogloss:0.326999\n",
      "[214]\ttrain-mlogloss:0.246732\ttest-mlogloss:0.326953\n",
      "[215]\ttrain-mlogloss:0.246392\ttest-mlogloss:0.326941\n",
      "[216]\ttrain-mlogloss:0.246048\ttest-mlogloss:0.326884\n",
      "[217]\ttrain-mlogloss:0.245797\ttest-mlogloss:0.326849\n",
      "[218]\ttrain-mlogloss:0.24548\ttest-mlogloss:0.326684\n",
      "[219]\ttrain-mlogloss:0.245182\ttest-mlogloss:0.326643\n",
      "[220]\ttrain-mlogloss:0.244847\ttest-mlogloss:0.326581\n",
      "[221]\ttrain-mlogloss:0.244566\ttest-mlogloss:0.326536\n",
      "[222]\ttrain-mlogloss:0.244272\ttest-mlogloss:0.326433\n",
      "[223]\ttrain-mlogloss:0.243941\ttest-mlogloss:0.326509\n",
      "[224]\ttrain-mlogloss:0.243616\ttest-mlogloss:0.32657\n",
      "[225]\ttrain-mlogloss:0.243313\ttest-mlogloss:0.326535\n",
      "[226]\ttrain-mlogloss:0.243044\ttest-mlogloss:0.326591\n",
      "[227]\ttrain-mlogloss:0.242821\ttest-mlogloss:0.326696\n",
      "[228]\ttrain-mlogloss:0.242637\ttest-mlogloss:0.326652\n",
      "[229]\ttrain-mlogloss:0.24241\ttest-mlogloss:0.326645\n",
      "[230]\ttrain-mlogloss:0.242069\ttest-mlogloss:0.326719\n",
      "[231]\ttrain-mlogloss:0.241806\ttest-mlogloss:0.326576\n",
      "[232]\ttrain-mlogloss:0.241552\ttest-mlogloss:0.326578\n",
      "[233]\ttrain-mlogloss:0.241308\ttest-mlogloss:0.32661\n",
      "[234]\ttrain-mlogloss:0.241067\ttest-mlogloss:0.326631\n",
      "[235]\ttrain-mlogloss:0.2408\ttest-mlogloss:0.32665\n",
      "[236]\ttrain-mlogloss:0.240528\ttest-mlogloss:0.326686\n",
      "[237]\ttrain-mlogloss:0.24025\ttest-mlogloss:0.326748\n",
      "[238]\ttrain-mlogloss:0.239962\ttest-mlogloss:0.326707\n",
      "[239]\ttrain-mlogloss:0.239748\ttest-mlogloss:0.326819\n",
      "Test error using softmax = 0.12751106571331292\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "xg_train=xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test=xgb.DMatrix(X_test, label=y_test)\n",
    "xg_t=xgb.DMatrix(X, label=Y)\n",
    "param={}\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['eta'] = 0.2\n",
    "param['max_depth'] = 2\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 3\n",
    "param['eval_metric']= \"mlogloss\"\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 240\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# get prediction\n",
    "pred = bst.predict(xg_test)\n",
    "error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.921491\ttest-mlogloss:0.922556\n",
      "[1]\ttrain-mlogloss:0.798604\ttest-mlogloss:0.800504\n",
      "[2]\ttrain-mlogloss:0.708918\ttest-mlogloss:0.711609\n",
      "[3]\ttrain-mlogloss:0.641138\ttest-mlogloss:0.644234\n",
      "[4]\ttrain-mlogloss:0.589194\ttest-mlogloss:0.592531\n",
      "[5]\ttrain-mlogloss:0.54839\ttest-mlogloss:0.552153\n",
      "[6]\ttrain-mlogloss:0.516373\ttest-mlogloss:0.520772\n",
      "[7]\ttrain-mlogloss:0.490757\ttest-mlogloss:0.495507\n",
      "[8]\ttrain-mlogloss:0.469581\ttest-mlogloss:0.475061\n",
      "[9]\ttrain-mlogloss:0.452602\ttest-mlogloss:0.458706\n",
      "[10]\ttrain-mlogloss:0.438182\ttest-mlogloss:0.44437\n",
      "[11]\ttrain-mlogloss:0.426372\ttest-mlogloss:0.432798\n",
      "[12]\ttrain-mlogloss:0.4165\ttest-mlogloss:0.423322\n",
      "[13]\ttrain-mlogloss:0.408062\ttest-mlogloss:0.415548\n",
      "[14]\ttrain-mlogloss:0.40077\ttest-mlogloss:0.408611\n",
      "[15]\ttrain-mlogloss:0.394704\ttest-mlogloss:0.402764\n",
      "[16]\ttrain-mlogloss:0.389174\ttest-mlogloss:0.397481\n",
      "[17]\ttrain-mlogloss:0.384247\ttest-mlogloss:0.393244\n",
      "[18]\ttrain-mlogloss:0.380209\ttest-mlogloss:0.389446\n",
      "[19]\ttrain-mlogloss:0.376382\ttest-mlogloss:0.386165\n",
      "[20]\ttrain-mlogloss:0.372874\ttest-mlogloss:0.383272\n",
      "[21]\ttrain-mlogloss:0.369807\ttest-mlogloss:0.380355\n",
      "[22]\ttrain-mlogloss:0.367108\ttest-mlogloss:0.378179\n",
      "[23]\ttrain-mlogloss:0.364248\ttest-mlogloss:0.37589\n",
      "[24]\ttrain-mlogloss:0.361871\ttest-mlogloss:0.373916\n",
      "[25]\ttrain-mlogloss:0.35933\ttest-mlogloss:0.371738\n",
      "[26]\ttrain-mlogloss:0.357122\ttest-mlogloss:0.370107\n",
      "[27]\ttrain-mlogloss:0.355187\ttest-mlogloss:0.368833\n",
      "[28]\ttrain-mlogloss:0.353017\ttest-mlogloss:0.367314\n",
      "[29]\ttrain-mlogloss:0.351283\ttest-mlogloss:0.365844\n",
      "[30]\ttrain-mlogloss:0.349667\ttest-mlogloss:0.36465\n",
      "[31]\ttrain-mlogloss:0.347984\ttest-mlogloss:0.363512\n",
      "[32]\ttrain-mlogloss:0.346273\ttest-mlogloss:0.362436\n",
      "[33]\ttrain-mlogloss:0.344756\ttest-mlogloss:0.361309\n",
      "[34]\ttrain-mlogloss:0.343422\ttest-mlogloss:0.360442\n",
      "[35]\ttrain-mlogloss:0.342238\ttest-mlogloss:0.359547\n",
      "[36]\ttrain-mlogloss:0.340702\ttest-mlogloss:0.3584\n",
      "[37]\ttrain-mlogloss:0.339433\ttest-mlogloss:0.357341\n",
      "[38]\ttrain-mlogloss:0.338227\ttest-mlogloss:0.356714\n",
      "[39]\ttrain-mlogloss:0.336746\ttest-mlogloss:0.355786\n",
      "[40]\ttrain-mlogloss:0.335556\ttest-mlogloss:0.354983\n",
      "[41]\ttrain-mlogloss:0.334403\ttest-mlogloss:0.354302\n",
      "[42]\ttrain-mlogloss:0.333291\ttest-mlogloss:0.353613\n",
      "[43]\ttrain-mlogloss:0.332053\ttest-mlogloss:0.352872\n",
      "[44]\ttrain-mlogloss:0.33099\ttest-mlogloss:0.352244\n",
      "[45]\ttrain-mlogloss:0.329789\ttest-mlogloss:0.351363\n",
      "[46]\ttrain-mlogloss:0.328755\ttest-mlogloss:0.350791\n",
      "[47]\ttrain-mlogloss:0.327752\ttest-mlogloss:0.350437\n",
      "[48]\ttrain-mlogloss:0.326766\ttest-mlogloss:0.349765\n",
      "[49]\ttrain-mlogloss:0.325855\ttest-mlogloss:0.349313\n",
      "[50]\ttrain-mlogloss:0.324758\ttest-mlogloss:0.348701\n",
      "[51]\ttrain-mlogloss:0.323943\ttest-mlogloss:0.348295\n",
      "[52]\ttrain-mlogloss:0.322922\ttest-mlogloss:0.347859\n",
      "[53]\ttrain-mlogloss:0.322023\ttest-mlogloss:0.347291\n",
      "[54]\ttrain-mlogloss:0.320991\ttest-mlogloss:0.34656\n",
      "[55]\ttrain-mlogloss:0.319947\ttest-mlogloss:0.346231\n",
      "[56]\ttrain-mlogloss:0.318968\ttest-mlogloss:0.345606\n",
      "[57]\ttrain-mlogloss:0.318241\ttest-mlogloss:0.345216\n",
      "[58]\ttrain-mlogloss:0.317446\ttest-mlogloss:0.344695\n",
      "[59]\ttrain-mlogloss:0.316675\ttest-mlogloss:0.3442\n",
      "[60]\ttrain-mlogloss:0.315812\ttest-mlogloss:0.343815\n",
      "[61]\ttrain-mlogloss:0.314948\ttest-mlogloss:0.343477\n",
      "[62]\ttrain-mlogloss:0.314045\ttest-mlogloss:0.343234\n",
      "[63]\ttrain-mlogloss:0.313184\ttest-mlogloss:0.342837\n",
      "[64]\ttrain-mlogloss:0.312344\ttest-mlogloss:0.34259\n",
      "[65]\ttrain-mlogloss:0.311792\ttest-mlogloss:0.342345\n",
      "[66]\ttrain-mlogloss:0.31109\ttest-mlogloss:0.342063\n",
      "[67]\ttrain-mlogloss:0.31029\ttest-mlogloss:0.341568\n",
      "[68]\ttrain-mlogloss:0.309638\ttest-mlogloss:0.341355\n",
      "[69]\ttrain-mlogloss:0.30885\ttest-mlogloss:0.34109\n",
      "[70]\ttrain-mlogloss:0.30816\ttest-mlogloss:0.340889\n",
      "[71]\ttrain-mlogloss:0.307484\ttest-mlogloss:0.340622\n",
      "[72]\ttrain-mlogloss:0.306774\ttest-mlogloss:0.340146\n",
      "[73]\ttrain-mlogloss:0.305997\ttest-mlogloss:0.339937\n",
      "[74]\ttrain-mlogloss:0.305341\ttest-mlogloss:0.339637\n",
      "[75]\ttrain-mlogloss:0.30464\ttest-mlogloss:0.339418\n",
      "[76]\ttrain-mlogloss:0.303847\ttest-mlogloss:0.339149\n",
      "[77]\ttrain-mlogloss:0.303013\ttest-mlogloss:0.338789\n",
      "[78]\ttrain-mlogloss:0.302311\ttest-mlogloss:0.338775\n",
      "[79]\ttrain-mlogloss:0.301651\ttest-mlogloss:0.338397\n",
      "[80]\ttrain-mlogloss:0.301019\ttest-mlogloss:0.338019\n",
      "[81]\ttrain-mlogloss:0.300483\ttest-mlogloss:0.33771\n",
      "[82]\ttrain-mlogloss:0.299976\ttest-mlogloss:0.337479\n",
      "[83]\ttrain-mlogloss:0.299326\ttest-mlogloss:0.337221\n",
      "[84]\ttrain-mlogloss:0.298647\ttest-mlogloss:0.337124\n",
      "[85]\ttrain-mlogloss:0.298069\ttest-mlogloss:0.337099\n",
      "[86]\ttrain-mlogloss:0.297523\ttest-mlogloss:0.336899\n",
      "[87]\ttrain-mlogloss:0.296953\ttest-mlogloss:0.336737\n",
      "[88]\ttrain-mlogloss:0.296447\ttest-mlogloss:0.33652\n",
      "[89]\ttrain-mlogloss:0.295895\ttest-mlogloss:0.33657\n",
      "[90]\ttrain-mlogloss:0.295467\ttest-mlogloss:0.336401\n",
      "[91]\ttrain-mlogloss:0.294898\ttest-mlogloss:0.336209\n",
      "[92]\ttrain-mlogloss:0.294346\ttest-mlogloss:0.336148\n",
      "[93]\ttrain-mlogloss:0.293747\ttest-mlogloss:0.336058\n",
      "[94]\ttrain-mlogloss:0.293231\ttest-mlogloss:0.335899\n",
      "[95]\ttrain-mlogloss:0.29262\ttest-mlogloss:0.335692\n",
      "[96]\ttrain-mlogloss:0.292116\ttest-mlogloss:0.335511\n",
      "[97]\ttrain-mlogloss:0.291575\ttest-mlogloss:0.335258\n",
      "[98]\ttrain-mlogloss:0.291008\ttest-mlogloss:0.335038\n",
      "[99]\ttrain-mlogloss:0.290621\ttest-mlogloss:0.334904\n",
      "[100]\ttrain-mlogloss:0.290158\ttest-mlogloss:0.334801\n",
      "[101]\ttrain-mlogloss:0.289555\ttest-mlogloss:0.334543\n",
      "[102]\ttrain-mlogloss:0.28906\ttest-mlogloss:0.334467\n",
      "[103]\ttrain-mlogloss:0.288533\ttest-mlogloss:0.334321\n",
      "[104]\ttrain-mlogloss:0.287963\ttest-mlogloss:0.334069\n",
      "[105]\ttrain-mlogloss:0.287434\ttest-mlogloss:0.333859\n",
      "[106]\ttrain-mlogloss:0.287002\ttest-mlogloss:0.333796\n",
      "[107]\ttrain-mlogloss:0.286375\ttest-mlogloss:0.333588\n",
      "[108]\ttrain-mlogloss:0.285917\ttest-mlogloss:0.333486\n",
      "[109]\ttrain-mlogloss:0.285371\ttest-mlogloss:0.333315\n",
      "[110]\ttrain-mlogloss:0.285011\ttest-mlogloss:0.333114\n",
      "[111]\ttrain-mlogloss:0.284494\ttest-mlogloss:0.333023\n",
      "[112]\ttrain-mlogloss:0.283938\ttest-mlogloss:0.332901\n",
      "[113]\ttrain-mlogloss:0.283406\ttest-mlogloss:0.332726\n",
      "[114]\ttrain-mlogloss:0.283001\ttest-mlogloss:0.332596\n",
      "[115]\ttrain-mlogloss:0.282554\ttest-mlogloss:0.332448\n",
      "[116]\ttrain-mlogloss:0.282152\ttest-mlogloss:0.332277\n",
      "[117]\ttrain-mlogloss:0.281537\ttest-mlogloss:0.332124\n",
      "[118]\ttrain-mlogloss:0.281093\ttest-mlogloss:0.331925\n",
      "[119]\ttrain-mlogloss:0.280709\ttest-mlogloss:0.331762\n",
      "[120]\ttrain-mlogloss:0.280217\ttest-mlogloss:0.331616\n",
      "[121]\ttrain-mlogloss:0.27983\ttest-mlogloss:0.33155\n",
      "[122]\ttrain-mlogloss:0.279443\ttest-mlogloss:0.331384\n",
      "[123]\ttrain-mlogloss:0.279089\ttest-mlogloss:0.331406\n",
      "[124]\ttrain-mlogloss:0.278686\ttest-mlogloss:0.331292\n",
      "[125]\ttrain-mlogloss:0.278252\ttest-mlogloss:0.331209\n",
      "[126]\ttrain-mlogloss:0.277738\ttest-mlogloss:0.330999\n",
      "[127]\ttrain-mlogloss:0.277313\ttest-mlogloss:0.330964\n",
      "[128]\ttrain-mlogloss:0.276792\ttest-mlogloss:0.330778\n",
      "[129]\ttrain-mlogloss:0.276405\ttest-mlogloss:0.330739\n",
      "[130]\ttrain-mlogloss:0.276034\ttest-mlogloss:0.330857\n",
      "[131]\ttrain-mlogloss:0.275636\ttest-mlogloss:0.330819\n",
      "[132]\ttrain-mlogloss:0.275207\ttest-mlogloss:0.330757\n",
      "[133]\ttrain-mlogloss:0.274798\ttest-mlogloss:0.330501\n",
      "[134]\ttrain-mlogloss:0.274407\ttest-mlogloss:0.330361\n",
      "[135]\ttrain-mlogloss:0.273933\ttest-mlogloss:0.33038\n",
      "[136]\ttrain-mlogloss:0.273449\ttest-mlogloss:0.330412\n",
      "[137]\ttrain-mlogloss:0.273136\ttest-mlogloss:0.330359\n",
      "[138]\ttrain-mlogloss:0.272723\ttest-mlogloss:0.330404\n",
      "[139]\ttrain-mlogloss:0.272367\ttest-mlogloss:0.330456\n",
      "[140]\ttrain-mlogloss:0.27197\ttest-mlogloss:0.330482\n",
      "[141]\ttrain-mlogloss:0.271603\ttest-mlogloss:0.330529\n",
      "[142]\ttrain-mlogloss:0.271184\ttest-mlogloss:0.330509\n",
      "[143]\ttrain-mlogloss:0.270792\ttest-mlogloss:0.330523\n",
      "[144]\ttrain-mlogloss:0.270361\ttest-mlogloss:0.330293\n",
      "[145]\ttrain-mlogloss:0.269941\ttest-mlogloss:0.33011\n",
      "[146]\ttrain-mlogloss:0.269596\ttest-mlogloss:0.329972\n",
      "[147]\ttrain-mlogloss:0.269206\ttest-mlogloss:0.329806\n",
      "[148]\ttrain-mlogloss:0.268801\ttest-mlogloss:0.329703\n",
      "[149]\ttrain-mlogloss:0.268407\ttest-mlogloss:0.329689\n",
      "[150]\ttrain-mlogloss:0.268025\ttest-mlogloss:0.329696\n",
      "[151]\ttrain-mlogloss:0.267601\ttest-mlogloss:0.329559\n",
      "[152]\ttrain-mlogloss:0.267266\ttest-mlogloss:0.329505\n",
      "[153]\ttrain-mlogloss:0.266843\ttest-mlogloss:0.329441\n",
      "[154]\ttrain-mlogloss:0.266478\ttest-mlogloss:0.329432\n",
      "[155]\ttrain-mlogloss:0.266197\ttest-mlogloss:0.329371\n",
      "[156]\ttrain-mlogloss:0.265811\ttest-mlogloss:0.329354\n",
      "[157]\ttrain-mlogloss:0.265426\ttest-mlogloss:0.329231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158]\ttrain-mlogloss:0.265062\ttest-mlogloss:0.329184\n",
      "[159]\ttrain-mlogloss:0.264783\ttest-mlogloss:0.329138\n",
      "[160]\ttrain-mlogloss:0.264363\ttest-mlogloss:0.329166\n",
      "[161]\ttrain-mlogloss:0.264102\ttest-mlogloss:0.329087\n",
      "[162]\ttrain-mlogloss:0.263768\ttest-mlogloss:0.329008\n",
      "[163]\ttrain-mlogloss:0.263426\ttest-mlogloss:0.328956\n",
      "[164]\ttrain-mlogloss:0.262933\ttest-mlogloss:0.328963\n",
      "[165]\ttrain-mlogloss:0.262586\ttest-mlogloss:0.328765\n",
      "[166]\ttrain-mlogloss:0.262196\ttest-mlogloss:0.328762\n",
      "[167]\ttrain-mlogloss:0.261851\ttest-mlogloss:0.328684\n",
      "[168]\ttrain-mlogloss:0.261571\ttest-mlogloss:0.328725\n",
      "[169]\ttrain-mlogloss:0.26122\ttest-mlogloss:0.328756\n",
      "[170]\ttrain-mlogloss:0.260942\ttest-mlogloss:0.328775\n",
      "[171]\ttrain-mlogloss:0.260638\ttest-mlogloss:0.328852\n",
      "[172]\ttrain-mlogloss:0.260242\ttest-mlogloss:0.328796\n",
      "[173]\ttrain-mlogloss:0.259864\ttest-mlogloss:0.328758\n",
      "[174]\ttrain-mlogloss:0.25949\ttest-mlogloss:0.328719\n",
      "[175]\ttrain-mlogloss:0.259133\ttest-mlogloss:0.328689\n",
      "[176]\ttrain-mlogloss:0.258801\ttest-mlogloss:0.328684\n",
      "[177]\ttrain-mlogloss:0.258406\ttest-mlogloss:0.328656\n",
      "[178]\ttrain-mlogloss:0.258058\ttest-mlogloss:0.328553\n",
      "[179]\ttrain-mlogloss:0.257809\ttest-mlogloss:0.328537\n",
      "[180]\ttrain-mlogloss:0.257542\ttest-mlogloss:0.328509\n",
      "[181]\ttrain-mlogloss:0.257138\ttest-mlogloss:0.328314\n",
      "[182]\ttrain-mlogloss:0.256819\ttest-mlogloss:0.328378\n",
      "[183]\ttrain-mlogloss:0.256489\ttest-mlogloss:0.328309\n",
      "[184]\ttrain-mlogloss:0.256191\ttest-mlogloss:0.328303\n",
      "[185]\ttrain-mlogloss:0.255863\ttest-mlogloss:0.328286\n",
      "[186]\ttrain-mlogloss:0.255521\ttest-mlogloss:0.328164\n",
      "[187]\ttrain-mlogloss:0.255207\ttest-mlogloss:0.328122\n",
      "[188]\ttrain-mlogloss:0.254821\ttest-mlogloss:0.327938\n",
      "[189]\ttrain-mlogloss:0.254453\ttest-mlogloss:0.327873\n",
      "[190]\ttrain-mlogloss:0.254038\ttest-mlogloss:0.327738\n",
      "[191]\ttrain-mlogloss:0.253683\ttest-mlogloss:0.327678\n",
      "[192]\ttrain-mlogloss:0.253394\ttest-mlogloss:0.327645\n",
      "[193]\ttrain-mlogloss:0.25312\ttest-mlogloss:0.327614\n",
      "[194]\ttrain-mlogloss:0.252756\ttest-mlogloss:0.327683\n",
      "[195]\ttrain-mlogloss:0.252433\ttest-mlogloss:0.327634\n",
      "[196]\ttrain-mlogloss:0.252152\ttest-mlogloss:0.327689\n",
      "[197]\ttrain-mlogloss:0.251768\ttest-mlogloss:0.327594\n",
      "[198]\ttrain-mlogloss:0.251428\ttest-mlogloss:0.327529\n",
      "[199]\ttrain-mlogloss:0.251015\ttest-mlogloss:0.327386\n",
      "[200]\ttrain-mlogloss:0.250701\ttest-mlogloss:0.327445\n",
      "[201]\ttrain-mlogloss:0.25041\ttest-mlogloss:0.32743\n",
      "[202]\ttrain-mlogloss:0.250129\ttest-mlogloss:0.32728\n",
      "[203]\ttrain-mlogloss:0.249814\ttest-mlogloss:0.327287\n",
      "[204]\ttrain-mlogloss:0.249528\ttest-mlogloss:0.327227\n",
      "[205]\ttrain-mlogloss:0.249218\ttest-mlogloss:0.327203\n",
      "[206]\ttrain-mlogloss:0.248865\ttest-mlogloss:0.327242\n",
      "[207]\ttrain-mlogloss:0.24862\ttest-mlogloss:0.327242\n",
      "[208]\ttrain-mlogloss:0.248402\ttest-mlogloss:0.327198\n",
      "[209]\ttrain-mlogloss:0.248142\ttest-mlogloss:0.327112\n",
      "[210]\ttrain-mlogloss:0.247833\ttest-mlogloss:0.327158\n",
      "[211]\ttrain-mlogloss:0.247543\ttest-mlogloss:0.327034\n",
      "[212]\ttrain-mlogloss:0.247225\ttest-mlogloss:0.32705\n",
      "[213]\ttrain-mlogloss:0.246954\ttest-mlogloss:0.326999\n",
      "[214]\ttrain-mlogloss:0.246732\ttest-mlogloss:0.326953\n",
      "[215]\ttrain-mlogloss:0.246392\ttest-mlogloss:0.326941\n",
      "[216]\ttrain-mlogloss:0.246048\ttest-mlogloss:0.326884\n",
      "[217]\ttrain-mlogloss:0.245797\ttest-mlogloss:0.326849\n",
      "[218]\ttrain-mlogloss:0.24548\ttest-mlogloss:0.326684\n",
      "[219]\ttrain-mlogloss:0.245182\ttest-mlogloss:0.326643\n",
      "[220]\ttrain-mlogloss:0.244847\ttest-mlogloss:0.326581\n",
      "[221]\ttrain-mlogloss:0.244566\ttest-mlogloss:0.326536\n",
      "[222]\ttrain-mlogloss:0.244272\ttest-mlogloss:0.326433\n",
      "[223]\ttrain-mlogloss:0.243941\ttest-mlogloss:0.326509\n",
      "[224]\ttrain-mlogloss:0.243616\ttest-mlogloss:0.32657\n",
      "[225]\ttrain-mlogloss:0.243313\ttest-mlogloss:0.326535\n",
      "[226]\ttrain-mlogloss:0.243044\ttest-mlogloss:0.326591\n",
      "[227]\ttrain-mlogloss:0.242821\ttest-mlogloss:0.326696\n",
      "[228]\ttrain-mlogloss:0.242637\ttest-mlogloss:0.326652\n",
      "[229]\ttrain-mlogloss:0.24241\ttest-mlogloss:0.326645\n",
      "[230]\ttrain-mlogloss:0.242069\ttest-mlogloss:0.326719\n",
      "[231]\ttrain-mlogloss:0.241806\ttest-mlogloss:0.326576\n",
      "[232]\ttrain-mlogloss:0.241552\ttest-mlogloss:0.326578\n",
      "[233]\ttrain-mlogloss:0.241308\ttest-mlogloss:0.32661\n",
      "[234]\ttrain-mlogloss:0.241067\ttest-mlogloss:0.326631\n",
      "[235]\ttrain-mlogloss:0.2408\ttest-mlogloss:0.32665\n",
      "[236]\ttrain-mlogloss:0.240528\ttest-mlogloss:0.326686\n",
      "[237]\ttrain-mlogloss:0.24025\ttest-mlogloss:0.326748\n",
      "[238]\ttrain-mlogloss:0.239962\ttest-mlogloss:0.326707\n",
      "[239]\ttrain-mlogloss:0.239748\ttest-mlogloss:0.326819\n",
      "Test error using softprob = 0.12751106571331292\n"
     ]
    }
   ],
   "source": [
    "# probabilities output\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_test).reshape(y_test.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != y_test) / y_test.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.921818\ttest-mlogloss:0.921863\n",
      "[1]\ttrain-mlogloss:0.79919\ttest-mlogloss:0.799002\n",
      "[2]\ttrain-mlogloss:0.709528\ttest-mlogloss:0.70911\n",
      "[3]\ttrain-mlogloss:0.641847\ttest-mlogloss:0.641432\n",
      "[4]\ttrain-mlogloss:0.58975\ttest-mlogloss:0.589307\n",
      "[5]\ttrain-mlogloss:0.549532\ttest-mlogloss:0.548942\n",
      "[6]\ttrain-mlogloss:0.51771\ttest-mlogloss:0.516905\n",
      "[7]\ttrain-mlogloss:0.492063\ttest-mlogloss:0.491249\n",
      "[8]\ttrain-mlogloss:0.470918\ttest-mlogloss:0.470304\n",
      "[9]\ttrain-mlogloss:0.453839\ttest-mlogloss:0.453268\n",
      "[10]\ttrain-mlogloss:0.439633\ttest-mlogloss:0.439011\n",
      "[11]\ttrain-mlogloss:0.42799\ttest-mlogloss:0.427375\n",
      "[12]\ttrain-mlogloss:0.418099\ttest-mlogloss:0.41718\n",
      "[13]\ttrain-mlogloss:0.409744\ttest-mlogloss:0.408586\n",
      "[14]\ttrain-mlogloss:0.402601\ttest-mlogloss:0.401298\n",
      "[15]\ttrain-mlogloss:0.396384\ttest-mlogloss:0.394931\n",
      "[16]\ttrain-mlogloss:0.391226\ttest-mlogloss:0.389685\n",
      "[17]\ttrain-mlogloss:0.386594\ttest-mlogloss:0.384904\n",
      "[18]\ttrain-mlogloss:0.382438\ttest-mlogloss:0.380924\n",
      "[19]\ttrain-mlogloss:0.378784\ttest-mlogloss:0.377231\n",
      "[20]\ttrain-mlogloss:0.375561\ttest-mlogloss:0.374344\n",
      "[21]\ttrain-mlogloss:0.372713\ttest-mlogloss:0.371459\n",
      "[22]\ttrain-mlogloss:0.369994\ttest-mlogloss:0.368769\n",
      "[23]\ttrain-mlogloss:0.367495\ttest-mlogloss:0.366415\n",
      "[24]\ttrain-mlogloss:0.365086\ttest-mlogloss:0.364174\n",
      "[25]\ttrain-mlogloss:0.363041\ttest-mlogloss:0.362072\n",
      "[26]\ttrain-mlogloss:0.360894\ttest-mlogloss:0.360286\n",
      "[27]\ttrain-mlogloss:0.358427\ttest-mlogloss:0.357889\n",
      "[28]\ttrain-mlogloss:0.356686\ttest-mlogloss:0.356236\n",
      "[29]\ttrain-mlogloss:0.354958\ttest-mlogloss:0.354772\n",
      "[30]\ttrain-mlogloss:0.353042\ttest-mlogloss:0.3529\n",
      "[31]\ttrain-mlogloss:0.351534\ttest-mlogloss:0.351453\n",
      "[32]\ttrain-mlogloss:0.350061\ttest-mlogloss:0.349966\n",
      "[33]\ttrain-mlogloss:0.348746\ttest-mlogloss:0.348559\n",
      "[34]\ttrain-mlogloss:0.347125\ttest-mlogloss:0.347208\n",
      "[35]\ttrain-mlogloss:0.345824\ttest-mlogloss:0.346096\n",
      "[36]\ttrain-mlogloss:0.344629\ttest-mlogloss:0.344921\n",
      "[37]\ttrain-mlogloss:0.343449\ttest-mlogloss:0.3438\n",
      "[38]\ttrain-mlogloss:0.342153\ttest-mlogloss:0.342593\n",
      "[39]\ttrain-mlogloss:0.340765\ttest-mlogloss:0.341417\n",
      "[40]\ttrain-mlogloss:0.339714\ttest-mlogloss:0.340368\n",
      "[41]\ttrain-mlogloss:0.338548\ttest-mlogloss:0.339423\n",
      "[42]\ttrain-mlogloss:0.337546\ttest-mlogloss:0.338412\n",
      "[43]\ttrain-mlogloss:0.336309\ttest-mlogloss:0.337171\n",
      "[44]\ttrain-mlogloss:0.335438\ttest-mlogloss:0.33627\n",
      "[45]\ttrain-mlogloss:0.334524\ttest-mlogloss:0.335482\n",
      "[46]\ttrain-mlogloss:0.333505\ttest-mlogloss:0.334478\n",
      "[47]\ttrain-mlogloss:0.332605\ttest-mlogloss:0.333578\n",
      "[48]\ttrain-mlogloss:0.331549\ttest-mlogloss:0.332439\n",
      "[49]\ttrain-mlogloss:0.330689\ttest-mlogloss:0.331511\n",
      "[50]\ttrain-mlogloss:0.329738\ttest-mlogloss:0.330761\n",
      "[51]\ttrain-mlogloss:0.328878\ttest-mlogloss:0.329782\n",
      "[52]\ttrain-mlogloss:0.328087\ttest-mlogloss:0.329092\n",
      "[53]\ttrain-mlogloss:0.327228\ttest-mlogloss:0.32823\n",
      "[54]\ttrain-mlogloss:0.326369\ttest-mlogloss:0.327479\n",
      "[55]\ttrain-mlogloss:0.325674\ttest-mlogloss:0.326782\n",
      "[56]\ttrain-mlogloss:0.324773\ttest-mlogloss:0.325986\n",
      "[57]\ttrain-mlogloss:0.324038\ttest-mlogloss:0.325258\n",
      "[58]\ttrain-mlogloss:0.323447\ttest-mlogloss:0.324665\n",
      "[59]\ttrain-mlogloss:0.322686\ttest-mlogloss:0.323971\n",
      "[60]\ttrain-mlogloss:0.321956\ttest-mlogloss:0.323325\n",
      "[61]\ttrain-mlogloss:0.321187\ttest-mlogloss:0.322695\n",
      "[62]\ttrain-mlogloss:0.320563\ttest-mlogloss:0.32214\n",
      "[63]\ttrain-mlogloss:0.319943\ttest-mlogloss:0.321597\n",
      "[64]\ttrain-mlogloss:0.31947\ttest-mlogloss:0.321042\n",
      "[65]\ttrain-mlogloss:0.318853\ttest-mlogloss:0.320398\n",
      "[66]\ttrain-mlogloss:0.318079\ttest-mlogloss:0.319816\n",
      "[67]\ttrain-mlogloss:0.317387\ttest-mlogloss:0.31921\n",
      "[68]\ttrain-mlogloss:0.316848\ttest-mlogloss:0.318509\n",
      "[69]\ttrain-mlogloss:0.316166\ttest-mlogloss:0.317913\n",
      "[70]\ttrain-mlogloss:0.31537\ttest-mlogloss:0.317283\n",
      "[71]\ttrain-mlogloss:0.314847\ttest-mlogloss:0.316713\n",
      "[72]\ttrain-mlogloss:0.314291\ttest-mlogloss:0.316135\n",
      "[73]\ttrain-mlogloss:0.313745\ttest-mlogloss:0.315715\n",
      "[74]\ttrain-mlogloss:0.313133\ttest-mlogloss:0.315104\n",
      "[75]\ttrain-mlogloss:0.312497\ttest-mlogloss:0.314502\n",
      "[76]\ttrain-mlogloss:0.311727\ttest-mlogloss:0.313871\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-dc91d9f41da0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# do the same thing again, but output probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'objective'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'multi:softprob'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbstp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxg_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwatchlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Note: this convention has been changed since xgboost-unity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# get prediction, this is in 1D array, need reshape to (ndata, nclass)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_t, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_t).reshape(Y.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != Y) / Y.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_43</th>\n",
       "      <th>svd_word_44</th>\n",
       "      <th>svd_word_45</th>\n",
       "      <th>svd_word_46</th>\n",
       "      <th>svd_word_47</th>\n",
       "      <th>svd_word_48</th>\n",
       "      <th>svd_word_49</th>\n",
       "      <th>nb_cvec_eap</th>\n",
       "      <th>nb_cvec_hpl</th>\n",
       "      <th>nb_cvec_mws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006320</td>\n",
       "      <td>0.019999</td>\n",
       "      <td>-0.006687</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>-0.046846</td>\n",
       "      <td>-0.042702</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.978387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>-0.000610</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>-0.001725</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>-0.006745</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007646</td>\n",
       "      <td>-0.004253</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>-0.012464</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>0.217325</td>\n",
       "      <td>0.782527</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7     ...       svd_word_43  svd_word_44  svd_word_45  \\\n",
       "0    0.003240     ...         -0.006320     0.019999    -0.006687   \n",
       "1   -0.000215     ...          0.003998    -0.000610     0.001042   \n",
       "2   -0.004598     ...          0.007646    -0.004253     0.031135   \n",
       "\n",
       "   svd_word_46  svd_word_47  svd_word_48  svd_word_49  nb_cvec_eap  \\\n",
       "0     0.008188    -0.046846    -0.042702     0.000003     0.021018   \n",
       "1    -0.000802    -0.001725     0.004586    -0.006745     0.999985   \n",
       "2     0.000088    -0.001402    -0.012464     0.007291     0.217325   \n",
       "\n",
       "   nb_cvec_hpl  nb_cvec_mws  \n",
       "0     0.000595     0.978387  \n",
       "1     0.000009     0.000006  \n",
       "2     0.782527     0.000148  \n",
       "\n",
       "[3 rows x 55 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_45</th>\n",
       "      <th>svd_word_46</th>\n",
       "      <th>svd_word_47</th>\n",
       "      <th>svd_word_48</th>\n",
       "      <th>svd_word_49</th>\n",
       "      <th>nb_cvec_eap</th>\n",
       "      <th>nb_cvec_hpl</th>\n",
       "      <th>nb_cvec_mws</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006687</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>-0.046846</td>\n",
       "      <td>-0.042702</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.978387</td>\n",
       "      <td>[still, i, urg, leav, ireland, inquietud, impa...</td>\n",
       "      <td>still i urg leav ireland inquietud impati fath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>-0.001725</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>-0.006745</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>[if, fire, want, fan, could, readili, fan, new...</td>\n",
       "      <td>if fire want fan could readili fan newspap gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031135</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>-0.012464</td>\n",
       "      <td>0.007291</td>\n",
       "      <td>0.217325</td>\n",
       "      <td>0.782527</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>[and, broken, frail, door, found, two, clean, ...</td>\n",
       "      <td>and broken frail door found two clean pick hum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7                        ...                          svd_word_45  \\\n",
       "0    0.003240                        ...                            -0.006687   \n",
       "1   -0.000215                        ...                             0.001042   \n",
       "2   -0.004598                        ...                             0.031135   \n",
       "\n",
       "   svd_word_46  svd_word_47  svd_word_48  svd_word_49  nb_cvec_eap  \\\n",
       "0     0.008188    -0.046846    -0.042702     0.000003     0.021018   \n",
       "1    -0.000802    -0.001725     0.004586    -0.006745     0.999985   \n",
       "2     0.000088    -0.001402    -0.012464     0.007291     0.217325   \n",
       "\n",
       "   nb_cvec_hpl  nb_cvec_mws  \\\n",
       "0     0.000595     0.978387   \n",
       "1     0.000009     0.000006   \n",
       "2     0.782527     0.000148   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [still, i, urg, leav, ireland, inquietud, impa...   \n",
       "1  [if, fire, want, fan, could, readili, fan, new...   \n",
       "2  [and, broken, frail, door, found, two, clean, ...   \n",
       "\n",
       "                                 cleaned_text_string  \n",
       "0  still i urg leav ireland inquietud impati fath...  \n",
       "1  if fire want fan could readili fan newspap gov...  \n",
       "2  and broken frail door found two clean pick hum...  \n",
       "\n",
       "[3 rows x 57 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['cleaned_text'] = df_test.text.apply(tokenize_stem)\n",
    "df_test['cleaned_text_string'] = df_test.cleaned_text.apply(' '.join)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>2.470797</td>\n",
       "      <td>0.148157</td>\n",
       "      <td>4.902270</td>\n",
       "      <td>0.550620</td>\n",
       "      <td>0.620488</td>\n",
       "      <td>0.293736</td>\n",
       "      <td>0.499431</td>\n",
       "      <td>-3.060433</td>\n",
       "      <td>-1.231265</td>\n",
       "      <td>1.104716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>6.992887</td>\n",
       "      <td>-0.361119</td>\n",
       "      <td>10.840488</td>\n",
       "      <td>0.832574</td>\n",
       "      <td>1.976278</td>\n",
       "      <td>1.081327</td>\n",
       "      <td>-0.232783</td>\n",
       "      <td>-7.698443</td>\n",
       "      <td>-2.857432</td>\n",
       "      <td>1.991308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>5.884607</td>\n",
       "      <td>-1.001613</td>\n",
       "      <td>6.743172</td>\n",
       "      <td>0.213241</td>\n",
       "      <td>1.738426</td>\n",
       "      <td>0.930032</td>\n",
       "      <td>-1.557037</td>\n",
       "      <td>-5.864972</td>\n",
       "      <td>-1.940434</td>\n",
       "      <td>0.822325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7       ...        w2v_feature_90  w2v_feature_91  w2v_feature_92  \\\n",
       "0    0.003240       ...              2.470797        0.148157        4.902270   \n",
       "1   -0.000215       ...              6.992887       -0.361119       10.840488   \n",
       "2   -0.004598       ...              5.884607       -1.001613        6.743172   \n",
       "\n",
       "   w2v_feature_93  w2v_feature_94  w2v_feature_95  w2v_feature_96  \\\n",
       "0        0.550620        0.620488        0.293736        0.499431   \n",
       "1        0.832574        1.976278        1.081327       -0.232783   \n",
       "2        0.213241        1.738426        0.930032       -1.557037   \n",
       "\n",
       "   w2v_feature_97  w2v_feature_98  w2v_feature_99  \n",
       "0       -3.060433       -1.231265        1.104716  \n",
       "1       -7.698443       -2.857432        1.991308  \n",
       "2       -5.864972       -1.940434        0.822325  \n",
       "\n",
       "[3 rows x 247 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['length']=df_test['cleaned_text_string'].apply(len)\n",
    "df_test[\"num_words\"] = df_test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_test[\"num_unique_words\"] = df_test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_test[\"num_punctuations\"] =df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test[\"num_words_upper\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_test[\"num_words_title\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_test[\"mean_word_len\"] = df_test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test[\"num_stopwords\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_test['lexical_diversity'] = df_test.text.apply(lexical_diversity)\n",
    "df_test['w2v_array'] = df_test.cleaned_text.apply(sum_up_word2vec_array)\n",
    "count_topwords(df_test)\n",
    "create_w2v_columns(df_test) \n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550620</td>\n",
       "      <td>0.620488</td>\n",
       "      <td>0.293736</td>\n",
       "      <td>0.499431</td>\n",
       "      <td>-3.060433</td>\n",
       "      <td>-1.231265</td>\n",
       "      <td>1.104716</td>\n",
       "      <td>0.071473</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>0.026541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832574</td>\n",
       "      <td>1.976278</td>\n",
       "      <td>1.081327</td>\n",
       "      <td>-0.232783</td>\n",
       "      <td>-7.698443</td>\n",
       "      <td>-2.857432</td>\n",
       "      <td>1.991308</td>\n",
       "      <td>0.035932</td>\n",
       "      <td>0.062884</td>\n",
       "      <td>0.039306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213241</td>\n",
       "      <td>1.738426</td>\n",
       "      <td>0.930032</td>\n",
       "      <td>-1.557037</td>\n",
       "      <td>-5.864972</td>\n",
       "      <td>-1.940434</td>\n",
       "      <td>0.822325</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.057723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "\n",
       "   svd_word_7    ...      w2v_feature_93  w2v_feature_94  w2v_feature_95  \\\n",
       "0    0.003240    ...            0.550620        0.620488        0.293736   \n",
       "1   -0.000215    ...            0.832574        1.976278        1.081327   \n",
       "2   -0.004598    ...            0.213241        1.738426        0.930032   \n",
       "\n",
       "   w2v_feature_96  w2v_feature_97  w2v_feature_98  w2v_feature_99  mws_index  \\\n",
       "0        0.499431       -3.060433       -1.231265        1.104716   0.071473   \n",
       "1       -0.232783       -7.698443       -2.857432        1.991308   0.035932   \n",
       "2       -1.557037       -5.864972       -1.940434        0.822325   0.027486   \n",
       "\n",
       "   eap_index  hpl_index  \n",
       "0   0.036315   0.026541  \n",
       "1   0.062884   0.039306  \n",
       "2   0.055142   0.057723  \n",
       "\n",
       "[3 rows x 250 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['mws_index']=df_test['cleaned_text'].apply(ind_val_mws)/df_test['length']\n",
    "df_test['eap_index']=df_test['cleaned_text'].apply(ind_val_eap)/df_test['length']\n",
    "df_test['hpl_index']=df_test['cleaned_text'].apply(ind_val_hpl)/df_test['length']\n",
    "df_test.head(n=3)\n",
    "# df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_test['cleaned_text']\n",
    "del df_test['cleaned_text_string']\n",
    "del df_test['w2v_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author2', 'author', 'cleaned_text', 'cleaned_text_string']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=(df_train.columns.tolist())[6:]\n",
    "[item for item in df_train.columns.tolist() if item not in df_test.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>svd_word_0</th>\n",
       "      <th>svd_word_1</th>\n",
       "      <th>svd_word_2</th>\n",
       "      <th>svd_word_3</th>\n",
       "      <th>svd_word_4</th>\n",
       "      <th>svd_word_5</th>\n",
       "      <th>svd_word_6</th>\n",
       "      <th>svd_word_7</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>-0.010185</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-0.005363</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550620</td>\n",
       "      <td>0.620488</td>\n",
       "      <td>0.293736</td>\n",
       "      <td>0.499431</td>\n",
       "      <td>-3.060433</td>\n",
       "      <td>-1.231265</td>\n",
       "      <td>1.104716</td>\n",
       "      <td>0.071473</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>0.026541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>0.022294</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.012514</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.009725</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832574</td>\n",
       "      <td>1.976278</td>\n",
       "      <td>1.081327</td>\n",
       "      <td>-0.232783</td>\n",
       "      <td>-7.698443</td>\n",
       "      <td>-2.857432</td>\n",
       "      <td>1.991308</td>\n",
       "      <td>0.035932</td>\n",
       "      <td>0.062884</td>\n",
       "      <td>0.039306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>-0.008934</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>-0.005058</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213241</td>\n",
       "      <td>1.738426</td>\n",
       "      <td>0.930032</td>\n",
       "      <td>-1.557037</td>\n",
       "      <td>-5.864972</td>\n",
       "      <td>-1.940434</td>\n",
       "      <td>0.822325</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.057723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "      <td>0.013408</td>\n",
       "      <td>-0.007515</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.004020</td>\n",
       "      <td>-0.004521</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>-0.002882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739516</td>\n",
       "      <td>1.780698</td>\n",
       "      <td>1.040653</td>\n",
       "      <td>-0.705218</td>\n",
       "      <td>-7.231385</td>\n",
       "      <td>-2.711493</td>\n",
       "      <td>1.857348</td>\n",
       "      <td>0.024369</td>\n",
       "      <td>0.065057</td>\n",
       "      <td>0.055736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>-0.003185</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.001152</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>-0.006110</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>-0.002149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410158</td>\n",
       "      <td>0.281921</td>\n",
       "      <td>0.229973</td>\n",
       "      <td>0.561596</td>\n",
       "      <td>-1.859675</td>\n",
       "      <td>-0.814361</td>\n",
       "      <td>0.731755</td>\n",
       "      <td>0.025615</td>\n",
       "      <td>0.070948</td>\n",
       "      <td>0.028437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  svd_word_0  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...    0.024516   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...    0.022294   \n",
       "2  id00134  And when they had broken down the frail door t...    0.016906   \n",
       "3  id27757  While I was thinking how I should possibly man...    0.013408   \n",
       "4  id04081  I am not sure to what limit his knowledge may ...    0.012565   \n",
       "\n",
       "   svd_word_1  svd_word_2  svd_word_3  svd_word_4  svd_word_5  svd_word_6  \\\n",
       "0   -0.010185    0.001168   -0.005363   -0.013319   -0.003444   -0.002816   \n",
       "1   -0.011968   -0.001596   -0.004478   -0.012514   -0.000641   -0.009725   \n",
       "2   -0.008934    0.000240   -0.006892   -0.008843    0.004787   -0.005058   \n",
       "3   -0.007515   -0.000154   -0.004020   -0.004521    0.002712   -0.004220   \n",
       "4   -0.003185   -0.000719   -0.001152    0.000930   -0.006110    0.001690   \n",
       "\n",
       "   svd_word_7    ...      w2v_feature_93  w2v_feature_94  w2v_feature_95  \\\n",
       "0    0.003240    ...            0.550620        0.620488        0.293736   \n",
       "1   -0.000215    ...            0.832574        1.976278        1.081327   \n",
       "2   -0.004598    ...            0.213241        1.738426        0.930032   \n",
       "3   -0.002882    ...            0.739516        1.780698        1.040653   \n",
       "4   -0.002149    ...            0.410158        0.281921        0.229973   \n",
       "\n",
       "   w2v_feature_96  w2v_feature_97  w2v_feature_98  w2v_feature_99  mws_index  \\\n",
       "0        0.499431       -3.060433       -1.231265        1.104716   0.071473   \n",
       "1       -0.232783       -7.698443       -2.857432        1.991308   0.035932   \n",
       "2       -1.557037       -5.864972       -1.940434        0.822325   0.027486   \n",
       "3       -0.705218       -7.231385       -2.711493        1.857348   0.024369   \n",
       "4        0.561596       -1.859675       -0.814361        0.731755   0.025615   \n",
       "\n",
       "   eap_index  hpl_index  \n",
       "0   0.036315   0.026541  \n",
       "1   0.062884   0.039306  \n",
       "2   0.055142   0.057723  \n",
       "3   0.065057   0.055736  \n",
       "4   0.070948   0.028437  \n",
       "\n",
       "[5 rows x 247 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_test=df_test[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.44851837e-02   5.50276274e-03   9.60012019e-01]\n",
      " [  9.96701777e-01   2.07544537e-03   1.22278044e-03]\n",
      " [  6.28928840e-02   9.35769796e-01   1.33732741e-03]\n",
      " ..., \n",
      " [  9.06760871e-01   5.87458573e-02   3.44932452e-02]\n",
      " [  2.11261455e-02   1.50194473e-03   9.77371871e-01]\n",
      " [  1.41863478e-02   9.85448301e-01   3.65293323e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.034485</td>\n",
       "      <td>0.005503</td>\n",
       "      <td>0.960012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.996702</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.001223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.062893</td>\n",
       "      <td>0.935770</td>\n",
       "      <td>0.001337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.809090</td>\n",
       "      <td>0.188739</td>\n",
       "      <td>0.002171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.978529</td>\n",
       "      <td>0.016969</td>\n",
       "      <td>0.004502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.034485  0.005503  0.960012\n",
       "1  id24541  0.996702  0.002075  0.001223\n",
       "2  id00134  0.062893  0.935770  0.001337\n",
       "3  id27757  0.809090  0.188739  0.002171\n",
       "4  id04081  0.978529  0.016969  0.004502"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export=pd.DataFrame(pred_prob)\n",
    "export.insert(loc=0, column='id', value=y_t)\n",
    "export.columns=['id','EAP', 'HPL', 'MWS']\n",
    "export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8392"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>id23301</td>\n",
       "      <td>0.337495</td>\n",
       "      <td>0.166845</td>\n",
       "      <td>0.49566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       EAP       HPL      MWS\n",
       "6106  id23301  0.337495  0.166845  0.49566"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export[export['id']=='id23301']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export.to_csv(path_or_buf=\"../data/export.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# попробуем теперь генсимовский лда\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set(stopwords.words('english')).union(set(get_stop_words('english')))\n",
    "\n",
    "\n",
    "words_exclude = set(eng_stopwords).union(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trying to create corpus as a list of words\n",
    "\n",
    "train_corpse = df_train['cleaned_text_string'].tolist()\n",
    "\n",
    "texts = [[word for word in document.lower().split() if word not in words_exclude] for document in train_corpse]\n",
    "dicccs = Dictionary(texts)\n",
    "\n",
    "\n",
    "corpus = [dicccs.doc2bow(text) for text in texts]\n",
    "\n",
    "# print(corpse[0:5])\n",
    "\n",
    "\n",
    "lda_model = LdaModel(corpus, id2word=dicccs, num_topics=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['process', 'howev', 'afford', 'mean', 'ascertain', 'dimens', 'dungeon', 'might', 'make', 'circuit', 'return', 'point', 'whenc', 'set', 'without', 'awar', 'fact', 'perfect', 'uniform', 'seem', 'wall']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.081501738548664951),\n",
       " (18, 0.39479289459575573),\n",
       " (19, 0.050642737227831379),\n",
       " (20, 0.37754174325765977),\n",
       " (24, 0.059157250001896047)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = dicccs.doc2bow(texts[0])\n",
    "\n",
    "lda_model.get_document_topics(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.60075588554034731),\n",
       " (5, 0.12909575248303104),\n",
       " (6, 0.086607033567281319),\n",
       " (7, 0.062167704750211973),\n",
       " (9, 0.052128071578546056),\n",
       " (23, 0.036202073815929887)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = dicccs.doc2bow(texts[15])\n",
    "\n",
    "lda_model.get_document_topics(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# поехали. времени мало\n",
    "\n",
    "def topicify_cleane_text_list(cl_tex_lst):\n",
    "    \n",
    "    cl_lst = [i for i in cl_tex_lst if i not in words_exclude]\n",
    "    \n",
    "    doc = dicccs.doc2bow(cl_lst)\n",
    "    \n",
    "    return lda_model.get_document_topics(doc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['topic_probs_lda'] = df_train.cleaned_text.apply(topicify_cleane_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "def split_topicdata(frm, topicnum):\n",
    "    \n",
    "    topic_probs = frm['topic_probs_lda'].tolist()\n",
    "    \n",
    "    rownum = len(topic_probs)\n",
    "    \n",
    "    generate_zeroes = [[0 for j in range(rownum)] for i in range(topicnum)]\n",
    "    \n",
    "    for tuplistnum, tuplist in enumerate(topic_probs):\n",
    "        for tup in tuplist:\n",
    "            \n",
    "            generate_zeroes[tup[0]][tuplistnum] = tup[1]\n",
    "            \n",
    "    return generate_zeroes\n",
    "\n",
    "\n",
    "test_run = split_topicdata(df_train, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for colnum, col in enumerate(test_run):\n",
    "    # print(len(col))\n",
    "    \n",
    "    df_train['lda_topic_' + str(colnum)] = col\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "df_test['cleaned_text'] = df_test.text.apply(tokenize_stem)\n",
    "\n",
    "df_test['topic_probs_lda'] = df_test.cleaned_text.apply(topicify_cleane_text_list)\n",
    "\n",
    "test_test_run = split_topicdata(df_test, 25)\n",
    "\n",
    "for colnum, col in enumerate(test_test_run):\n",
    "    # print(len(col))\n",
    "    \n",
    "    df_test['lda_topic_' + str(colnum)] = col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_test['topic_probs_lda']\n",
    "del df_train['topic_probs_lda']\n",
    "del df_test['cleaned_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
