{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Radames\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Radames\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "from string import digits\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes, pipeline\n",
    "from stop_words import get_stop_words\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "# 3 столбца - id, text, author\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нулей не должно быть!\n",
    "\n",
    "\n",
    "def sum_up_word2vec_array(clnd_text):\n",
    "    \n",
    "    total = None\n",
    "    \n",
    "    for word in clnd_text:\n",
    "        if word in w2v:\n",
    "            if total is None:\n",
    "                total = w2v[word]\n",
    "            else:\n",
    "                total = np.add(total, w2v[word])\n",
    "                \n",
    "    if total is None:\n",
    "        return np.zeros(w2v_array_len)\n",
    "    else:\n",
    "        return total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нулей не должно быть!\n",
    "\n",
    "\n",
    "def avg_up_word2vec_array(clnd_text):\n",
    "    \n",
    "    total = None\n",
    "    \n",
    "    for word in clnd_text:\n",
    "        if word in w2v:\n",
    "            if total is None:\n",
    "                total = w2v[word]\n",
    "            else:                \n",
    "                total = np.mean([total, w2v[word]], axis=0)\n",
    "                \n",
    "                \n",
    "    if total is None:\n",
    "        return np.zeros(w2v_array_len)\n",
    "    else:\n",
    "        return total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "def tokenize_stem(file_text):\n",
    "    #firstly let's apply nltk tokenization\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #deleting stop_words\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_text'] = df_train.text.apply(tokenize_stem)\n",
    "df_train['cleaned_text_string'] = df_train.cleaned_text.apply(' '.join)\n",
    "df_train.head(n=3)\n",
    "eng_stopwords = set(stopwords.words('english')).union(set(get_stop_words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(file_text):\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "    except:\n",
    "        nltk.download('punkt')\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "        \n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return len(set(tokens))/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[matrix([[ 0.00053369,  0.00053369,  0.00106738, ...,  0.00053369,\n",
      "          0.        ,  0.        ]]), matrix([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "          0.00135189,  0.00067595]]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "# вытаскиваем \"значимые\" слова\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "raw_documents_authors = ['', '', '']\n",
    "\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    if row['author'] == 'EAP':\n",
    "        raw_documents_authors[0] += row['cleaned_text_string'] + ' '\n",
    "    elif row['author'] == 'HPL':\n",
    "        raw_documents_authors[1] += row['cleaned_text_string'] + ' '\n",
    "    else:\n",
    "        raw_documents_authors[2] += row['cleaned_text_string'] + ' '\n",
    "        \n",
    "\n",
    "# удалим уникальные слова, не встречающиеся у других писателей\n",
    "\n",
    "eap_only = set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "hpl_only = set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "msh_only = set(raw_documents_authors[2].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' '))\n",
    "\n",
    "unique_words = eap_only.union(hpl_only).union(msh_only)\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word')\n",
    "idf_matrix =  tf.fit_transform(raw_documents_authors)\n",
    "feature_names = tf.get_feature_names()\n",
    "# dictionary_word = dict(zip(feature_names, idf_matrix))\n",
    "\n",
    "dense_idf = [i.todense() for i in idf_matrix]\n",
    "print(dense_idf)\n",
    "\n",
    "max_weighted_term = []\n",
    "\n",
    "eap_dense_list = dense_idf[0].tolist()[0]\n",
    "hpl_dense_list = dense_idf[1].tolist()[0]\n",
    "mws_dense_list = dense_idf[2].tolist()[0]\n",
    "\n",
    "for inum, i in enumerate(eap_dense_list):\n",
    "    max_weighted_term.append(max(hpl_dense_list[inum], mws_dense_list[inum], \n",
    "                             i))\n",
    "\n",
    "max_tf_dict = dict(zip(feature_names, max_weighted_term))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(df_train['cleaned_text'], size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_array_len = list(w2v.items())[0][1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_words(tfidfdict, numwrd):\n",
    "\n",
    "    top_word_dict, min_value, min_key = {}, 99, ''\n",
    "    \n",
    "\n",
    "    for k, v in max_tf_dict.items():\n",
    "        # print(top_word_dict.values())\n",
    "        # print(v)\n",
    "        if k not in unique_words and k not in eng_stopwords:\n",
    "        \n",
    "            if len(top_word_dict) < numwrd:\n",
    "                top_word_dict[k] = v\n",
    "                if v <= min_value:\n",
    "                    min_key = k\n",
    "            else:\n",
    "                # print(v, min(list(top_word_dict.values())))\n",
    "                if v > min(list(top_word_dict.values())) and k not in eng_stopwords:\n",
    "\n",
    "                    min_value = min(top_word_dict.values())\n",
    "\n",
    "                    for ky, va in top_word_dict.items():\n",
    "                        if va == min_value:\n",
    "                            min_key = ky\n",
    "\n",
    "                    top_word_dict.pop(min_key)\n",
    "                    top_word_dict[k] = v\n",
    "                \n",
    "    return top_word_dict\n",
    "another_top_words_dict = extract_top_words(max_tf_dict, 80)\n",
    "high_tf_idf_words_columns = list(another_top_words_dict.keys())\n",
    "\n",
    "\n",
    "def count_topwords(target_df):\n",
    "\n",
    "    for word in high_tf_idf_words_columns:\n",
    "        \n",
    "        # TODO: костыль, нужен, когда у нас уже есть такие столбцы\n",
    "        # в датасете\n",
    "#         try:\n",
    "#             target_df = target_df.drop(word, 1)\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "        \n",
    "\n",
    "        def count_numwords(collist):\n",
    "            value = 0\n",
    "\n",
    "            for wd in collist:\n",
    "                if wd == word:\n",
    "                    value += 1\n",
    "            return value\n",
    "\n",
    "\n",
    "        target_df[word] = target_df.cleaned_text.apply(count_numwords)\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>heart</th>\n",
       "      <th>mind</th>\n",
       "      <th>even</th>\n",
       "      <th>life</th>\n",
       "      <th>like</th>\n",
       "      <th>may</th>\n",
       "      <th>pass</th>\n",
       "      <th>first</th>\n",
       "      <th>said</th>\n",
       "      <th>friend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper   ...    heart  mind  \\\n",
       "0                35                 7                2   ...        0     0   \n",
       "1                14                 1                0   ...        0     0   \n",
       "2                32                 5                0   ...        0     0   \n",
       "\n",
       "   even  life like  may  pass  first  said  friend  \n",
       "0     0     0    0    0     0      0     0       0  \n",
       "1     0     0    0    0     0      0     0       0  \n",
       "2     0     0    0    0     0      0     0       0  \n",
       "\n",
       "[3 rows x 95 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['length']=df_train['cleaned_text_string'].apply(len)\n",
    "df_train[\"num_words\"] = df_train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_train[\"num_unique_words\"] = df_train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_train[\"num_punctuations\"] =df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_train[\"num_words_upper\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_train[\"num_words_title\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_train[\"mean_word_len\"] = df_train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_train[\"num_stopwords\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_train['lexical_diversity'] = df_train.text.apply(lexical_diversity)\n",
    "df_train['w2v_array'] = df_train.cleaned_text.apply(avg_up_word2vec_array)\n",
    "count_topwords(df_train)\n",
    "\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_w2v_columns(target_df):\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    for index, row in target_df.iterrows():\n",
    "        counter += 1\n",
    "        \n",
    "#         if counter % 100 == 0:\n",
    "#             print('another hundred processed')\n",
    "        \n",
    "        w2v_array = row['w2v_array']\n",
    "        \n",
    "        for elnum, el in enumerate(w2v_array):\n",
    "            target_df['w2v_feature_' + str(elnum)] = el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_w2v_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hpl=df_train[df_train['author']=='HPL']\n",
    "df_hpl.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eap=df_train[df_train['author']=='EAP']\n",
    "df_eap.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mws=df_train[df_train['author']=='MWS']\n",
    "df_mws.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordset=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#делаю сет со всеми словами\n",
    "for i in df_train.index:\n",
    "    wordset |= set(df_train['cleaned_text'][i])\n",
    "wordlist=list(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#делаю фрейм со словами\n",
    "df_word=pd.DataFrame(columns=[\"word\", \"mws\", \"eap\", \"hpl\", \"all\"])\n",
    "df_word[\"word\"]=wordlist\n",
    "df_word[\"mws\"]=0\n",
    "df_word[\"eap\"]=0\n",
    "df_word[\"hpl\"]=0\n",
    "df_word[\"all\"]=0\n",
    "df_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# как мы будем эту штуку правильнее делать (возможно это жуткий костыль), я хз\n",
    "# сначала создаем словарь где ключ - уникальное слово, а значение - его порядковый номер\n",
    "# затем создаем разреженную матрицу, которую заполняем в зависимости от порядковых номеров \n",
    "word_dict = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#делаю сет со всеми словами\n",
    "# и сразу заготовку под шапку(потом увидишь зачем)\n",
    "counter = 0\n",
    "head = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    for word in wordlist:\n",
    "        if word not in word_dict:\n",
    "            head.append(word)\n",
    "            word_dict[word] = counter\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# видоизменять колонки в pandas руками по одному значению в строке или столбце - очень плохая идея\n",
    "# колонка это numpy.ndarray, а значит при каждой итерации она будет пересоздаваться\n",
    "# что угробит производительность\n",
    "# делаем значит так. считаем где сколько и где встречались отдельные слова, затем создаем строку за строкой для \n",
    "# каждого предложения\n",
    "\n",
    "list_of_lists = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    row = [0 for i in range(len(word_dict))]\n",
    "    for word in wordlist:\n",
    "        row[word_dict[word]] += 1\n",
    "    list_of_lists.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_of_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... и для того чтобы посмотреть встречаемость того или иного слова по авторам добавим такую колонку\n",
    "\n",
    "count_frame = pd.DataFrame(list_of_lists)\n",
    "count_frame['author'] = df_train['author']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... и теперь нормальную шапку делаем\n",
    "\n",
    "count_frame.columns = head + ['author']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пока объединим все, потом может быть будем использовать\n",
    "col=list(count_frame.columns)\n",
    "col[-1]='author_name'\n",
    "count_frame.columns=col\n",
    "pivot_col=pd.pivot_table(count_frame, aggfunc=np.sum, values=col, index=['author_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Убираем лишние слова, которые не учли раньше\n",
    "col=list(pivot_col.columns)\n",
    "col2=[string for string in col if (string[0]!='\"' and string[0]!=\"'\"\n",
    "                                  and string[0]!='.' and string[0]!='`'\n",
    "                                   and len(string)>3 and '.' not in string)]\n",
    "col=[]\n",
    "pivot_col=pivot_col[col2]\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pivot\n",
    "pivot_col=pivot_col.append(pivot_col.sum(), ignore_index=True)\n",
    "pivot_col.index=['EAP', 'HPL', 'MWS', 'SUMA']\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summa=[pivot_col.loc['EAP'].sum(), pivot_col.loc['HPL'].sum(), \n",
    "       pivot_col.loc['MWS'].sum(), pivot_col.loc['SUMA'].sum()]\n",
    "pivot_col['summa']=summa\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probability of author text knowing that a word was used\n",
    "pivot_part=pivot_col\n",
    "pivot_part.loc['EAP']=pivot_col.loc['EAP']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['HPL']=pivot_col.loc['HPL']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['MWS']=pivot_col.loc['MWS']/pivot_col.loc['SUMA']\n",
    "pivot_part=pivot_part.loc[['EAP', 'HPL', 'MWS']]\n",
    "# Delete unique words\n",
    "pivot_part=pivot_part.loc[:, (pivot_part!=1).all(axis=0)]\n",
    "pivot_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will be easier to work this way\n",
    "eap_dict=pivot_part.loc['EAP'].to_dict()\n",
    "hpl_dict=pivot_part.loc['HPL'].to_dict()\n",
    "mws_dict=pivot_part.loc['MWS'].to_dict()\n",
    "eap_dict['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create author score \n",
    "def ind_val_eap(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=eap_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_hpl(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=hpl_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_mws(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=mws_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add index of author\n",
    "df_train['mws_index']=df_train['cleaned_text'].apply(ind_val_mws)/df_train['length']\n",
    "df_train['eap_index']=df_train['cleaned_text'].apply(ind_val_eap)/df_train['length']\n",
    "df_train['hpl_index']=df_train['cleaned_text'].apply(ind_val_hpl)/df_train['length']\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform authors' names to numeric\n",
    "df_train['author']=df_train['author'].astype('category')\n",
    "df_train['author2']=df_train['author'].cat.codes\n",
    "# Create different features \n",
    "df_train.head(n=3)\n",
    "mid = df_train['author2']\n",
    "df_train.drop(labels=['author2'], axis=1,inplace = True)\n",
    "df_train.insert(0, 'author2', mid)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = df_train['author'].map(author_mapping_dict)\n",
    "train_id = df_train['id'].values\n",
    "test_id = df_test['id'].values\n",
    "cols_to_drop = ['id', 'text']\n",
    "train_X = df_train.drop(cols_to_drop+['author'], axis=1)\n",
    "test_X = df_test.drop(cols_to_drop, axis=1)\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf = tfidf_vec.fit_transform(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=3, shuffle=True, random_state=194)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 50\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "df_test = pd.concat([df_test, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the count vectorizer ###\n",
    "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "tfidf_vec.fit(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# add the predictions as new features #\n",
    "df_train[\"nb_cvec_eap\"] = pred_train[:,0]\n",
    "df_train[\"nb_cvec_hpl\"] = pred_train[:,1]\n",
    "df_train[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "df_test[\"nb_cvec_eap\"] = pred_full_test[:,0]\n",
    "df_test[\"nb_cvec_hpl\"] = pred_full_test[:,1]\n",
    "df_test[\"nb_cvec_mws\"] = pred_full_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train['w2v_array']\n",
    "df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data set\n",
    "ds_train=df_train.values\n",
    "X=ds_train[:, 6:]\n",
    "Y=ds_train[:, 0]\n",
    "seed=7\n",
    "test_size=0.3\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X,Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "xg_train=xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test=xgb.DMatrix(X_test, label=y_test)\n",
    "xg_t=xgb.DMatrix(X, label=Y)\n",
    "param={}\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['eta'] = 0.2\n",
    "param['max_depth'] = 2\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 3\n",
    "param['eval_metric']= \"mlogloss\"\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 240\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# get prediction\n",
    "pred = bst.predict(xg_test)\n",
    "error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_test).reshape(y_test.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != y_test) / y_test.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_t, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_t).reshape(Y.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != Y) / Y.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['cleaned_text'] = df_test.text.apply(tokenize_stem)\n",
    "df_test['cleaned_text_string'] = df_test.cleaned_text.apply(' '.join)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['length']=df_test['cleaned_text_string'].apply(len)\n",
    "df_test[\"num_words\"] = df_test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_test[\"num_unique_words\"] = df_test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_test[\"num_punctuations\"] =df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test[\"num_words_upper\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_test[\"num_words_title\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_test[\"mean_word_len\"] = df_test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test[\"num_stopwords\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_test['lexical_diversity'] = df_test.text.apply(lexical_diversity)\n",
    "df_test['w2v_array'] = df_test.cleaned_text.apply(avg_up_word2vec_array)\n",
    "count_topwords(df_test)\n",
    "create_w2v_columns(df_test)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test['mws_index']=df_test['cleaned_text'].apply(ind_val_mws)/df_test['length']\n",
    "df_test['eap_index']=df_test['cleaned_text'].apply(ind_val_eap)/df_test['length']\n",
    "df_test['hpl_index']=df_test['cleaned_text'].apply(ind_val_hpl)/df_test['length']\n",
    "df_test.head(n=3)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_test['cleaned_text']\n",
    "del df_test['cleaned_text_string']\n",
    "del df_test['w2v_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=(df_train.columns.tolist())[6:]\n",
    "[item for item in df_train.columns.tolist() if item not in df_test.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_test=df_test[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t=ds_test[:, :]\n",
    "y_t=df_test['id'].values\n",
    "xg_t=xgb.DMatrix(x_t)\n",
    "pred_prob = bstp.predict(xg_t).reshape(y_t.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export=pd.DataFrame(pred_prob)\n",
    "export.insert(loc=0, column='id', value=y_t)\n",
    "export.columns=['id','EAP', 'HPL', 'MWS']\n",
    "export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export[export['id']=='id23301']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export.to_csv(path_or_buf=\"../data/export.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
