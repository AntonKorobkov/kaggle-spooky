{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tnsuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tnsuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "from string import digits\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes, pipeline\n",
    "from stop_words import get_stop_words\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "# 3 столбца - id, text, author\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# нулей не должно быть!\n",
    "\n",
    "\n",
    "def sum_up_word2vec_array(clnd_text):\n",
    "    \n",
    "    total = None\n",
    "    \n",
    "    for word in clnd_text:\n",
    "        if word in w2v:\n",
    "            if total is None:\n",
    "                total = w2v[word]\n",
    "            else:\n",
    "                total = np.add(total, w2v[word])\n",
    "                \n",
    "    if total is None:\n",
    "        return np.zeros(w2v_array_len)\n",
    "    else:\n",
    "        return total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# нулей не должно быть!\n",
    "\n",
    "\n",
    "def avg_up_word2vec_array(clnd_text):\n",
    "    \n",
    "    total = None\n",
    "    \n",
    "    for word in clnd_text:\n",
    "        if word in w2v:\n",
    "            if total is None:\n",
    "                total = w2v[word]\n",
    "            else:                \n",
    "                total = np.mean([total, w2v[word]], axis=0)\n",
    "                \n",
    "                \n",
    "    if total is None:\n",
    "        return np.zeros(w2v_array_len)\n",
    "    else:\n",
    "        return total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "def tokenize_stem(file_text):\n",
    "    #firstly let's apply nltk tokenization\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #deleting stop_words\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens = [i for i in tokens if ( i not in stop_words )]\n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['cleaned_text'] = df_train.text.apply(tokenize_stem)\n",
    "df_train['cleaned_text_string'] = df_train.cleaned_text.apply(' '.join)\n",
    "df_train.head(n=3)\n",
    "eng_stopwords = set(stopwords.words('english')).union(set(get_stop_words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(file_text):\n",
    "    file_text = file_text.translate(remove_digits)\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "    except:\n",
    "        nltk.download('punkt')\n",
    "        tokens = nltk.word_tokenize(file_text)\n",
    "        \n",
    "\n",
    "    #let's delete punctuation symbols\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation )]\n",
    "\n",
    "    #cleaning words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    tokens = [stemmer.stem(i) for i in tokens]\n",
    "\n",
    "    return len(set(tokens))/len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[matrix([[ 0.00053369,  0.00053369,  0.00106738, ...,  0.00053369,\n",
      "          0.        ,  0.        ]]), matrix([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "          0.00135189,  0.00067595]]), matrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "# вытаскиваем \"значимые\" слова\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "raw_documents_authors = ['', '', '']\n",
    "\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    \n",
    "    if row['author'] == 'EAP':\n",
    "        raw_documents_authors[0] += row['cleaned_text_string'] + ' '\n",
    "    elif row['author'] == 'HPL':\n",
    "        raw_documents_authors[1] += row['cleaned_text_string'] + ' '\n",
    "    else:\n",
    "        raw_documents_authors[2] += row['cleaned_text_string'] + ' '\n",
    "        \n",
    "\n",
    "# удалим уникальные слова, не встречающиеся у других писателей\n",
    "\n",
    "eap_only = set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "hpl_only = set(raw_documents_authors[1].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[2].split(' '))\n",
    "msh_only = set(raw_documents_authors[2].split(' ')) - set(raw_documents_authors[0].split(' ')) - set(raw_documents_authors[1].split(' '))\n",
    "\n",
    "unique_words = eap_only.union(hpl_only).union(msh_only)\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word')\n",
    "idf_matrix =  tf.fit_transform(raw_documents_authors)\n",
    "feature_names = tf.get_feature_names()\n",
    "# dictionary_word = dict(zip(feature_names, idf_matrix))\n",
    "\n",
    "dense_idf = [i.todense() for i in idf_matrix]\n",
    "print(dense_idf)\n",
    "\n",
    "max_weighted_term = []\n",
    "\n",
    "eap_dense_list = dense_idf[0].tolist()[0]\n",
    "hpl_dense_list = dense_idf[1].tolist()[0]\n",
    "mws_dense_list = dense_idf[2].tolist()[0]\n",
    "\n",
    "for inum, i in enumerate(eap_dense_list):\n",
    "    max_weighted_term.append(max(hpl_dense_list[inum], mws_dense_list[inum], \n",
    "                             i))\n",
    "\n",
    "max_tf_dict = dict(zip(feature_names, max_weighted_term))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(df_train['cleaned_text'], size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_array_len = list(w2v.items())[0][1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_top_words(tfidfdict, numwrd):\n",
    "\n",
    "    top_word_dict, min_value, min_key = {}, 99, ''\n",
    "    \n",
    "\n",
    "    for k, v in max_tf_dict.items():\n",
    "        # print(top_word_dict.values())\n",
    "        # print(v)\n",
    "        if k not in unique_words and k not in eng_stopwords:\n",
    "        \n",
    "            if len(top_word_dict) < numwrd:\n",
    "                top_word_dict[k] = v\n",
    "                if v <= min_value:\n",
    "                    min_key = k\n",
    "            else:\n",
    "                # print(v, min(list(top_word_dict.values())))\n",
    "                if v > min(list(top_word_dict.values())) and k not in eng_stopwords:\n",
    "\n",
    "                    min_value = min(top_word_dict.values())\n",
    "\n",
    "                    for ky, va in top_word_dict.items():\n",
    "                        if va == min_value:\n",
    "                            min_key = ky\n",
    "\n",
    "                    top_word_dict.pop(min_key)\n",
    "                    top_word_dict[k] = v\n",
    "                \n",
    "    return top_word_dict\n",
    "another_top_words_dict = extract_top_words(max_tf_dict, 80)\n",
    "high_tf_idf_words_columns = list(another_top_words_dict.keys())\n",
    "\n",
    "\n",
    "def count_topwords(target_df):\n",
    "\n",
    "    for word in high_tf_idf_words_columns:\n",
    "        \n",
    "        # TODO: костыль, нужен, когда у нас уже есть такие столбцы\n",
    "        # в датасете\n",
    "#         try:\n",
    "#             target_df = target_df.drop(word, 1)\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "        \n",
    "\n",
    "        def count_numwords(collist):\n",
    "            value = 0\n",
    "\n",
    "            for wd in collist:\n",
    "                if wd == word:\n",
    "                    value += 1\n",
    "            return value\n",
    "\n",
    "\n",
    "        target_df[word] = target_df.cleaned_text.apply(count_numwords)\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>pass</th>\n",
       "      <th>said</th>\n",
       "      <th>never</th>\n",
       "      <th>yet</th>\n",
       "      <th>much</th>\n",
       "      <th>year</th>\n",
       "      <th>found</th>\n",
       "      <th>natur</th>\n",
       "      <th>life</th>\n",
       "      <th>two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper ...   pass  said  \\\n",
       "0                35                 7                2 ...      0     0   \n",
       "1                14                 1                0 ...      0     0   \n",
       "2                32                 5                0 ...      0     0   \n",
       "\n",
       "   never  yet much  year  found  natur  life  two  \n",
       "0      0    0    0     0      0      0     0    0  \n",
       "1      1    0    0     0      0      0     0    0  \n",
       "2      0    0    0     0      0      0     0    0  \n",
       "\n",
       "[3 rows x 95 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['length']=df_train['cleaned_text_string'].apply(len)\n",
    "df_train[\"num_words\"] = df_train[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_train[\"num_unique_words\"] = df_train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_train[\"num_punctuations\"] =df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_train[\"num_words_upper\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_train[\"num_words_title\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_train[\"mean_word_len\"] = df_train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_train[\"num_stopwords\"] = df_train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_train['lexical_diversity'] = df_train.text.apply(lexical_diversity)\n",
    "df_train['w2v_array'] = df_train.cleaned_text.apply(avg_up_word2vec_array)\n",
    "count_topwords(df_train)\n",
    "\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.536687, 0.144487, -0.586192, -0.187148, 0.4...\n",
       "1        [0.30463, 0.116497, -0.45114, -0.045512, 0.322...\n",
       "2        [0.314471, 0.105539, -0.433048, -0.0620516, 0....\n",
       "3        [0.430405, 0.14495, -0.584528, -0.0876907, 0.4...\n",
       "4        [0.242544, 0.0918613, -0.37189, -0.0214398, 0....\n",
       "5        [0.337986, 0.120764, -0.463934, -0.0648725, 0....\n",
       "6        [0.424276, 0.150119, -0.60864, -0.0703386, 0.4...\n",
       "7        [0.513419, 0.14092, -0.633176, -0.163838, 0.46...\n",
       "8        [0.407558, 0.187889, -0.650236, -0.0121077, 0....\n",
       "9        [0.38811, 0.127324, -0.528913, -0.0789476, 0.3...\n",
       "10       [0.468623, 0.167777, -0.657269, -0.0939805, 0....\n",
       "11       [0.403018, 0.142294, -0.569398, -0.0767927, 0....\n",
       "12       [0.408165, 0.130615, -0.54854, -0.0835062, 0.3...\n",
       "13       [0.553728, 0.150219, -0.631013, -0.163421, 0.4...\n",
       "14       [0.418794, 0.152837, -0.613017, -0.0654715, 0....\n",
       "15       [0.304207, 0.131213, -0.463955, -0.0212996, 0....\n",
       "16       [0.283493, 0.094566, -0.398978, -0.0498545, 0....\n",
       "17       [0.462856, 0.14245, -0.628787, -0.106687, 0.45...\n",
       "18       [0.48158, 0.142683, -0.55507, -0.140639, 0.436...\n",
       "19       [0.260519, 0.0896029, -0.376585, -0.0485131, 0...\n",
       "20       [0.152975, 0.0509675, -0.208005, -0.032677, 0....\n",
       "21       [0.366825, 0.122031, -0.487683, -0.0769974, 0....\n",
       "22       [0.299021, 0.0965573, -0.38548, -0.0718142, 0....\n",
       "23       [0.222492, 0.0753065, -0.302467, -0.0501097, 0...\n",
       "24       [0.29525, 0.102466, -0.414464, -0.0579095, 0.3...\n",
       "25       [0.291055, 0.0895593, -0.378734, -0.0678047, 0...\n",
       "26       [0.249954, 0.0639918, -0.389133, -0.0452739, 0...\n",
       "27       [0.404971, 0.156354, -0.612362, -0.0418036, 0....\n",
       "28       [0.295138, 0.0940116, -0.382595, -0.0723782, 0...\n",
       "29       [0.217128, 0.0702714, -0.300338, -0.0419008, 0...\n",
       "                               ...                        \n",
       "19549    [0.330308, 0.108848, -0.452739, -0.0682885, 0....\n",
       "19550    [0.241572, 0.0869306, -0.364755, -0.0317756, 0...\n",
       "19551    [0.314708, 0.103413, -0.391925, -0.0931216, 0....\n",
       "19552    [0.423903, 0.151167, -0.613848, -0.0760627, 0....\n",
       "19553    [0.174314, 0.059088, -0.253014, -0.0299444, 0....\n",
       "19554    [0.490665, 0.142824, -0.564507, -0.148664, 0.4...\n",
       "19555    [0.31648, 0.109288, -0.435353, -0.0648233, 0.3...\n",
       "19556    [0.358853, 0.115325, -0.468359, -0.0866083, 0....\n",
       "19557    [0.3862, 0.179314, -0.66863, 0.00978534, 0.464...\n",
       "19558    [0.211156, 0.0724194, -0.292578, -0.0476633, 0...\n",
       "19559    [0.57134, 0.158796, -0.605693, -0.194166, 0.46...\n",
       "19560    [0.454646, 0.16503, -0.707055, -0.0531082, 0.4...\n",
       "19561    [0.300584, 0.0800218, -0.454814, -0.0546788, 0...\n",
       "19562    [0.49163, 0.150681, -0.630307, -0.129576, 0.46...\n",
       "19563    [0.405584, 0.145605, -0.599009, -0.0664091, 0....\n",
       "19564    [0.373309, 0.212497, -0.666428, 0.0702118, 0.4...\n",
       "19565    [0.218976, 0.0749376, -0.323306, -0.0336888, 0...\n",
       "19566    [0.48156, 0.166904, -0.653536, -0.0984684, 0.4...\n",
       "19567    [0.476901, 0.154087, -0.631888, -0.116123, 0.4...\n",
       "19568    [0.455808, 0.182233, -0.678714, -0.0523515, 0....\n",
       "19569    [0.350173, 0.122605, -0.486097, -0.0646842, 0....\n",
       "19570    [0.234643, 0.0930482, -0.453291, 0.00489118, 0...\n",
       "19571    [0.36761, 0.130793, -0.49032, -0.0798536, 0.35...\n",
       "19572    [0.467939, 0.139496, -0.548537, -0.144443, 0.4...\n",
       "19573    [0.211751, 0.0711795, -0.28609, -0.0477532, 0....\n",
       "19574    [0.227042, 0.0769206, -0.302508, -0.0465001, 0...\n",
       "19575    [0.213244, 0.0653552, -0.283814, -0.0508428, 0...\n",
       "19576    [0.386635, 0.155642, -0.583291, -0.0377719, 0....\n",
       "19577    [0.257555, 0.107729, -0.955341, 0.0856091, 0.5...\n",
       "19578    [0.249515, 0.0801307, -0.340307, -0.0521383, 0...\n",
       "Name: w2v_array, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['w2v_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_w2v_columns(target_df):\n",
    "    \n",
    "    # сначала вытаскиваем колонку как список списков\n",
    "    \n",
    "    w2v_array = target_df['w2v_array'].tolist()\n",
    "    \n",
    "    for i in range(100):\n",
    "        \n",
    "        target_df['w2v_feature_' + str(i)] = [j[i] for j in w2v_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_w2v_columns(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095598</td>\n",
       "      <td>-0.170478</td>\n",
       "      <td>0.208524</td>\n",
       "      <td>0.375774</td>\n",
       "      <td>-0.273344</td>\n",
       "      <td>0.227232</td>\n",
       "      <td>-0.379291</td>\n",
       "      <td>0.440729</td>\n",
       "      <td>0.400570</td>\n",
       "      <td>-0.046439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081228</td>\n",
       "      <td>-0.060527</td>\n",
       "      <td>0.169384</td>\n",
       "      <td>0.227275</td>\n",
       "      <td>-0.181661</td>\n",
       "      <td>0.134238</td>\n",
       "      <td>-0.227279</td>\n",
       "      <td>0.193570</td>\n",
       "      <td>0.400741</td>\n",
       "      <td>-0.118885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>-0.078356</td>\n",
       "      <td>0.161830</td>\n",
       "      <td>0.239616</td>\n",
       "      <td>-0.185399</td>\n",
       "      <td>0.136541</td>\n",
       "      <td>-0.230260</td>\n",
       "      <td>0.224564</td>\n",
       "      <td>0.364550</td>\n",
       "      <td>-0.105685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper       ...        \\\n",
       "0                35                 7                2       ...         \n",
       "1                14                 1                0       ...         \n",
       "2                32                 5                0       ...         \n",
       "\n",
       "   w2v_feature_90  w2v_feature_91  w2v_feature_92  w2v_feature_93  \\\n",
       "0        0.095598       -0.170478        0.208524        0.375774   \n",
       "1        0.081228       -0.060527        0.169384        0.227275   \n",
       "2        0.076400       -0.078356        0.161830        0.239616   \n",
       "\n",
       "  w2v_feature_94  w2v_feature_95  w2v_feature_96  w2v_feature_97  \\\n",
       "0      -0.273344        0.227232       -0.379291        0.440729   \n",
       "1      -0.181661        0.134238       -0.227279        0.193570   \n",
       "2      -0.185399        0.136541       -0.230260        0.224564   \n",
       "\n",
       "   w2v_feature_98  w2v_feature_99  \n",
       "0        0.400570       -0.046439  \n",
       "1        0.400741       -0.118885  \n",
       "2        0.364550       -0.105685  \n",
       "\n",
       "[3 rows x 195 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hpl=df_train[df_train['author']=='HPL']\n",
    "df_hpl.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_eap=df_train[df_train['author']=='EAP']\n",
    "df_eap.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mws=df_train[df_train['author']=='MWS']\n",
    "df_mws.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordset=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#делаю сет со всеми словами\n",
    "for i in df_train.index:\n",
    "    wordset |= set(df_train['cleaned_text'][i])\n",
    "wordlist=list(wordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>mws</th>\n",
       "      <th>eap</th>\n",
       "      <th>hpl</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stubb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ilenti</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recontr</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brandi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>linguist</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  mws  eap  hpl  all\n",
       "0     stubb    0    0    0    0\n",
       "1    ilenti    0    0    0    0\n",
       "2   recontr    0    0    0    0\n",
       "3    brandi    0    0    0    0\n",
       "4  linguist    0    0    0    0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#делаю фрейм со словами\n",
    "df_word=pd.DataFrame(columns=[\"word\", \"mws\", \"eap\", \"hpl\", \"all\"])\n",
    "df_word[\"word\"]=wordlist\n",
    "df_word[\"mws\"]=0\n",
    "df_word[\"eap\"]=0\n",
    "df_word[\"hpl\"]=0\n",
    "df_word[\"all\"]=0\n",
    "df_word.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# как мы будем эту штуку правильнее делать (возможно это жуткий костыль), я хз\n",
    "# сначала создаем словарь где ключ - уникальное слово, а значение - его порядковый номер\n",
    "# затем создаем разреженную матрицу, которую заполняем в зависимости от порядковых номеров \n",
    "word_dict = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#делаю сет со всеми словами\n",
    "# и сразу заготовку под шапку(потом увидишь зачем)\n",
    "counter = 0\n",
    "head = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    for word in wordlist:\n",
    "        if word not in word_dict:\n",
    "            head.append(word)\n",
    "            word_dict[word] = counter\n",
    "            counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# видоизменять колонки в pandas руками по одному значению в строке или столбце - очень плохая идея\n",
    "# колонка это numpy.ndarray, а значит при каждой итерации она будет пересоздаваться\n",
    "# что угробит производительность\n",
    "# делаем значит так. считаем где сколько и где встречались отдельные слова, затем создаем строку за строкой для \n",
    "# каждого предложения\n",
    "\n",
    "list_of_lists = []\n",
    "\n",
    "for wordlist in df_train['cleaned_text']:\n",
    "    row = [0 for i in range(len(word_dict))]\n",
    "    for word in wordlist:\n",
    "        row[word_dict[word]] += 1\n",
    "    list_of_lists.append(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... и для того чтобы посмотреть встречаемость того или иного слова по авторам добавим такую колонку\n",
    "\n",
    "count_frame = pd.DataFrame(list_of_lists)\n",
    "count_frame['author'] = df_train['author']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ... и теперь нормальную шапку делаем\n",
    "\n",
    "count_frame.columns = head + ['author']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   this  process  howev  afford  mean  ascertain  dimens  dungeon  i  might  \\\n",
      "0     1        1      1       1     1          1       1        1  2      1   \n",
      "1     0        0      0       0     0          0       0        0  0      1   \n",
      "2     0        0      0       0     0          0       0        0  0      0   \n",
      "3     0        0      0       0     0          0       0        0  0      0   \n",
      "4     0        0      0       0     0          0       0        0  0      0   \n",
      "\n",
      "    ...    aegidus  burr  bentley  waltzer  binder  brusqueri  adriat  ancona  \\\n",
      "0   ...          0     0        0        0       0          0       0       0   \n",
      "1   ...          0     0        0        0       0          0       0       0   \n",
      "2   ...          0     0        0        0       0          0       0       0   \n",
      "3   ...          0     0        0        0       0          0       0       0   \n",
      "4   ...          0     0        0        0       0          0       0       0   \n",
      "\n",
      "   agir  author  \n",
      "0     0     EAP  \n",
      "1     0     HPL  \n",
      "2     0     EAP  \n",
      "3     0     MWS  \n",
      "4     0     HPL  \n",
      "\n",
      "[5 rows x 15231 columns]\n"
     ]
    }
   ],
   "source": [
    "print(count_frame.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Пока объединим все, потом может быть будем использовать\n",
    "col=list(count_frame.columns)\n",
    "col[-1]='author_name'\n",
    "count_frame.columns=col\n",
    "pivot_col=pd.pivot_table(count_frame, aggfunc=np.sum, values=col, index=['author_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 14351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  \\\n",
       "author_name                                                                  \n",
       "EAP             1      2      0       22       0     2      1     2      3   \n",
       "HPL             0      0      0       17      24     0      1     3      0   \n",
       "MWS             0      0      1        9       0     0      0     1      2   \n",
       "\n",
       "             abbrevi  ...    æmilianus  æneid  ærial  æronaut  ærostat  \\\n",
       "author_name           ...                                                \n",
       "EAP                2  ...            0      0      1        3        1   \n",
       "HPL                0  ...            2      1      0        0        0   \n",
       "MWS                0  ...            0      0      0        0        0   \n",
       "\n",
       "             æschylus  élite  émeut  οἶδα  υπνος  \n",
       "author_name                                       \n",
       "EAP                 1      1      1     0      0  \n",
       "HPL                 0      0      0     2      1  \n",
       "MWS                 0      0      0     0      0  \n",
       "\n",
       "[3 rows x 14351 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Убираем лишние слова, которые не учли раньше\n",
    "col=list(pivot_col.columns)\n",
    "col2=[string for string in col if (string[0]!='\"' and string[0]!=\"'\"\n",
    "                                  and string[0]!='.' and string[0]!='`'\n",
    "                                   and len(string)>3 and '.' not in string)]\n",
    "col=[]\n",
    "pivot_col=pivot_col[col2]\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUMA</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 14351 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  abbrevi  \\\n",
       "EAP      1      2      0       22       0     2      1     2      3        2   \n",
       "HPL      0      0      0       17      24     0      1     3      0        0   \n",
       "MWS      0      0      1        9       0     0      0     1      2        0   \n",
       "SUMA     1      2      1       48      24     2      2     6      5        2   \n",
       "\n",
       "      ...    æmilianus  æneid  ærial  æronaut  ærostat  æschylus  élite  \\\n",
       "EAP   ...            0      0      1        3        1         1      1   \n",
       "HPL   ...            2      1      0        0        0         0      0   \n",
       "MWS   ...            0      0      0        0        0         0      0   \n",
       "SUMA  ...            2      1      1        3        1         1      1   \n",
       "\n",
       "      émeut  οἶδα  υπνος  \n",
       "EAP       1     0      0  \n",
       "HPL       0     2      1  \n",
       "MWS       0     0      0  \n",
       "SUMA      1     2      1  \n",
       "\n",
       "[4 rows x 14351 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create pivot\n",
    "pivot_col=pivot_col.append(pivot_col.sum(), ignore_index=True)\n",
    "pivot_col.index=['EAP', 'HPL', 'MWS', 'SUMA']\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abas</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbrevi</th>\n",
       "      <th>...</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostat</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeut</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "      <th>summa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>74269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUMA</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>235194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 14352 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaem  aback  abaft  abandon  abaout  abas  abash  abat  abbey  abbrevi  \\\n",
       "EAP      1      2      0       22       0     2      1     2      3        2   \n",
       "HPL      0      0      0       17      24     0      1     3      0        0   \n",
       "MWS      0      0      1        9       0     0      0     1      2        0   \n",
       "SUMA     1      2      1       48      24     2      2     6      5        2   \n",
       "\n",
       "       ...    æneid  ærial  æronaut  ærostat  æschylus  élite  émeut  οἶδα  \\\n",
       "EAP    ...        0      1        3        1         1      1      1     0   \n",
       "HPL    ...        1      0        0        0         0      0      0     2   \n",
       "MWS    ...        0      0        0        0         0      0      0     0   \n",
       "SUMA   ...        1      1        3        1         1      1      1     2   \n",
       "\n",
       "      υπνος   summa  \n",
       "EAP       0   87765  \n",
       "HPL       1   74269  \n",
       "MWS       0   73160  \n",
       "SUMA      1  235194  \n",
       "\n",
       "[4 rows x 14352 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summa=[pivot_col.loc['EAP'].sum(), pivot_col.loc['HPL'].sum(), \n",
    "       pivot_col.loc['MWS'].sum(), pivot_col.loc['SUMA'].sum()]\n",
    "pivot_col['summa']=summa\n",
    "pivot_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abash</th>\n",
       "      <th>abat</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdic</th>\n",
       "      <th>aberr</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abhorr</th>\n",
       "      <th>abil</th>\n",
       "      <th>abject</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>your</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zenith</th>\n",
       "      <th>zest</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>zone</th>\n",
       "      <th>summa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.373160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.315778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 6619 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandon  abash      abat  abbey     abdic     aberr     abhor    abhorr  \\\n",
       "EAP  0.458333    0.5  0.333333    0.6  0.142857  0.166667  0.058824  0.111111   \n",
       "HPL  0.354167    0.5  0.500000    0.0  0.000000  0.666667  0.235294  0.555556   \n",
       "MWS  0.187500    0.0  0.166667    0.4  0.857143  0.166667  0.705882  0.333333   \n",
       "\n",
       "         abil    abject    ...      younger  youngest      your     youth  \\\n",
       "EAP  0.789474  0.333333    ...     0.272727       0.2  0.534884  0.101562   \n",
       "HPL  0.052632  0.000000    ...     0.000000       0.4  0.069767  0.429688   \n",
       "MWS  0.157895  0.666667    ...     0.727273       0.4  0.395349  0.468750   \n",
       "\n",
       "         zeal  zenith  zest  zigzag      zone     summa  \n",
       "EAP  0.117647     0.4   0.2     0.4  0.666667  0.373160  \n",
       "HPL  0.470588     0.6   0.2     0.6  0.333333  0.315778  \n",
       "MWS  0.411765     0.0   0.6     0.0  0.000000  0.311062  \n",
       "\n",
       "[3 rows x 6619 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create probability of author text knowing that a word was used\n",
    "pivot_part=pivot_col\n",
    "pivot_part.loc['EAP']=pivot_col.loc['EAP']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['HPL']=pivot_col.loc['HPL']/pivot_col.loc['SUMA']\n",
    "pivot_part.loc['MWS']=pivot_col.loc['MWS']/pivot_col.loc['SUMA']\n",
    "pivot_part=pivot_part.loc[['EAP', 'HPL', 'MWS']]\n",
    "# Delete unique words\n",
    "pivot_part=pivot_part.loc[:, (pivot_part!=1).all(axis=0)]\n",
    "pivot_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44859813084112149"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will be easier to work this way\n",
    "eap_dict=pivot_part.loc['EAP'].to_dict()\n",
    "hpl_dict=pivot_part.loc['HPL'].to_dict()\n",
    "mws_dict=pivot_part.loc['MWS'].to_dict()\n",
    "eap_dict['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create author score \n",
    "def ind_val_eap(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=eap_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_hpl(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=hpl_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant\n",
    "\n",
    "def ind_val_mws(listn):\n",
    "    quant=0\n",
    "    for word in listn:\n",
    "        try:\n",
    "            quant+=mws_dict[word]\n",
    "        except KeyError:\n",
    "            quant+=0\n",
    "    return quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, howev, afford, mean, ascertain...</td>\n",
       "      <td>this process howev afford mean ascertain dimen...</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375774</td>\n",
       "      <td>-0.273344</td>\n",
       "      <td>0.227232</td>\n",
       "      <td>-0.379291</td>\n",
       "      <td>0.440729</td>\n",
       "      <td>0.400570</td>\n",
       "      <td>-0.046439</td>\n",
       "      <td>0.035935</td>\n",
       "      <td>0.074388</td>\n",
       "      <td>0.034504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumbl, might, mere, mistak]</td>\n",
       "      <td>it never occur fumbl might mere mistak</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227275</td>\n",
       "      <td>-0.181661</td>\n",
       "      <td>0.134238</td>\n",
       "      <td>-0.227279</td>\n",
       "      <td>0.193570</td>\n",
       "      <td>0.400741</td>\n",
       "      <td>-0.118885</td>\n",
       "      <td>0.035860</td>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.034739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "      <td>in left hand gold snuff box caper hill cut man...</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239616</td>\n",
       "      <td>-0.185399</td>\n",
       "      <td>0.136541</td>\n",
       "      <td>-0.230260</td>\n",
       "      <td>0.224564</td>\n",
       "      <td>0.364550</td>\n",
       "      <td>-0.105685</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.037337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [this, process, howev, afford, mean, ascertain...   \n",
       "1     [it, never, occur, fumbl, might, mere, mistak]   \n",
       "2  [in, left, hand, gold, snuff, box, caper, hill...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  this process howev afford mean ascertain dimen...     145         41   \n",
       "1             it never occur fumbl might mere mistak      38         14   \n",
       "2  in left hand gold snuff box caper hill cut man...     116         36   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper    ...      \\\n",
       "0                35                 7                2    ...       \n",
       "1                14                 1                0    ...       \n",
       "2                32                 5                0    ...       \n",
       "\n",
       "   w2v_feature_93  w2v_feature_94  w2v_feature_95  w2v_feature_96  \\\n",
       "0        0.375774       -0.273344        0.227232       -0.379291   \n",
       "1        0.227275       -0.181661        0.134238       -0.227279   \n",
       "2        0.239616       -0.185399        0.136541       -0.230260   \n",
       "\n",
       "  w2v_feature_97  w2v_feature_98  w2v_feature_99  mws_index  eap_index  \\\n",
       "0       0.440729        0.400570       -0.046439   0.035935   0.074388   \n",
       "1       0.193570        0.400741       -0.118885   0.035860   0.060980   \n",
       "2       0.224564        0.364550       -0.105685   0.026900   0.047832   \n",
       "\n",
       "   hpl_index  \n",
       "0   0.034504  \n",
       "1   0.034739  \n",
       "2   0.037337  \n",
       "\n",
       "[3 rows x 198 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add index of author\n",
    "df_train['mws_index']=df_train['cleaned_text'].apply(ind_val_mws)/df_train['length']\n",
    "df_train['eap_index']=df_train['cleaned_text'].apply(ind_val_eap)/df_train['length']\n",
    "df_train['hpl_index']=df_train['cleaned_text'].apply(ind_val_hpl)/df_train['length']\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transform authors' names to numeric\n",
    "df_train['author']=df_train['author'].astype('category')\n",
    "df_train['author2']=df_train['author'].cat.codes\n",
    "# Create different features \n",
    "df_train.head(n=3)\n",
    "mid = df_train['author2']\n",
    "df_train.drop(labels=['author2'], axis=1,inplace = True)\n",
    "df_train.insert(0, 'author2', mid)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = df_train['author'].map(author_mapping_dict)\n",
    "train_id = df_train['id'].values\n",
    "test_id = df_test['id'].values\n",
    "cols_to_drop = ['id', 'text']\n",
    "train_X = df_train.drop(cols_to_drop+['author'], axis=1)\n",
    "test_X = df_test.drop(cols_to_drop, axis=1)\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf = tfidf_vec.fit_transform(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=3, shuffle=True, random_state=194)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_comp = 50\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "df_train = pd.concat([df_train, train_svd], axis=1)\n",
    "df_test = pd.concat([df_test, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the count vectorizer ###\n",
    "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "tfidf_vec.fit(df_train['text'].values.tolist() + df_test['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(df_train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(df_test['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([df_train.shape[0], 3])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# add the predictions as new features #\n",
    "df_train[\"nb_cvec_eap\"] = pred_train[:,0]\n",
    "df_train[\"nb_cvec_hpl\"] = pred_train[:,1]\n",
    "df_train[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "df_test[\"nb_cvec_eap\"] = pred_full_test[:,0]\n",
    "df_test[\"nb_cvec_hpl\"] = pred_full_test[:,1]\n",
    "df_test[\"nb_cvec_mws\"] = pred_full_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_train['w2v_array']\n",
    "df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create data set\n",
    "ds_train=df_train.values\n",
    "X=ds_train[:, 6:]\n",
    "Y=ds_train[:, 0]\n",
    "seed=7\n",
    "test_size=0.3\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X,Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a float is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-c81c1aed78b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxg_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mxg_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxg_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'can not initialize DMatrix from {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mset_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0minformation\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mset\u001b[0m \u001b[0minto\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_float_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mset_float_info\u001b[0;34m(self, field, data)\u001b[0m\n\u001b[1;32m    379\u001b[0m         _check_call(_LIB.XGDMatrixSetFloatInfo(self.handle,\n\u001b[1;32m    380\u001b[0m                                                \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                                                \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                                                len(data)))\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mc_array\u001b[0;34m(ctype, values)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m\"\"\"Convert a python string to c array.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a float is required"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "xg_train=xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test=xgb.DMatrix(X_test, label=y_test)\n",
    "xg_t=xgb.DMatrix(X, label=Y)\n",
    "param={}\n",
    "param['objective'] = 'multi:softmax'\n",
    "param['eta'] = 0.2\n",
    "param['max_depth'] = 2\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 3\n",
    "param['eval_metric']= \"mlogloss\"\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 240\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# get prediction\n",
    "pred = bst.predict(xg_test)\n",
    "error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'param' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-279a9388d6c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# do the same thing again, but output probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'objective'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'multi:softprob'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbstp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxg_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwatchlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Note: this convention has been changed since xgboost-unity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# get prediction, this is in 1D array, need reshape to (ndata, nclass)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'param' is not defined"
     ]
    }
   ],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_test).reshape(y_test.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != y_test) / y_test.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do the same thing again, but output probabilities\n",
    "param['objective'] = 'multi:softprob'\n",
    "bstp = xgb.train(param, xg_t, num_round, watchlist)\n",
    "# Note: this convention has been changed since xgboost-unity\n",
    "# get prediction, this is in 1D array, need reshape to (ndata, nclass)\n",
    "pred_prob = bstp.predict(xg_t).reshape(Y.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != Y) / Y.shape[0]\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>[still, i, urg, leav, ireland, inquietud, impa...</td>\n",
       "      <td>still i urg leav ireland inquietud impati fath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>[if, fire, want, fan, could, readili, fan, new...</td>\n",
       "      <td>if fire want fan could readili fan newspap gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>[and, broken, frail, door, found, two, clean, ...</td>\n",
       "      <td>and broken frail door found two clean pick hum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [still, i, urg, leav, ireland, inquietud, impa...   \n",
       "1  [if, fire, want, fan, could, readili, fan, new...   \n",
       "2  [and, broken, frail, door, found, two, clean, ...   \n",
       "\n",
       "                                 cleaned_text_string  \n",
       "0  still i urg leav ireland inquietud impati fath...  \n",
       "1  if fire want fan could readili fan newspap gov...  \n",
       "2  and broken frail door found two clean pick hum...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['cleaned_text'] = df_test.text.apply(tokenize_stem)\n",
    "df_test['cleaned_text_string'] = df_test.cleaned_text.apply(' '.join)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_90</th>\n",
       "      <th>w2v_feature_91</th>\n",
       "      <th>w2v_feature_92</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>[still, i, urg, leav, ireland, inquietud, impa...</td>\n",
       "      <td>still i urg leav ireland inquietud impati fath...</td>\n",
       "      <td>67</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105963</td>\n",
       "      <td>-0.068932</td>\n",
       "      <td>0.208588</td>\n",
       "      <td>0.281460</td>\n",
       "      <td>-0.230604</td>\n",
       "      <td>0.171170</td>\n",
       "      <td>-0.281482</td>\n",
       "      <td>0.229407</td>\n",
       "      <td>0.515649</td>\n",
       "      <td>-0.152245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>[if, fire, want, fan, could, readili, fan, new...</td>\n",
       "      <td>if fire want fan could readili fan newspap gov...</td>\n",
       "      <td>181</td>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050845</td>\n",
       "      <td>-0.045915</td>\n",
       "      <td>0.107483</td>\n",
       "      <td>0.158236</td>\n",
       "      <td>-0.119818</td>\n",
       "      <td>0.085733</td>\n",
       "      <td>-0.149473</td>\n",
       "      <td>0.142083</td>\n",
       "      <td>0.252069</td>\n",
       "      <td>-0.075805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>[and, broken, frail, door, found, two, clean, ...</td>\n",
       "      <td>and broken frail door found two clean pick hum...</td>\n",
       "      <td>114</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071826</td>\n",
       "      <td>-0.093079</td>\n",
       "      <td>0.154684</td>\n",
       "      <td>0.250185</td>\n",
       "      <td>-0.180466</td>\n",
       "      <td>0.142070</td>\n",
       "      <td>-0.236339</td>\n",
       "      <td>0.252476</td>\n",
       "      <td>0.340402</td>\n",
       "      <td>-0.087160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [still, i, urg, leav, ireland, inquietud, impa...   \n",
       "1  [if, fire, want, fan, could, readili, fan, new...   \n",
       "2  [and, broken, frail, door, found, two, clean, ...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  still i urg leav ireland inquietud impati fath...      67         19   \n",
       "1  if fire want fan could readili fan newspap gov...     181         62   \n",
       "2  and broken frail door found two clean pick hum...     114         33   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper  num_words_title  \\\n",
       "0                19                 3                1                3   \n",
       "1                49                 7                1                3   \n",
       "2                30                 3                0                1   \n",
       "\n",
       "        ...        w2v_feature_90  w2v_feature_91  w2v_feature_92  \\\n",
       "0       ...              0.105963       -0.068932        0.208588   \n",
       "1       ...              0.050845       -0.045915        0.107483   \n",
       "2       ...              0.071826       -0.093079        0.154684   \n",
       "\n",
       "  w2v_feature_93  w2v_feature_94  w2v_feature_95  w2v_feature_96  \\\n",
       "0       0.281460       -0.230604        0.171170       -0.281482   \n",
       "1       0.158236       -0.119818        0.085733       -0.149473   \n",
       "2       0.250185       -0.180466        0.142070       -0.236339   \n",
       "\n",
       "   w2v_feature_97  w2v_feature_98  w2v_feature_99  \n",
       "0        0.229407        0.515649       -0.152245  \n",
       "1        0.142083        0.252069       -0.075805  \n",
       "2        0.252476        0.340402       -0.087160  \n",
       "\n",
       "[3 rows x 194 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['length']=df_test['cleaned_text_string'].apply(len)\n",
    "df_test[\"num_words\"] = df_test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df_test[\"num_unique_words\"] = df_test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "df_test[\"num_punctuations\"] =df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "df_test[\"num_words_upper\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "df_test[\"num_words_title\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "df_test[\"mean_word_len\"] = df_test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "df_test[\"num_stopwords\"] = df_test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "df_test['lexical_diversity'] = df_test.text.apply(lexical_diversity)\n",
    "df_test['w2v_array'] = df_test.cleaned_text.apply(avg_up_word2vec_array)\n",
    "count_topwords(df_test)\n",
    "create_w2v_columns(df_test)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_string</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_feature_93</th>\n",
       "      <th>w2v_feature_94</th>\n",
       "      <th>w2v_feature_95</th>\n",
       "      <th>w2v_feature_96</th>\n",
       "      <th>w2v_feature_97</th>\n",
       "      <th>w2v_feature_98</th>\n",
       "      <th>w2v_feature_99</th>\n",
       "      <th>mws_index</th>\n",
       "      <th>eap_index</th>\n",
       "      <th>hpl_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>[still, i, urg, leav, ireland, inquietud, impa...</td>\n",
       "      <td>still i urg leav ireland inquietud impati fath...</td>\n",
       "      <td>67</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281460</td>\n",
       "      <td>-0.230604</td>\n",
       "      <td>0.171170</td>\n",
       "      <td>-0.281482</td>\n",
       "      <td>0.229407</td>\n",
       "      <td>0.515649</td>\n",
       "      <td>-0.152245</td>\n",
       "      <td>0.071473</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>0.026541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>[if, fire, want, fan, could, readili, fan, new...</td>\n",
       "      <td>if fire want fan could readili fan newspap gov...</td>\n",
       "      <td>181</td>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158236</td>\n",
       "      <td>-0.119818</td>\n",
       "      <td>0.085733</td>\n",
       "      <td>-0.149473</td>\n",
       "      <td>0.142083</td>\n",
       "      <td>0.252069</td>\n",
       "      <td>-0.075805</td>\n",
       "      <td>0.035932</td>\n",
       "      <td>0.062884</td>\n",
       "      <td>0.039306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>[and, broken, frail, door, found, two, clean, ...</td>\n",
       "      <td>and broken frail door found two clean pick hum...</td>\n",
       "      <td>114</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250185</td>\n",
       "      <td>-0.180466</td>\n",
       "      <td>0.142070</td>\n",
       "      <td>-0.236339</td>\n",
       "      <td>0.252476</td>\n",
       "      <td>0.340402</td>\n",
       "      <td>-0.087160</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.057723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...   \n",
       "2  id00134  And when they had broken down the frail door t...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  [still, i, urg, leav, ireland, inquietud, impa...   \n",
       "1  [if, fire, want, fan, could, readili, fan, new...   \n",
       "2  [and, broken, frail, door, found, two, clean, ...   \n",
       "\n",
       "                                 cleaned_text_string  length  num_words  \\\n",
       "0  still i urg leav ireland inquietud impati fath...      67         19   \n",
       "1  if fire want fan could readili fan newspap gov...     181         62   \n",
       "2  and broken frail door found two clean pick hum...     114         33   \n",
       "\n",
       "   num_unique_words  num_punctuations  num_words_upper  num_words_title  \\\n",
       "0                19                 3                1                3   \n",
       "1                49                 7                1                3   \n",
       "2                30                 3                0                1   \n",
       "\n",
       "     ...      w2v_feature_93  w2v_feature_94  w2v_feature_95 w2v_feature_96  \\\n",
       "0    ...            0.281460       -0.230604        0.171170      -0.281482   \n",
       "1    ...            0.158236       -0.119818        0.085733      -0.149473   \n",
       "2    ...            0.250185       -0.180466        0.142070      -0.236339   \n",
       "\n",
       "   w2v_feature_97  w2v_feature_98  w2v_feature_99  mws_index  eap_index  \\\n",
       "0        0.229407        0.515649       -0.152245   0.071473   0.036315   \n",
       "1        0.142083        0.252069       -0.075805   0.035932   0.062884   \n",
       "2        0.252476        0.340402       -0.087160   0.027486   0.055142   \n",
       "\n",
       "   hpl_index  \n",
       "0   0.026541  \n",
       "1   0.039306  \n",
       "2   0.057723  \n",
       "\n",
       "[3 rows x 197 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['mws_index']=df_test['cleaned_text'].apply(ind_val_mws)/df_test['length']\n",
    "df_test['eap_index']=df_test['cleaned_text'].apply(ind_val_eap)/df_test['length']\n",
    "df_test['hpl_index']=df_test['cleaned_text'].apply(ind_val_hpl)/df_test['length']\n",
    "df_test.head(n=3)\n",
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_test['cleaned_text']\n",
    "del df_test['cleaned_text_string']\n",
    "del df_test['w2v_array']\n",
    "del df_train['w2v_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author', 'cleaned_text', 'cleaned_text_string']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=(df_train.columns.tolist())[6:]\n",
    "[item for item in df_train.columns.tolist() if item not in df_test.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_test=df_test[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bstp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-37848931b58e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxg_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbstp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bstp' is not defined"
     ]
    }
   ],
   "source": [
    "x_t=ds_test[:, :]\n",
    "y_t=df_test['id'].values\n",
    "xg_t=xgb.DMatrix(x_t)\n",
    "pred_prob = bstp.predict(xg_t).reshape(y_t.shape[0], 3)\n",
    "pred_label = np.argmax(pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export=pd.DataFrame(pred_prob)\n",
    "export.insert(loc=0, column='id', value=y_t)\n",
    "export.columns=['id','EAP', 'HPL', 'MWS']\n",
    "export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export[export['id']=='id23301']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export.to_csv(path_or_buf=\"../data/export.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
